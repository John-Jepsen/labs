{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"agentic/01_reactive_agent/","title":"Reactive Agent: Learning Through Implementation","text":""},{"location":"agentic/01_reactive_agent/#objective","title":"Objective","text":"<p>Build a reactive agent that responds directly to inputs without maintaining internal state, demonstrating the most basic agent architecture and its limitations.</p>"},{"location":"agentic/01_reactive_agent/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Create and activate a virtual environment 2. Install dependencies 3. Configure API access <pre><code># Create a virtual environment\npython -m venv agentic-env\nsource agentic-env/bin/activate  # On Windows: agentic-env\\Scripts\\activate\n\n# Install dependencies\npip install openai langchain pydantic python-dotenv\n\n# Create a .env file for your API key\necho \"OPENAI_API_KEY=your_api_key_here\" &gt; .env\n</code></pre>"},{"location":"agentic/01_reactive_agent/#verifying-setup","title":"Verifying Setup","text":"<p>Run the following test code to ensure your environment is ready:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\n\n# Load environment variables\nload_dotenv()\n\n# Test API connection\nllm = OpenAI(temperature=0)\nresult = llm.invoke(\"Hello!\")\nprint(f\"LLM Response: {result}\")\n</code></pre>"},{"location":"agentic/01_reactive_agent/#core-concept-reactive-agents","title":"Core Concept: Reactive Agents","text":"<p>Reactive agents follow the simplest agent paradigm:</p> <ol> <li>Perceive the current environment through inputs</li> <li>Select an action based on a predefined set of rules</li> <li>Execute the action</li> <li>Repeat for each new input</li> </ol> <p>Key characteristics:</p> <ul> <li>Stateless - No memory of past interactions</li> <li>Immediate response - Direct mapping from input to output</li> <li>No planning - Cannot reason about future states</li> <li>Rule-based - Follow predetermined action patterns</li> </ul>"},{"location":"agentic/01_reactive_agent/#real-world-examples","title":"Real-world Examples","text":"<ul> <li>Thermostats</li> <li>Basic chatbots</li> <li>Robotic vacuum cleaners (basic models)</li> <li>Traffic lights</li> </ul>"},{"location":"agentic/01_reactive_agent/#modular-code-implementation","title":"Modular Code Implementation","text":""},{"location":"agentic/01_reactive_agent/#1-agent-design","title":"1. Agent Design","text":"<pre><code>import os\nfrom typing import Dict, List, Any\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nclass ReactiveAgent:\n    \"\"\"A simple reactive agent that responds to inputs without maintaining state.\"\"\"\n\n    def __init__(self, llm=None):\n        \"\"\"Initialize the reactive agent with an optional language model.\"\"\"\n        # YOUR CODE: Initialize the agent with a language model\n        # If no model is provided, create a default one\n        load_dotenv()\n        self.llm = llm or OpenAI(temperature=0.7)\n\n        # Define the basic prompt template for the agent\n        self.prompt_template = PromptTemplate(\n            input_variables=[\"input\"],\n            template=\"\"\"\n            You are a helpful assistant that responds directly to user requests.\n            Respond to the following input:\n\n            User Input: {input}\n\n            Your response:\n            \"\"\"\n        )\n\n    def act(self, input_text: str) -&gt; str:\n        \"\"\"Process the input and generate a response based on current input only.\"\"\"\n        # YOUR CODE: Generate a response using the prompt template and LLM\n        prompt = self.prompt_template.format(input=input_text)\n        response = self.llm.invoke(prompt)\n        return response\n</code></pre>"},{"location":"agentic/01_reactive_agent/#2-collaborative-coding-task","title":"2. Collaborative Coding Task","text":"<p>Each team member should take on one of the following roles:</p> <ul> <li>Implementer: Completes the code above and handles execution</li> <li>Analyst: Documents limitations and strengths observed during testing</li> <li>Verifier: Creates test cases and validates agent responses</li> </ul>"},{"location":"agentic/01_reactive_agent/#3-test-harness","title":"3. Test Harness","text":"<pre><code># YOUR GROUP TASK: Build a test harness to interact with your agent\n\ndef test_reactive_agent():\n    \"\"\"Test the reactive agent with various inputs and analyze responses.\"\"\"\n    agent = ReactiveAgent()\n\n    test_inputs = [\n        \"What is the capital of France?\",\n        \"I just told you I'm planning a trip to Paris.\",\n        \"Could you recommend some attractions there?\",\n    ]\n\n    print(\"=== Testing Reactive Agent ===\")\n    for input_text in test_inputs:\n        print(f\"\\nInput: {input_text}\")\n        response = agent.act(input_text)\n        print(f\"Response: {response}\")\n\n        # GROUP DISCUSSION POINT: Does the agent remember previous context?\n        # How does this affect the quality of responses?\n\nif __name__ == \"__main__\":\n    test_reactive_agent()\n</code></pre>"},{"location":"agentic/01_reactive_agent/#live-coding-collaboration","title":"Live Coding Collaboration","text":""},{"location":"agentic/01_reactive_agent/#session-structure-45-60-minutes","title":"Session Structure (45-60 minutes)","text":"<ol> <li> <p>Setup &amp; Walkthrough (10 min)</p> </li> <li> <p>Configure environment</p> </li> <li> <p>Review code structure</p> </li> <li> <p>Implementation Phase (20 min)</p> </li> <li> <p>Complete the <code>ReactiveAgent</code> class</p> </li> <li>Build the test harness</li> <li> <p>Create additional test scenarios</p> </li> <li> <p>Testing &amp; Analysis (15 min)</p> </li> <li> <p>Run tests with varied inputs</p> </li> <li>Document observations</li> <li> <p>Identify strengths and limitations</p> </li> <li> <p>Extension (15 min)</p> </li> <li>Implement one of the extension challenges</li> </ol>"},{"location":"agentic/01_reactive_agent/#reflection-checkpoints","title":"Reflection Checkpoints","text":""},{"location":"agentic/01_reactive_agent/#checkpoint-1-after-initial-testing","title":"Checkpoint 1: After Initial Testing","text":"<ul> <li>What patterns do you notice in the agent's responses?</li> <li>When does the agent perform well? When does it struggle?</li> <li>How does the lack of memory impact user experience?</li> </ul>"},{"location":"agentic/01_reactive_agent/#checkpoint-2-after-code-completion","title":"Checkpoint 2: After Code Completion","text":"<ul> <li>How would you rate the agent's:</li> <li>Responsiveness</li> <li>Accuracy</li> <li>Consistency</li> <li>Helpfulness</li> <li>What key limitations have you observed?</li> </ul>"},{"location":"agentic/01_reactive_agent/#debugging-and-extension-challenges","title":"Debugging and Extension Challenges","text":""},{"location":"agentic/01_reactive_agent/#common-issues","title":"Common Issues","text":"<ul> <li>API rate limiting or authentication errors</li> <li>Prompt engineering challenges</li> <li>Response parsing issues</li> </ul>"},{"location":"agentic/01_reactive_agent/#extension-ideas","title":"Extension Ideas","text":"<ol> <li> <p>Rule-Based Enhancement</p> </li> <li> <p>Add explicit rules for common queries</p> </li> <li> <p>Implement a \"fallback\" mechanism for unknown inputs</p> </li> <li> <p>Response Templating</p> </li> <li> <p>Create category-specific response templates</p> </li> <li> <p>Implement basic output formatting</p> </li> <li> <p>Simple Classifier</p> </li> <li>Add a pre-processing step that categorizes inputs</li> <li>Customize prompts based on input category</li> </ol>"},{"location":"agentic/01_reactive_agent/#agent-evaluation","title":"Agent Evaluation","text":""},{"location":"agentic/01_reactive_agent/#effectiveness-metrics","title":"Effectiveness Metrics","text":"<ul> <li>Response relevance to input</li> <li>Information accuracy</li> <li>Task completion rate</li> </ul>"},{"location":"agentic/01_reactive_agent/#efficiency-metrics","title":"Efficiency Metrics","text":"<ul> <li>Response time</li> <li>Token usage</li> <li>Error rate</li> </ul>"},{"location":"agentic/01_reactive_agent/#alignment-metrics","title":"Alignment Metrics","text":"<ul> <li>Adherence to user instructions</li> <li>Tone consistency</li> <li>Safety and ethical considerations</li> </ul>"},{"location":"agentic/01_reactive_agent/#evaluation-exercise","title":"Evaluation Exercise","text":"<p>Create a standardized set of 5-10 inputs that cover different scenarios and evaluate your agent across all metrics above. Record your findings in a shared document.</p>"},{"location":"agentic/01_reactive_agent/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>How does a reactive agent differ from how humans process information?</li> <li>What types of applications are well-suited for reactive agents?</li> <li>What is the fundamental limitation of reactive agents?</li> <li>How might adding simple rules improve performance without adding memory?</li> <li>If you were to redesign this agent, what would be your first improvement?</li> </ol>"},{"location":"agentic/01_reactive_agent/#next-steps","title":"Next Steps","text":"<ul> <li>Explore how adding memory can enhance agent capabilities</li> <li>Investigate planning mechanisms for multi-step reasoning</li> <li>Consider how to maintain agent focus across complex interactions</li> </ul>"},{"location":"agentic/02_planning_agent/","title":"Planning Agent: Goal-Directed Problem Solving","text":""},{"location":"agentic/02_planning_agent/#objective","title":"Objective","text":"<p>Build a planning agent that breaks down complex tasks into manageable steps, demonstrates goal-setting capabilities, and creates/executes multi-step plans to achieve specified objectives.</p>"},{"location":"agentic/02_planning_agent/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Create and activate a virtual environment (if not already done) 2. Install dependencies 3. Configure API access <pre><code># Create a virtual environment (if not already created)\npython -m venv agentic-env\nsource agentic-env/bin/activate  # On Windows: agentic-env\\Scripts\\activate\n\n# Install dependencies\npip install openai langchain pydantic python-dotenv\n\n# Create a .env file for your API key (if not already done)\necho \"OPENAI_API_KEY=your_api_key_here\" &gt; .env\n</code></pre>"},{"location":"agentic/02_planning_agent/#verifying-setup","title":"Verifying Setup","text":"<p>Run the following test code to ensure your environment is ready:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\n\n# Load environment variables\nload_dotenv()\n\n# Test API connection\nllm = OpenAI(temperature=0)\nresult = llm.invoke(\"Hello!\")\nprint(f\"LLM Response: {result}\")\n</code></pre>"},{"location":"agentic/02_planning_agent/#core-concept-planning-agents","title":"Core Concept: Planning Agents","text":"<p>Planning agents elevate agent capabilities by:</p> <ol> <li>Defining goals based on user requests</li> <li>Breaking down complex tasks into sequential steps</li> <li>Executing each step in a logical order</li> <li>Adjusting the plan as needed based on intermediate results</li> </ol> <p>Key characteristics:</p> <ul> <li>Goal-oriented - Actions drive toward specific objectives</li> <li>Sequential - Follows step-by-step reasoning</li> <li>Structured - Organizes thoughts and actions before execution</li> <li>Flexible - Can replan when obstacles arise</li> </ul>"},{"location":"agentic/02_planning_agent/#real-world-examples","title":"Real-world Examples","text":"<ul> <li>GPS navigation systems</li> <li>Manufacturing robots</li> <li>Personal assistants</li> <li>Chess-playing algorithms</li> </ul>"},{"location":"agentic/02_planning_agent/#modular-code-implementation","title":"Modular Code Implementation","text":""},{"location":"agentic/02_planning_agent/#1-agent-design","title":"1. Agent Design","text":"<pre><code>import os\nfrom typing import Dict, List, Any\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nimport json\n\nclass PlanningAgent:\n    \"\"\"A planning agent that breaks down complex tasks into steps and executes them.\"\"\"\n\n    def __init__(self, llm=None):\n        \"\"\"Initialize the planning agent with an optional language model.\"\"\"\n        load_dotenv()\n        self.llm = llm or OpenAI(temperature=0.7)\n\n        # Define the planning prompt template\n        self.planning_template = PromptTemplate(\n            input_variables=[\"goal\"],\n            template=\"\"\"\n            You are a helpful assistant that creates detailed plans.\n\n            Goal: {goal}\n\n            First, analyze this goal. Then, create a step-by-step plan to accomplish it.\n            Format your response as a JSON object with:\n            1. \"analysis\": Brief analysis of the goal\n            2. \"steps\": Array of sequential steps, each with \"step_number\", \"description\", and \"success_criteria\"\n\n            Your plan should be detailed, logical, and achievable.\n            \"\"\"\n        )\n\n        # Define the execution prompt template\n        self.execution_template = PromptTemplate(\n            input_variables=[\"goal\", \"plan\", \"current_step\", \"progress_so_far\"],\n            template=\"\"\"\n            You are executing a step in a plan.\n\n            Goal: {goal}\n            Overall Plan: {plan}\n            Current Step: {current_step}\n            Progress So Far: {progress_so_far}\n\n            Execute this step and provide the result. Be thorough and detailed in your execution.\n            \"\"\"\n        )\n\n    def create_plan(self, goal: str) -&gt; Dict:\n        \"\"\"Generate a structured plan for achieving the specified goal.\"\"\"\n        # YOUR CODE: Generate a plan using the planning template\n        prompt = self.planning_template.format(goal=goal)\n        response = self.llm.invoke(prompt)\n\n        # Parse the JSON response\n        try:\n            plan = json.loads(response)\n        except json.JSONDecodeError:\n            # If the response is not valid JSON, try to extract it\n            import re\n            json_match = re.search(r'```json\\n(.*?)```', response, re.DOTALL)\n            if json_match:\n                try:\n                    plan = json.loads(json_match.group(1))\n                except:\n                    plan = {\"analysis\": \"Failed to parse plan\", \"steps\": []}\n            else:\n                plan = {\"analysis\": \"Failed to parse plan\", \"steps\": []}\n\n        return plan\n\n    def execute_step(self, goal: str, plan: Dict, step_index: int, progress_so_far: List[str]) -&gt; str:\n        \"\"\"Execute a specific step in the plan and return the result.\"\"\"\n        # YOUR CODE: Execute the step using the execution template\n        if step_index &gt;= len(plan[\"steps\"]):\n            return \"Error: Step index out of range\"\n\n        current_step = plan[\"steps\"][step_index]\n        prompt = self.execution_template.format(\n            goal=goal,\n            plan=str(plan[\"steps\"]),\n            current_step=str(current_step),\n            progress_so_far=\"\\n\".join(progress_so_far)\n        )\n\n        result = self.llm.invoke(prompt)\n        return result\n\n    def execute_plan(self, goal: str) -&gt; List[str]:\n        \"\"\"Create and execute a complete plan for the given goal.\"\"\"\n        # YOUR CODE: Implement complete plan execution\n        plan = self.create_plan(goal)\n        progress = []\n\n        print(f\"Goal: {goal}\")\n        print(f\"Analysis: {plan['analysis']}\")\n        print(\"\\nPlan:\")\n        for i, step in enumerate(plan[\"steps\"]):\n            print(f\"Step {step['step_number']}: {step['description']}\")\n\n        print(\"\\nExecution:\")\n        for i in range(len(plan[\"steps\"])):\n            print(f\"\\nExecuting Step {i+1}...\")\n            result = self.execute_step(goal, plan, i, progress)\n            progress.append(f\"Step {i+1} Result: {result}\")\n            print(result)\n\n        return progress\n</code></pre>"},{"location":"agentic/02_planning_agent/#2-collaborative-coding-task","title":"2. Collaborative Coding Task","text":"<p>Each team member should take on one of the following roles:</p> <ul> <li>Planner: Completes the <code>create_plan</code> method and improves the planning prompt</li> <li>Executor: Completes the <code>execute_step</code> and <code>execute_plan</code> methods</li> <li>Evaluator: Creates test cases and evaluates plan quality and execution</li> </ul>"},{"location":"agentic/02_planning_agent/#3-test-harness","title":"3. Test Harness","text":"<pre><code># YOUR GROUP TASK: Build a test harness to interact with your planning agent\n\ndef test_planning_agent():\n    \"\"\"Test the planning agent with various goals and analyze its plans and execution.\"\"\"\n    agent = PlanningAgent()\n\n    test_goals = [\n        \"Write a blog post about artificial intelligence\",\n        \"Plan a birthday party for a 10-year-old\",\n        \"Research and compare three different smartphones\",\n    ]\n\n    print(\"=== Testing Planning Agent ===\")\n\n    # Test detailed planning\n    goal = test_goals[0]\n    plan = agent.create_plan(goal)\n\n    print(f\"\\nGoal: {goal}\")\n    print(f\"Analysis: {plan['analysis']}\")\n    print(\"\\nGenerated Plan:\")\n    for step in plan[\"steps\"]:\n        print(f\"Step {step['step_number']}: {step['description']}\")\n        print(f\"  Success Criteria: {step['success_criteria']}\")\n\n    # GROUP ACTIVITY: Select one of the goals and execute the full plan\n    # execute_goal = test_goals[1]\n    # progress = agent.execute_plan(execute_goal)\n\n    # GROUP DISCUSSION POINT: Assess plan quality, execution effectiveness,\n    # and how well the planning stage accounted for potential issues\n\nif __name__ == \"__main__\":\n    test_planning_agent()\n</code></pre>"},{"location":"agentic/02_planning_agent/#live-coding-collaboration","title":"Live Coding Collaboration","text":""},{"location":"agentic/02_planning_agent/#session-structure-60-75-minutes","title":"Session Structure (60-75 minutes)","text":"<ol> <li> <p>Setup &amp; Planning (15 min)</p> </li> <li> <p>Configure environment</p> </li> <li>Review code structure</li> <li> <p>Understand the planning architecture</p> </li> <li> <p>Implementation Phase (25 min)</p> </li> <li> <p>Complete the agent's planning and execution methods</p> </li> <li>Enhance the planning and execution prompts</li> <li> <p>Implement test scenarios</p> </li> <li> <p>Testing &amp; Execution (20 min)</p> </li> <li> <p>Test with varied goals</p> </li> <li>Analyze plan quality</li> <li> <p>Execute plans and observe results</p> </li> <li> <p>Analysis &amp; Extension (15 min)</p> </li> <li>Identify strengths and weaknesses</li> <li>Implement one extension challenge</li> </ol>"},{"location":"agentic/02_planning_agent/#reflection-checkpoints","title":"Reflection Checkpoints","text":""},{"location":"agentic/02_planning_agent/#checkpoint-1-after-plan-generation","title":"Checkpoint 1: After Plan Generation","text":"<ul> <li>How comprehensive is the generated plan?</li> <li>Are the steps logically ordered?</li> <li>Does the plan account for potential challenges?</li> <li>How could the planning prompt be improved?</li> </ul>"},{"location":"agentic/02_planning_agent/#checkpoint-2-after-plan-execution","title":"Checkpoint 2: After Plan Execution","text":"<ul> <li>Did execution follow the plan faithfully?</li> <li>What unexpected issues arose during execution?</li> <li>How might the agent better handle unexpected outcomes?</li> <li>Was the plan sufficiently detailed for effective execution?</li> </ul>"},{"location":"agentic/02_planning_agent/#debugging-and-extension-challenges","title":"Debugging and Extension Challenges","text":""},{"location":"agentic/02_planning_agent/#common-issues","title":"Common Issues","text":"<ul> <li>JSON parsing errors from malformed LLM output</li> <li>Plans that are too abstract or vague</li> <li>Plans with missing prerequisite steps</li> <li>Execution failures due to incomplete context</li> </ul>"},{"location":"agentic/02_planning_agent/#extension-ideas","title":"Extension Ideas","text":"<ol> <li> <p>Dynamic Replanning</p> </li> <li> <p>Add logic to detect execution failures</p> </li> <li>Implement replanning when steps fail</li> <li> <p>Track overall goal progress</p> </li> <li> <p>Step Dependencies</p> </li> <li> <p>Enhance the plan structure to include step dependencies</p> </li> <li>Implement parallel execution for independent steps</li> <li> <p>Add resource tracking for steps</p> </li> <li> <p>User Feedback Loop</p> </li> <li>Add checkpoints for user feedback</li> <li>Incorporate user preferences into planning</li> <li>Allow plan modification based on feedback</li> </ol>"},{"location":"agentic/02_planning_agent/#agent-evaluation","title":"Agent Evaluation","text":""},{"location":"agentic/02_planning_agent/#effectiveness-metrics","title":"Effectiveness Metrics","text":"<ul> <li>Plan completeness</li> <li>Logical step ordering</li> <li>Success criteria clarity</li> <li>Task completion rate</li> </ul>"},{"location":"agentic/02_planning_agent/#efficiency-metrics","title":"Efficiency Metrics","text":"<ul> <li>Planning time</li> <li>Execution time</li> <li>Token usage</li> <li>Number of steps generated</li> </ul>"},{"location":"agentic/02_planning_agent/#alignment-metrics","title":"Alignment Metrics","text":"<ul> <li>Adherence to user constraints</li> <li>Ethical consideration of actions</li> <li>Transparency of reasoning</li> </ul>"},{"location":"agentic/02_planning_agent/#evaluation-exercise","title":"Evaluation Exercise","text":"<p>Select one complex goal and have each team member independently evaluate the agent's plan using the metrics above. Compare results and discuss differences in assessment.</p>"},{"location":"agentic/02_planning_agent/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>How does planning improve agent performance compared to reactive agents?</li> <li>What cognitive limitations of humans does a planning agent help overcome?</li> <li>When might a simple reactive agent outperform a planning agent?</li> <li>How might you implement \"common sense\" checking in a planning agent?</li> <li>What information should be tracked between planning and execution stages?</li> <li>How would you redesign this agent to better handle uncertainty?</li> </ol>"},{"location":"agentic/02_planning_agent/#next-steps","title":"Next Steps","text":"<ul> <li>Explore how adding tools can extend the agent's capabilities</li> <li>Investigate how to combine planning with memory</li> <li>Consider how multiple planning agents might collaborate</li> </ul>"},{"location":"agentic/03_tool_using_agent/","title":"Tool-Using Agent: Extending Agent Capabilities","text":""},{"location":"agentic/03_tool_using_agent/#objective","title":"Objective","text":"<p>Build an agent that can use external tools like web search and calculators to solve problems beyond its built-in knowledge, demonstrating how agent capabilities can be extended through tool integration.</p>"},{"location":"agentic/03_tool_using_agent/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Create and activate a virtual environment (if not already done) 2. Install dependencies (including tool-specific ones) 3. Configure API access <pre><code># Create a virtual environment (if not already created)\npython -m venv agentic-env\nsource agentic-env/bin/activate  # On Windows: agentic-env\\Scripts\\activate\n\n# Install dependencies\npip install openai langchain pydantic python-dotenv wikipedia duckduckgo-search\n\n# Create a .env file for your API key (if not already done)\necho \"OPENAI_API_KEY=your_api_key_here\" &gt; .env\n</code></pre>"},{"location":"agentic/03_tool_using_agent/#verifying-setup","title":"Verifying Setup","text":"<p>Run the following test code to ensure your environment is ready:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\n\n# Load environment variables\nload_dotenv()\n\n# Test API connection\nllm = OpenAI(temperature=0)\nresult = llm.invoke(\"Hello!\")\nprint(f\"LLM Response: {result}\")\n</code></pre>"},{"location":"agentic/03_tool_using_agent/#core-concept-tool-using-agents","title":"Core Concept: Tool-Using Agents","text":"<p>Tool-using agents expand capabilities through:</p> <ol> <li>Tool recognition - Identifying when external tools are needed</li> <li>Tool selection - Choosing the appropriate tool for a task</li> <li>Tool usage - Properly formatting inputs and interpreting outputs</li> <li>Result integration - Incorporating tool outputs into responses</li> </ol> <p>Key characteristics:</p> <ul> <li>Extensible - Can be enhanced with new tools</li> <li>Specialized - Uses purpose-built tools for specific tasks</li> <li>Resource-aware - Leverages external systems for efficiency</li> <li>Capability-augmented - Overcomes built-in limitations</li> </ul>"},{"location":"agentic/03_tool_using_agent/#real-world-examples","title":"Real-world Examples","text":"<ul> <li>Virtual assistants using weather APIs</li> <li>Programming assistants using code execution</li> <li>Research agents using search engines</li> <li>Math tutors using calculation tools</li> </ul>"},{"location":"agentic/03_tool_using_agent/#modular-code-implementation","title":"Modular Code Implementation","text":""},{"location":"agentic/03_tool_using_agent/#1-tool-definitions","title":"1. Tool Definitions","text":"<pre><code>import os\nimport math\nimport json\nfrom typing import Dict, List, Any, Optional, Callable\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\n# Tool definition (simplified for learning purposes)\nclass Tool:\n    \"\"\"A tool that can be used by an agent to perform specific tasks.\"\"\"\n\n    def __init__(self, name: str, description: str, func: Callable):\n        \"\"\"Initialize a tool with a name, description, and function.\"\"\"\n        self.name = name\n        self.description = description\n        self.func = func\n\n    def use(self, input_str: str) -&gt; str:\n        \"\"\"Use the tool with the given input.\"\"\"\n        return self.func(input_str)\n\n# Example tool implementations\ndef calculator_tool(expression: str) -&gt; str:\n    \"\"\"Evaluates a mathematical expression and returns the result.\"\"\"\n    try:\n        # Using eval is generally unsafe, but this is a simplified example\n        # In production, use a proper mathematical expression parser\n        cleaned_expression = expression.strip()\n        result = eval(cleaned_expression, {\"__builtins__\": {}}, {\"math\": math})\n        return f\"Calculator result: {result}\"\n    except Exception as e:\n        return f\"Calculator error: {str(e)}\"\n\ndef wikipedia_search(query: str) -&gt; str:\n    \"\"\"Searches Wikipedia for information about the query.\"\"\"\n    try:\n        import wikipedia\n        # Search for the query\n        search_results = wikipedia.search(query, results=3)\n        if not search_results:\n            return \"No Wikipedia results found.\"\n\n        # Get the summary of the first result\n        try:\n            page = wikipedia.page(search_results[0])\n            summary = wikipedia.summary(search_results[0], sentences=3)\n            return f\"Wikipedia: {summary}\\nURL: {page.url}\"\n        except wikipedia.DisambiguationError as e:\n            # If there's a disambiguation, get the first option\n            try:\n                page = wikipedia.page(e.options[0])\n                summary = wikipedia.summary(e.options[0], sentences=3)\n                return f\"Wikipedia: {summary}\\nURL: {page.url}\"\n            except:\n                return f\"Multiple Wikipedia matches found: {', '.join(e.options[:5])}\"\n    except Exception as e:\n        return f\"Wikipedia search error: {str(e)}\"\n\n# YOUR GROUP TASK: Implement at least one additional tool\n# Examples: Weather API, Unit converter, News search, etc.\n</code></pre>"},{"location":"agentic/03_tool_using_agent/#2-agent-design","title":"2. Agent Design","text":"<pre><code>class ToolUsingAgent:\n    \"\"\"An agent that can use tools to complete tasks.\"\"\"\n\n    def __init__(self, llm=None):\n        \"\"\"Initialize the tool-using agent with an optional language model.\"\"\"\n        load_dotenv()\n        self.llm = llm or OpenAI(temperature=0.7)\n\n        # Initialize tools\n        self.tools = [\n            Tool(\"calculator\", \"Use this tool to perform mathematical calculations\", calculator_tool),\n            Tool(\"wikipedia\", \"Use this tool to search for information on Wikipedia\", wikipedia_search),\n            # Add your custom tool here\n        ]\n\n        # Build tool descriptions for the prompt\n        self.tool_descriptions = \"\\n\".join([\n            f\"- {tool.name}: {tool.description}\" for tool in self.tools\n        ])\n\n        # Map tool names to tools for easy lookup\n        self.tool_map = {tool.name: tool for tool in self.tools}\n\n        # Define the main prompt template\n        self.prompt_template = PromptTemplate(\n            input_variables=[\"input\", \"tools\", \"tool_history\"],\n            template=\"\"\"\n            You are a helpful assistant that can use tools to answer questions.\n\n            Available tools:\n            {tools}\n\n            Previous tool usage (if any):\n            {tool_history}\n\n            To use a tool, respond with a JSON object with the following structure:\n            {{\n                \"reasoning\": \"Your step-by-step reasoning about what tool to use and why\",\n                \"tool\": \"the_tool_name\",\n                \"tool_input\": \"the input to pass to the tool\"\n            }}\n\n            If you can answer without using a tool, or after you've received a tool result,\n            respond with a JSON object with this structure:\n            {{\n                \"reasoning\": \"Your reasoning process\",\n                \"final_answer\": \"Your comprehensive answer to the question\"\n            }}\n\n            User question: {input}\n\n            Think carefully about whether you need to use a tool and which one is most appropriate.\n            \"\"\"\n        )\n\n    def _parse_response(self, response: str) -&gt; Dict:\n        \"\"\"Parse the JSON response from the LLM.\"\"\"\n        # YOUR CODE: Parse the JSON response\n        try:\n            # Try to parse the entire response as JSON\n            return json.loads(response)\n        except json.JSONDecodeError:\n            # If that fails, try to extract JSON from the response\n            import re\n            json_match = re.search(r'```json\\n(.*?)```', response, re.DOTALL)\n            if json_match:\n                try:\n                    return json.loads(json_match.group(1))\n                except:\n                    # If JSON extraction fails, create a default response\n                    return {\n                        \"reasoning\": \"Failed to parse response\",\n                        \"final_answer\": \"I encountered an error while processing your request.\"\n                    }\n            else:\n                # If no JSON found, create a default response\n                return {\n                    \"reasoning\": \"Failed to parse response\",\n                    \"final_answer\": \"I encountered an error while processing your request.\"\n                }\n\n    def run(self, input_text: str, max_turns: int = 3) -&gt; str:\n        \"\"\"Process the input using tools as needed and generate a response.\"\"\"\n        # YOUR CODE: Implement the main agent loop\n        tool_history = []\n        turns = 0\n\n        while turns &lt; max_turns:\n            # Format the prompt with the input and tool history\n            prompt = self.prompt_template.format(\n                input=input_text,\n                tools=self.tool_descriptions,\n                tool_history=\"\\n\".join(tool_history) if tool_history else \"None\"\n            )\n\n            # Get a response from the LLM\n            response = self.llm.invoke(prompt)\n            parsed_response = self._parse_response(response)\n\n            # Check if the response contains a final answer\n            if \"final_answer\" in parsed_response:\n                return parsed_response[\"final_answer\"]\n\n            # Check if the response requests a tool\n            if \"tool\" in parsed_response and \"tool_input\" in parsed_response:\n                tool_name = parsed_response[\"tool\"]\n                tool_input = parsed_response[\"tool_input\"]\n\n                # Check if the requested tool exists\n                if tool_name in self.tool_map:\n                    # Use the tool\n                    tool_result = self.tool_map[tool_name].use(tool_input)\n\n                    # Add the tool usage to the history\n                    tool_history.append(f\"Tool: {tool_name}\\nInput: {tool_input}\\nResult: {tool_result}\")\n\n                    # Continue to the next turn\n                    turns += 1\n                    continue\n                else:\n                    # If the tool doesn't exist, add an error to the history\n                    tool_history.append(f\"Error: Tool '{tool_name}' not found\")\n            else:\n                # If the response is invalid, add an error to the history\n                tool_history.append(\"Error: Invalid response format\")\n\n            turns += 1\n\n        # If we've reached the maximum number of turns without a final answer\n        return \"I wasn't able to provide a complete answer within the allowed number of tool uses.\"\n</code></pre>"},{"location":"agentic/03_tool_using_agent/#3-test-harness","title":"3. Test Harness","text":"<pre><code>def test_tool_using_agent():\n    \"\"\"Test the tool-using agent with various queries and analyze its tool usage.\"\"\"\n    agent = ToolUsingAgent()\n\n    test_queries = [\n        \"What is the square root of 144?\",\n        \"Tell me about the Python programming language.\",\n        \"If I have 5 apples and give away 2, then buy 3 more, how many do I have?\",\n        # Add your own test queries here\n    ]\n\n    print(\"=== Testing Tool-Using Agent ===\")\n\n    for query in test_queries:\n        print(f\"\\nQuery: {query}\")\n        result = agent.run(query)\n        print(f\"Response: {result}\")\n\n        # GROUP DISCUSSION POINT: Did the agent select the appropriate tool?\n        # Could it have answered without using a tool? Was the tool usage effective?\n\nif __name__ == \"__main__\":\n    test_tool_using_agent()\n</code></pre>"},{"location":"agentic/03_tool_using_agent/#live-coding-collaboration","title":"Live Coding Collaboration","text":""},{"location":"agentic/03_tool_using_agent/#session-structure-60-75-minutes","title":"Session Structure (60-75 minutes)","text":"<ol> <li> <p>Setup &amp; Tool Exploration (15 min)</p> </li> <li> <p>Configure environment</p> </li> <li>Understand available tools</li> <li> <p>Plan custom tool implementation</p> </li> <li> <p>Implementation Phase (25 min)</p> </li> <li> <p>Complete the agent's response parsing and run methods</p> </li> <li>Implement at least one custom tool</li> <li> <p>Enhance the main agent prompt</p> </li> <li> <p>Testing &amp; Analysis (20 min)</p> </li> <li> <p>Test with varied queries requiring different tools</p> </li> <li>Analyze tool selection accuracy</li> <li> <p>Evaluate response quality with and without tools</p> </li> <li> <p>Extension &amp; Refinement (15 min)</p> </li> <li>Implement more sophisticated tool selection logic</li> <li>Add error handling for tool failures</li> <li>Create specialized prompts for tool result processing</li> </ol>"},{"location":"agentic/03_tool_using_agent/#reflection-checkpoints","title":"Reflection Checkpoints","text":""},{"location":"agentic/03_tool_using_agent/#checkpoint-1-after-tool-implementation","title":"Checkpoint 1: After Tool Implementation","text":"<ul> <li>How does each tool extend the agent's capabilities?</li> <li>What are the limitations of each tool?</li> <li>How might users misuse or misunderstand the tools?</li> <li>What additional tools would be most valuable to add?</li> </ul>"},{"location":"agentic/03_tool_using_agent/#checkpoint-2-after-agent-testing","title":"Checkpoint 2: After Agent Testing","text":"<ul> <li>When does the agent correctly identify the need for a tool?</li> <li>When does it fail to use a tool when one would be helpful?</li> <li>How well does it interpret and incorporate tool outputs?</li> <li>What patterns emerge in successful vs. unsuccessful tool usage?</li> </ul>"},{"location":"agentic/03_tool_using_agent/#debugging-and-extension-challenges","title":"Debugging and Extension Challenges","text":""},{"location":"agentic/03_tool_using_agent/#common-issues","title":"Common Issues","text":"<ul> <li>JSON parsing errors from malformed LLM responses</li> <li>Tool selection errors (choosing wrong tool or using unnecessarily)</li> <li>Tool input formatting problems</li> <li>Failure to properly incorporate tool results into final answers</li> </ul>"},{"location":"agentic/03_tool_using_agent/#extension-ideas","title":"Extension Ideas","text":"<ol> <li> <p>Tool Chain Execution</p> </li> <li> <p>Allow the agent to use multiple tools in sequence</p> </li> <li> <p>Implement a framework for passing outputs between tools</p> </li> <li> <p>Dynamic Tool Discovery</p> </li> <li> <p>Add the ability to register new tools at runtime</p> </li> <li> <p>Implement a tool recommendation system</p> </li> <li> <p>Tool Result Validation</p> </li> <li> <p>Add checks to validate tool outputs</p> </li> <li> <p>Implement fallback strategies for tool failures</p> </li> <li> <p>Specialized Tool Prompt</p> </li> <li>Create custom prompts for specific tools</li> <li>Optimize tool result interpretation</li> </ol>"},{"location":"agentic/03_tool_using_agent/#agent-evaluation","title":"Agent Evaluation","text":""},{"location":"agentic/03_tool_using_agent/#effectiveness-metrics","title":"Effectiveness Metrics","text":"<ul> <li>Appropriate tool selection rate</li> <li>Tool usage success rate</li> <li>Answer accuracy with tools vs. without</li> <li>Problem-solving versatility</li> </ul>"},{"location":"agentic/03_tool_using_agent/#efficiency-metrics","title":"Efficiency Metrics","text":"<ul> <li>Number of tool calls needed</li> <li>Response time (including tool execution)</li> <li>Token usage</li> <li>Success rate within turn limit</li> </ul>"},{"location":"agentic/03_tool_using_agent/#alignment-metrics","title":"Alignment Metrics","text":"<ul> <li>Transparency about tool usage</li> <li>Accurate representation of tool capabilities</li> <li>Proper attribution of information sources</li> <li>Error handling quality</li> </ul>"},{"location":"agentic/03_tool_using_agent/#evaluation-exercise","title":"Evaluation Exercise","text":"<p>Create a test suite of 5-10 queries specifically designed to test boundary cases in tool selection. Record when the agent:</p> <ol> <li>Uses a tool correctly when needed</li> <li>Correctly answers without tools when possible</li> <li>Uses a tool unnecessarily</li> <li>Fails to use a tool when one would help</li> </ol>"},{"location":"agentic/03_tool_using_agent/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>How does tool usage parallel how humans use external resources?</li> <li>What cognitive biases might affect an agent's tool selection?</li> <li>How might you design an agent that learns which tools are most effective for different tasks?</li> <li>What are the ethical considerations when using external tools (e.g., search, calculators)?</li> <li>How would you design a system for the agent to request new tools when needed?</li> <li>What are the security implications of allowing agents to use external tools?</li> </ol>"},{"location":"agentic/03_tool_using_agent/#next-steps","title":"Next Steps","text":"<ul> <li>Explore how memory can help agents learn which tools work best</li> <li>Investigate how planning can improve multi-step tool usage</li> <li>Consider how multiple agents might share and coordinate tool usage</li> </ul>"},{"location":"agentic/04_memory_augmented_agent/","title":"Memory-Augmented Agent: Persistent Context Across Interactions","text":""},{"location":"agentic/04_memory_augmented_agent/#objective","title":"Objective","text":"<p>Build an agent capable of maintaining context across multiple interactions by implementing memory systems, allowing for more natural conversations and incremental problem solving.</p>"},{"location":"agentic/04_memory_augmented_agent/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Create and activate a virtual environment (if not already done) 2. Install dependencies (including memory-specific ones) 3. Configure API access <pre><code># Create a virtual environment (if not already created)\npython -m venv agentic-env\nsource agentic-env/bin/activate  # On Windows: agentic-env\\Scripts\\activate\n\n# Install dependencies\npip install openai langchain pydantic python-dotenv chromadb tiktoken\n\n# Create a .env file for your API key (if not already done)\necho \"OPENAI_API_KEY=your_api_key_here\" &gt; .env\n</code></pre>"},{"location":"agentic/04_memory_augmented_agent/#verifying-setup","title":"Verifying Setup","text":"<p>Run the following test code to ensure your environment is ready:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nimport chromadb\n\n# Load environment variables\nload_dotenv()\n\n# Test API connection\nllm = OpenAI(temperature=0)\nresult = llm.invoke(\"Hello!\")\nprint(f\"LLM Response: {result}\")\n\n# Test embedding creation\nembeddings = OpenAIEmbeddings()\ntest_embedding = embeddings.embed_query(\"Test query\")\nprint(f\"Embedding length: {len(test_embedding)}\")\n\n# Test ChromaDB\nclient = chromadb.Client()\nprint(\"ChromaDB connection successful\")\n</code></pre>"},{"location":"agentic/04_memory_augmented_agent/#core-concept-memory-augmented-agents","title":"Core Concept: Memory-Augmented Agents","text":"<p>Memory-augmented agents enhance capabilities through:</p> <ol> <li>Short-term memory - Maintaining immediate context within a conversation</li> <li>Long-term memory - Storing and retrieving information across separate sessions</li> <li>Memory organization - Structuring knowledge for efficient retrieval</li> <li>Memory-aware reasoning - Using past experiences to inform decisions</li> </ol> <p>Key characteristics:</p> <ul> <li>Contextual - Maintains conversational thread across turns</li> <li>Personalized - Remembers user preferences and past interactions</li> <li>Progressive - Builds knowledge over time</li> <li>Adaptive - Learns from past experiences</li> </ul>"},{"location":"agentic/04_memory_augmented_agent/#memory-types","title":"Memory Types","text":"<ul> <li>Conversation memory - Recent dialogue history</li> <li>Vector memory - Semantic similarity-based retrieval</li> <li>Episodic memory - Time-ordered sequence of interactions</li> <li>Declarative memory - Explicit facts and information</li> </ul>"},{"location":"agentic/04_memory_augmented_agent/#real-world-examples","title":"Real-world Examples","text":"<ul> <li>Personal assistants remembering preferences</li> <li>Customer service bots with conversation history</li> <li>Educational tutors tracking student progress</li> <li>Smart home systems adapting to user habits</li> </ul>"},{"location":"agentic/04_memory_augmented_agent/#modular-code-implementation","title":"Modular Code Implementation","text":""},{"location":"agentic/04_memory_augmented_agent/#1-memory-systems","title":"1. Memory Systems","text":"<pre><code>import os\nimport json\nimport time\nfrom typing import Dict, List, Any, Optional\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.embeddings import OpenAIEmbeddings\nimport chromadb\n\nclass ConversationMemory:\n    \"\"\"Simple buffer-based memory that stores recent conversation turns.\"\"\"\n\n    def __init__(self, max_turns: int = 10):\n        \"\"\"Initialize the conversation memory with a maximum number of turns.\"\"\"\n        self.max_turns = max_turns\n        self.buffer = []\n\n    def add(self, role: str, content: str):\n        \"\"\"Add a new message to the conversation memory.\"\"\"\n        self.buffer.append({\"role\": role, \"content\": content, \"timestamp\": time.time()})\n        # Trim the buffer if it exceeds the maximum size\n        if len(self.buffer) &gt; self.max_turns:\n            self.buffer = self.buffer[-self.max_turns:]\n\n    def get(self) -&gt; List[Dict]:\n        \"\"\"Get the conversation history.\"\"\"\n        return self.buffer\n\n    def get_formatted(self) -&gt; str:\n        \"\"\"Get the conversation history as a formatted string.\"\"\"\n        formatted = []\n        for message in self.buffer:\n            formatted.append(f\"{message['role'].capitalize()}: {message['content']}\")\n        return \"\\n\".join(formatted)\n\n    def clear(self):\n        \"\"\"Clear the conversation memory.\"\"\"\n        self.buffer = []\n\nclass VectorMemory:\n    \"\"\"Vector-based memory system for semantic retrieval of information.\"\"\"\n\n    def __init__(self, collection_name: str = \"agent_memory\"):\n        \"\"\"Initialize the vector memory with a collection name.\"\"\"\n        load_dotenv()\n        self.embeddings = OpenAIEmbeddings()\n        self.client = chromadb.Client()\n\n        # Create or get the collection\n        try:\n            self.collection = self.client.get_or_create_collection(collection_name)\n        except:\n            # If the collection exists but with different settings, recreate it\n            self.client.delete_collection(collection_name)\n            self.collection = self.client.create_collection(collection_name)\n\n        self.next_id = 1\n\n    def add(self, content: str, metadata: Optional[Dict] = None):\n        \"\"\"Add a new memory to the vector store.\"\"\"\n        if metadata is None:\n            metadata = {}\n\n        # Add timestamp if not present\n        if \"timestamp\" not in metadata:\n            metadata[\"timestamp\"] = time.time()\n\n        # Convert content to embedding and store\n        embedding = self.embeddings.embed_query(content)\n\n        # Store in ChromaDB\n        self.collection.add(\n            ids=[f\"mem_{self.next_id}\"],\n            embeddings=[embedding],\n            documents=[content],\n            metadatas=[metadata]\n        )\n\n        self.next_id += 1\n\n    def retrieve(self, query: str, k: int = 3) -&gt; List[Dict]:\n        \"\"\"Retrieve the k most relevant memories for a given query.\"\"\"\n        # Convert query to embedding\n        query_embedding = self.embeddings.embed_query(query)\n\n        # Query the collection\n        results = self.collection.query(\n            query_embeddings=[query_embedding],\n            n_results=k\n        )\n\n        # Format the results\n        memories = []\n        if results[\"documents\"] and len(results[\"documents\"]) &gt; 0:\n            for i, doc in enumerate(results[\"documents\"][0]):\n                memories.append({\n                    \"content\": doc,\n                    \"metadata\": results[\"metadatas\"][0][i] if i &lt; len(results[\"metadatas\"][0]) else {}\n                })\n\n        return memories\n\n    def clear(self):\n        \"\"\"Clear all memories from the collection.\"\"\"\n        self.collection.delete(ids=self.collection.get()[\"ids\"])\n        self.next_id = 1\n\n# YOUR GROUP TASK: Implement at least one additional memory system\n# Examples: Structured knowledge base, Episodic memory, etc.\n</code></pre>"},{"location":"agentic/04_memory_augmented_agent/#2-agent-design","title":"2. Agent Design","text":"<pre><code>class MemoryAugmentedAgent:\n    \"\"\"An agent that uses memory systems to maintain context across interactions.\"\"\"\n\n    def __init__(self, llm=None):\n        \"\"\"Initialize the memory-augmented agent with memory systems and an optional language model.\"\"\"\n        load_dotenv()\n        self.llm = llm or OpenAI(temperature=0.7)\n\n        # Initialize memory systems\n        self.conversation_memory = ConversationMemory()\n        self.vector_memory = VectorMemory()\n\n        # Define the prompt template\n        self.prompt_template = PromptTemplate(\n            input_variables=[\"input\", \"conversation_history\", \"relevant_memories\"],\n            template=\"\"\"\n            You are a helpful assistant with both conversation memory and long-term memory.\n\n            Conversation history:\n            {conversation_history}\n\n            Relevant information from your long-term memory:\n            {relevant_memories}\n\n            User's current input: {input}\n\n            Respond to the user's current input, taking into account both the conversation history\n            and any relevant information from your long-term memory. If you learn any new important\n            information that should be remembered, note it with [REMEMBER: information to remember].\n            \"\"\"\n        )\n\n    def _format_memories(self, memories: List[Dict]) -&gt; str:\n        \"\"\"Format retrieved memories for inclusion in the prompt.\"\"\"\n        # YOUR CODE: Format the memories for the prompt\n        if not memories:\n            return \"No relevant memories found.\"\n\n        result = []\n        for i, memory in enumerate(memories):\n            timestamp = memory[\"metadata\"].get(\"timestamp\", \"Unknown time\")\n            if isinstance(timestamp, (int, float)):\n                from datetime import datetime\n                timestamp = datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n            result.append(f\"Memory {i+1} [{timestamp}]: {memory['content']}\")\n\n        return \"\\n\".join(result)\n\n    def _extract_memories(self, response: str) -&gt; List[str]:\n        \"\"\"Extract new memories to store from the agent's response.\"\"\"\n        # YOUR CODE: Extract information marked for remembering\n        import re\n        memory_pattern = r\"\\[REMEMBER: (.*?)\\]\"\n        return re.findall(memory_pattern, response)\n\n    def process(self, input_text: str) -&gt; str:\n        \"\"\"Process the user input using memory systems and generate a response.\"\"\"\n        # YOUR CODE: Implement the main agent processing logic\n\n        # Retrieve relevant memories\n        relevant_memories = self.vector_memory.retrieve(input_text)\n        formatted_memories = self._format_memories(relevant_memories)\n\n        # Get conversation history\n        conversation_history = self.conversation_memory.get_formatted()\n\n        # Generate response\n        prompt = self.prompt_template.format(\n            input=input_text,\n            conversation_history=conversation_history if conversation_history else \"No conversation history yet.\",\n            relevant_memories=formatted_memories\n        )\n\n        response = self.llm.invoke(prompt)\n\n        # Extract and store new memories\n        new_memories = self._extract_memories(response)\n        for memory in new_memories:\n            self.vector_memory.add(memory, {\"source\": \"conversation\"})\n\n        # Clean the response by removing memory markers\n        clean_response = response.replace(\"[REMEMBER: \", \"[Noted: \").replace(\"]\", \"]\")\n\n        # Update conversation memory\n        self.conversation_memory.add(\"user\", input_text)\n        self.conversation_memory.add(\"assistant\", clean_response)\n\n        return clean_response\n</code></pre>"},{"location":"agentic/04_memory_augmented_agent/#3-test-harness","title":"3. Test Harness","text":"<pre><code>def test_memory_agent():\n    \"\"\"Test the memory-augmented agent with a multi-turn conversation.\"\"\"\n    agent = MemoryAugmentedAgent()\n\n    # Pre-populate some memories (optional)\n    agent.vector_memory.add(\"The user's name is Alex.\", {\"type\": \"user_info\"})\n    agent.vector_memory.add(\"Alex likes hiking and photography.\", {\"type\": \"user_preference\"})\n    agent.vector_memory.add(\"Today's weather is sunny with a high of 75\u00b0F.\", {\"type\": \"environment\"})\n\n    # Test conversation\n    conversation = [\n        \"Hi there! How are you today?\",\n        \"Can you tell me what you remember about me?\",\n        \"I recently went on a trip to Japan. It was amazing!\",\n        \"What was my favorite activity again?\",\n        \"I'm planning another trip soon. Any suggestions based on what I like?\"\n    ]\n\n    print(\"=== Testing Memory-Augmented Agent ===\\n\")\n\n    for i, user_input in enumerate(conversation):\n        print(f\"Turn {i+1}:\")\n        print(f\"User: {user_input}\")\n        response = agent.process(user_input)\n        print(f\"Agent: {response}\\n\")\n\n        # GROUP DISCUSSION POINT: How does the agent's memory affect its responses?\n        # What information is being remembered correctly? What is being forgotten?\n\nif __name__ == \"__main__\":\n    test_memory_agent()\n</code></pre>"},{"location":"agentic/04_memory_augmented_agent/#live-coding-collaboration","title":"Live Coding Collaboration","text":""},{"location":"agentic/04_memory_augmented_agent/#session-structure-60-75-minutes","title":"Session Structure (60-75 minutes)","text":"<ol> <li> <p>Setup &amp; Memory Design (15 min)</p> </li> <li> <p>Configure environment</p> </li> <li>Understand memory systems</li> <li> <p>Plan custom memory implementation</p> </li> <li> <p>Implementation Phase (25 min)</p> </li> <li> <p>Complete the agent's memory processing methods</p> </li> <li>Implement memory extraction and formatting</li> <li> <p>Create test scenarios with multi-turn conversations</p> </li> <li> <p>Testing &amp; Analysis (20 min)</p> </li> <li> <p>Test with varied conversation flows</p> </li> <li>Analyze memory retrieval effectiveness</li> <li> <p>Evaluate response quality with memory vs. without memory</p> </li> <li> <p>Extension &amp; Refinement (15 min)</p> </li> <li>Implement more sophisticated memory prioritization</li> <li>Add memory decay or importance weighting</li> <li>Create a custom memory type for specific information</li> </ol>"},{"location":"agentic/04_memory_augmented_agent/#reflection-checkpoints","title":"Reflection Checkpoints","text":""},{"location":"agentic/04_memory_augmented_agent/#checkpoint-1-after-memory-system-implementation","title":"Checkpoint 1: After Memory System Implementation","text":"<ul> <li>What types of information should be stored in each memory system?</li> <li>How should memories be organized for effective retrieval?</li> <li>What metadata is important to track for each memory?</li> <li>How might different memory structures affect agent behavior?</li> </ul>"},{"location":"agentic/04_memory_augmented_agent/#checkpoint-2-after-agent-testing","title":"Checkpoint 2: After Agent Testing","text":"<ul> <li>When does the agent effectively retrieve relevant memories?</li> <li>When does it fail to recall important information?</li> <li>How does conversation quality change as memory accumulates?</li> <li>What patterns of forgetting or misremembering do you observe?</li> </ul>"},{"location":"agentic/04_memory_augmented_agent/#debugging-and-extension-challenges","title":"Debugging and Extension Challenges","text":""},{"location":"agentic/04_memory_augmented_agent/#common-issues","title":"Common Issues","text":"<ul> <li>Memory retrieval misses relevant information</li> <li>Memory overload (too many irrelevant memories)</li> <li>Temporal confusion (mixing up when information was learned)</li> <li>Contradictory memories causing inconsistent responses</li> </ul>"},{"location":"agentic/04_memory_augmented_agent/#extension-ideas","title":"Extension Ideas","text":"<ol> <li> <p>Memory Forgetting</p> </li> <li> <p>Implement memory decay based on time</p> </li> <li>Add importance scoring to prioritize memories</li> <li> <p>Create a memory consolidation process</p> </li> <li> <p>Structured Knowledge</p> </li> <li> <p>Implement entity-relationship based memory</p> </li> <li>Add schema validation for memory objects</li> <li> <p>Create domain-specific memory structures</p> </li> <li> <p>Active Memory Management</p> </li> <li> <p>Add memory summarization to compress information</p> </li> <li>Implement memory contradiction detection</li> <li> <p>Create memory reconstruction capabilities</p> </li> <li> <p>Memory Visualization</p> </li> <li>Create a timeline view of memories</li> <li>Implement knowledge graph visualization</li> <li>Add memory influence tracing</li> </ol>"},{"location":"agentic/04_memory_augmented_agent/#agent-evaluation","title":"Agent Evaluation","text":""},{"location":"agentic/04_memory_augmented_agent/#effectiveness-metrics","title":"Effectiveness Metrics","text":"<ul> <li>Information recall accuracy</li> <li>Contextual relevance of responses</li> <li>Conversation coherence across turns</li> <li>Memory relevance to current query</li> </ul>"},{"location":"agentic/04_memory_augmented_agent/#efficiency-metrics","title":"Efficiency Metrics","text":"<ul> <li>Response time as memory grows</li> <li>Memory storage requirements</li> <li>Retrieval precision and recall</li> <li>Context window utilization</li> </ul>"},{"location":"agentic/04_memory_augmented_agent/#alignment-metrics","title":"Alignment Metrics","text":"<ul> <li>Consistency in factual responses</li> <li>Appropriate recall of personal information</li> <li>Privacy considerations in memory storage</li> <li>Transparency about remembered information</li> </ul>"},{"location":"agentic/04_memory_augmented_agent/#evaluation-exercise","title":"Evaluation Exercise","text":"<p>Design a challenging multi-turn conversation scenario that tests the agent's ability to:</p> <ol> <li>Remember factual information provided by the user</li> <li>Recall preferences and apply them in recommendations</li> <li>Maintain conversational context over 5+ turns</li> <li>Recognize and resolve contradictions in information</li> </ol>"},{"location":"agentic/04_memory_augmented_agent/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>How does memory augmentation parallel human memory processes?</li> <li>What are the ethical implications of agents remembering personal information?</li> <li>How might different memory architectures affect agent personality and behavior?</li> <li>What are the tradeoffs between different memory systems (vector vs. conversation vs. structured)?</li> <li>How would you design memory systems for an agent that needs to learn and evolve over time?</li> <li>What privacy and security considerations exist for memory-augmented agents?</li> </ol>"},{"location":"agentic/04_memory_augmented_agent/#next-steps","title":"Next Steps","text":"<ul> <li>Explore how planning can use memory to improve future interactions</li> <li>Investigate how tool usage can be enhanced with memory of past effectiveness</li> <li>Consider how multiple agents might share and build collective memory</li> </ul>"},{"location":"agentic/05_multi_agent_collaboration/","title":"Multi-Agent Collaboration: Coordinated Problem Solving","text":""},{"location":"agentic/05_multi_agent_collaboration/#objective","title":"Objective","text":"<p>Build a system of multiple specialized agents that coordinate to solve complex problems, demonstrating how different agent roles, communication protocols, and collaborative frameworks can enhance collective intelligence.</p>"},{"location":"agentic/05_multi_agent_collaboration/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Create and activate a virtual environment (if not already done) 2. Install dependencies 3. Configure API access <pre><code># Create a virtual environment (if not already created)\npython -m venv agentic-env\nsource agentic-env/bin/activate  # On Windows: agentic-env\\Scripts\\activate\n\n# Install dependencies\npip install openai langchain pydantic python-dotenv asyncio\n\n# Create a .env file for your API key (if not already done)\necho \"OPENAI_API_KEY=your_api_key_here\" &gt; .env\n</code></pre>"},{"location":"agentic/05_multi_agent_collaboration/#verifying-setup","title":"Verifying Setup","text":"<p>Run the following test code to ensure your environment is ready:</p> <pre><code>import os\nimport asyncio\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\n\n# Load environment variables\nload_dotenv()\n\n# Test API connection\nllm = OpenAI(temperature=0)\nresult = llm.invoke(\"Hello!\")\nprint(f\"LLM Response: {result}\")\n\n# Test async capabilities\nasync def test_async():\n    return \"Async is working!\"\n\nresult = asyncio.run(test_async())\nprint(result)\n</code></pre>"},{"location":"agentic/05_multi_agent_collaboration/#core-concept-multi-agent-collaboration","title":"Core Concept: Multi-Agent Collaboration","text":"<p>Multi-agent systems enhance problem-solving through:</p> <ol> <li>Role specialization - Agents focus on specific aspects of a problem</li> <li>Parallel processing - Multiple agents work simultaneously</li> <li>Diverse perspectives - Different agents approach problems differently</li> <li>Collective intelligence - The system outperforms any individual agent</li> </ol> <p>Key characteristics:</p> <ul> <li>Coordinated - Agents work together toward common goals</li> <li>Communicative - Information flows between agents</li> <li>Modular - Specialized agents can be added or removed</li> <li>Scalable - Systems can grow as problems become more complex</li> </ul>"},{"location":"agentic/05_multi_agent_collaboration/#collaboration-patterns","title":"Collaboration Patterns","text":"<ul> <li>Manager-Worker - Central agent delegates tasks to specialists</li> <li>Peer-to-Peer - Agents communicate directly with each other</li> <li>Blackboard - Agents share information through a common workspace</li> <li>Market-based - Agents bid for tasks based on capabilities</li> </ul>"},{"location":"agentic/05_multi_agent_collaboration/#real-world-examples","title":"Real-world Examples","text":"<ul> <li>Distributed task management systems</li> <li>Multi-disciplinary research teams</li> <li>Supply chain optimization</li> <li>Emergency response coordination</li> </ul>"},{"location":"agentic/05_multi_agent_collaboration/#modular-code-implementation","title":"Modular Code Implementation","text":""},{"location":"agentic/05_multi_agent_collaboration/#1-agent-base-class-and-message-system","title":"1. Agent Base Class and Message System","text":"<pre><code>import os\nimport json\nimport uuid\nimport asyncio\nfrom typing import Dict, List, Any, Optional, Callable\nfrom enum import Enum\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nclass MessageType(Enum):\n    \"\"\"Types of messages that can be exchanged between agents.\"\"\"\n    TASK = \"task\"\n    RESULT = \"result\"\n    QUESTION = \"question\"\n    ANSWER = \"answer\"\n    STATUS = \"status\"\n    FINAL = \"final\"\n\nclass Message:\n    \"\"\"A message that can be sent between agents.\"\"\"\n\n    def __init__(self, sender: str, receiver: str, content: Any, msg_type: MessageType):\n        \"\"\"Initialize a message with sender, receiver, content, and type.\"\"\"\n        self.id = str(uuid.uuid4())\n        self.sender = sender\n        self.receiver = receiver\n        self.content = content\n        self.type = msg_type\n        self.timestamp = asyncio.get_event_loop().time()\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"Convert the message to a dictionary.\"\"\"\n        return {\n            \"id\": self.id,\n            \"sender\": self.sender,\n            \"receiver\": self.receiver,\n            \"content\": self.content,\n            \"type\": self.type.value,\n            \"timestamp\": self.timestamp\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict) -&gt; 'Message':\n        \"\"\"Create a message from a dictionary.\"\"\"\n        return cls(\n            sender=data[\"sender\"],\n            receiver=data[\"receiver\"],\n            content=data[\"content\"],\n            msg_type=MessageType(data[\"type\"])\n        )\n\nclass Blackboard:\n    \"\"\"A shared workspace where agents can read and write information.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize an empty blackboard.\"\"\"\n        self.data = {}\n        self.message_history = []\n        self._subscribers = {}\n\n    def write(self, key: str, value: Any, author: str):\n        \"\"\"Write data to the blackboard.\"\"\"\n        self.data[key] = {\"value\": value, \"author\": author, \"timestamp\": asyncio.get_event_loop().time()}\n\n    def read(self, key: str) -&gt; Optional[Any]:\n        \"\"\"Read data from the blackboard.\"\"\"\n        entry = self.data.get(key)\n        return entry[\"value\"] if entry else None\n\n    def get_all(self) -&gt; Dict:\n        \"\"\"Get all data from the blackboard.\"\"\"\n        return {k: v[\"value\"] for k, v in self.data.items()}\n\n    def post_message(self, message: Message):\n        \"\"\"Post a message to the blackboard and notify subscribers.\"\"\"\n        self.message_history.append(message)\n\n        # Notify subscribers interested in this message\n        receiver = message.receiver\n        if receiver in self._subscribers:\n            for callback in self._subscribers[receiver]:\n                asyncio.create_task(callback(message))\n\n        # Also notify subscribers interested in ALL messages\n        if \"*\" in self._subscribers:\n            for callback in self._subscribers[\"*\"]:\n                asyncio.create_task(callback(message))\n\n    def subscribe(self, agent_id: str, callback: Callable):\n        \"\"\"Subscribe to messages for a specific agent or all messages.\"\"\"\n        if agent_id not in self._subscribers:\n            self._subscribers[agent_id] = []\n        self._subscribers[agent_id].append(callback)\n\n    def get_messages(self, agent_id: str = None, msg_type: MessageType = None) -&gt; List[Message]:\n        \"\"\"Get messages filtered by agent ID and/or message type.\"\"\"\n        filtered = self.message_history\n\n        if agent_id:\n            filtered = [msg for msg in filtered if msg.receiver == agent_id or msg.sender == agent_id]\n\n        if msg_type:\n            filtered = [msg for msg in filtered if msg.type == msg_type]\n\n        return filtered\n\nclass Agent:\n    \"\"\"Base class for all agents in the multi-agent system.\"\"\"\n\n    def __init__(self, agent_id: str, name: str, role: str, blackboard: Blackboard, llm=None):\n        \"\"\"Initialize an agent with ID, name, role, and a reference to the shared blackboard.\"\"\"\n        load_dotenv()\n        self.agent_id = agent_id\n        self.name = name\n        self.role = role\n        self.blackboard = blackboard\n        self.llm = llm or OpenAI(temperature=0.7)\n        self.prompt_template = self._create_prompt_template()\n\n        # Subscribe to messages addressed to this agent\n        self.blackboard.subscribe(self.agent_id, self.handle_message)\n\n    def _create_prompt_template(self) -&gt; PromptTemplate:\n        \"\"\"Create the prompt template for this agent. Override in subclasses.\"\"\"\n        return PromptTemplate(\n            input_variables=[\"input\", \"role\", \"context\"],\n            template=\"\"\"\n            You are a {role}.\n\n            Context information:\n            {context}\n\n            Please process the following input:\n            {input}\n            \"\"\"\n        )\n\n    async def process(self, input_text: str, context: str = \"\") -&gt; str:\n        \"\"\"Process input and generate a response based on the agent's role.\"\"\"\n        prompt = self.prompt_template.format(\n            input=input_text,\n            role=self.role,\n            context=context\n        )\n\n        response = self.llm.invoke(prompt)\n        return response\n\n    async def handle_message(self, message: Message):\n        \"\"\"Handle a message sent to this agent. Override in subclasses.\"\"\"\n        if message.receiver != self.agent_id and message.receiver != \"*\":\n            return  # Message not for this agent\n\n        print(f\"{self.name} received message: {message.type.value} from {message.sender}\")\n\n        # Default implementation just processes the message content\n        if message.type == MessageType.TASK:\n            result = await self.process(message.content)\n\n            # Send back a result message\n            response = Message(\n                sender=self.agent_id,\n                receiver=message.sender,\n                content=result,\n                msg_type=MessageType.RESULT\n            )\n            self.blackboard.post_message(response)\n\n    def send_message(self, receiver: str, content: Any, msg_type: MessageType):\n        \"\"\"Send a message to another agent via the blackboard.\"\"\"\n        message = Message(\n            sender=self.agent_id,\n            receiver=receiver,\n            content=content,\n            msg_type=msg_type\n        )\n        self.blackboard.post_message(message)\n</code></pre>"},{"location":"agentic/05_multi_agent_collaboration/#2-specialized-agents","title":"2. Specialized Agents","text":"<pre><code>class Manager(Agent):\n    \"\"\"Manager agent that coordinates the work of other agents.\"\"\"\n\n    def __init__(self, agent_id: str, name: str, blackboard: Blackboard, worker_ids: List[str], llm=None):\n        \"\"\"Initialize a manager agent with a list of worker agent IDs.\"\"\"\n        super().__init__(agent_id, name, \"Task Manager\", blackboard, llm)\n        self.worker_ids = worker_ids\n        self.pending_tasks = {}\n        self.results = {}\n\n    def _create_prompt_template(self) -&gt; PromptTemplate:\n        \"\"\"Create the prompt template for the manager agent.\"\"\"\n        return PromptTemplate(\n            input_variables=[\"input\", \"workers\", \"context\"],\n            template=\"\"\"\n            You are a Task Manager who coordinates multiple specialist workers.\n\n            Your available workers are:\n            {workers}\n\n            Context information:\n            {context}\n\n            Based on this complex task, break it down into smaller tasks that can be assigned\n            to your workers. Consider their specialties when making assignments.\n\n            Task: {input}\n\n            Respond with a JSON object containing task assignments:\n            {{\n                \"analysis\": \"Your analysis of the overall task\",\n                \"tasks\": [\n                    {{\n                        \"worker_id\": \"ID of the worker\",\n                        \"task_description\": \"Detailed description of the subtask\"\n                    }}\n                ]\n            }}\n            \"\"\"\n        )\n\n    async def process_task(self, task: str, context: str = \"\"):\n        \"\"\"Process a complex task by breaking it down and assigning subtasks to workers.\"\"\"\n        # Format worker information for the prompt\n        worker_info = \"\\n\".join([f\"- Worker ID: {worker_id}\" for worker_id in self.worker_ids])\n\n        # Generate the task breakdown\n        prompt = self.prompt_template.format(\n            input=task,\n            workers=worker_info,\n            context=context\n        )\n\n        response = self.llm.invoke(prompt)\n\n        # Parse the response and assign tasks\n        try:\n            task_data = json.loads(response)\n            print(f\"Task Analysis: {task_data['analysis']}\")\n\n            for subtask in task_data[\"tasks\"]:\n                worker_id = subtask[\"worker_id\"]\n                task_description = subtask[\"task_description\"]\n\n                # Track the pending task\n                task_id = str(uuid.uuid4())\n                self.pending_tasks[task_id] = {\n                    \"worker_id\": worker_id,\n                    \"description\": task_description,\n                    \"status\": \"assigned\"\n                }\n\n                # Assign the task to the worker\n                self.send_message(\n                    receiver=worker_id,\n                    content={\"task_id\": task_id, \"description\": task_description},\n                    msg_type=MessageType.TASK\n                )\n\n                print(f\"Assigned task to {worker_id}: {task_description}\")\n\n            # Wait for all tasks to complete\n            while any(task[\"status\"] == \"assigned\" for task in self.pending_tasks.values()):\n                await asyncio.sleep(0.1)\n\n            # Compile the results\n            final_result = await self.compile_results(task, task_data[\"analysis\"])\n            return final_result\n\n        except json.JSONDecodeError:\n            return \"Error: Failed to parse task assignments.\"\n\n    async def handle_message(self, message: Message):\n        \"\"\"Handle messages sent to the manager agent.\"\"\"\n        if message.type == MessageType.RESULT:\n            # Handle task results from workers\n            result = message.content\n            task_id = result.get(\"task_id\")\n\n            if task_id in self.pending_tasks:\n                self.pending_tasks[task_id][\"status\"] = \"completed\"\n                self.results[task_id] = result.get(\"result\")\n                print(f\"Received result from {message.sender} for task {task_id}\")\n\n    async def compile_results(self, original_task: str, analysis: str) -&gt; str:\n        \"\"\"Compile all worker results into a final answer.\"\"\"\n        # Create a prompt to compile the results\n        results_str = \"\\n\".join([\n            f\"Task: {self.pending_tasks[task_id]['description']}\\nResult: {result}\"\n            for task_id, result in self.results.items()\n        ])\n\n        compile_prompt = f\"\"\"\n        You are a Task Manager compiling results from multiple workers.\n\n        Original Task: {original_task}\n\n        Task Analysis: {analysis}\n\n        Worker Results:\n        {results_str}\n\n        Please compile these results into a comprehensive final answer that addresses the original task.\n        Ensure your response is well-structured and integrates all the information provided by the workers.\n        \"\"\"\n\n        final_result = self.llm.invoke(compile_prompt)\n        return final_result\n\nclass Researcher(Agent):\n    \"\"\"Agent specialized in gathering and analyzing information.\"\"\"\n\n    def __init__(self, agent_id: str, name: str, blackboard: Blackboard, llm=None):\n        \"\"\"Initialize a researcher agent.\"\"\"\n        super().__init__(agent_id, name, \"Research Specialist\", blackboard, llm)\n\n    def _create_prompt_template(self) -&gt; PromptTemplate:\n        \"\"\"Create the prompt template for the researcher agent.\"\"\"\n        return PromptTemplate(\n            input_variables=[\"input\", \"context\"],\n            template=\"\"\"\n            You are a Research Specialist who excels at gathering and analyzing information.\n\n            Context information:\n            {context}\n\n            Research task: {input}\n\n            Conduct thorough research on this topic. Focus on:\n            1. Key facts and data\n            2. Different perspectives or approaches\n            3. Relevant background information\n            4. Recent developments or trends\n\n            Provide a comprehensive research report with properly organized findings.\n            \"\"\"\n        )\n\n    async def handle_message(self, message: Message):\n        \"\"\"Handle messages sent to the researcher agent.\"\"\"\n        if message.type == MessageType.TASK:\n            task_data = message.content\n            task_id = task_data[\"task_id\"]\n            description = task_data[\"description\"]\n\n            print(f\"{self.name} researching: {description}\")\n            result = await self.process(description)\n\n            # Send the result back\n            self.send_message(\n                receiver=message.sender,\n                content={\"task_id\": task_id, \"result\": result},\n                msg_type=MessageType.RESULT\n            )\n\nclass Critic(Agent):\n    \"\"\"Agent specialized in critically evaluating ideas and identifying issues.\"\"\"\n\n    def __init__(self, agent_id: str, name: str, blackboard: Blackboard, llm=None):\n        \"\"\"Initialize a critic agent.\"\"\"\n        super().__init__(agent_id, name, \"Critical Evaluator\", blackboard, llm)\n\n    def _create_prompt_template(self) -&gt; PromptTemplate:\n        \"\"\"Create the prompt template for the critic agent.\"\"\"\n        return PromptTemplate(\n            input_variables=[\"input\", \"context\"],\n            template=\"\"\"\n            You are a Critical Evaluator who specializes in analyzing ideas for flaws, limitations,\n            and potential improvements.\n\n            Context information:\n            {context}\n\n            Content to evaluate: {input}\n\n            Critically evaluate this content. Focus on:\n            1. Logical inconsistencies or fallacies\n            2. Factual inaccuracies or missing context\n            3. Potential biases or assumptions\n            4. Limitations of the approach\n            5. Areas for improvement or alternative perspectives\n\n            Provide a balanced critique that acknowledges strengths while identifying areas for improvement.\n            \"\"\"\n        )\n\n    async def handle_message(self, message: Message):\n        \"\"\"Handle messages sent to the critic agent.\"\"\"\n        if message.type == MessageType.TASK:\n            task_data = message.content\n            task_id = task_data[\"task_id\"]\n            description = task_data[\"description\"]\n\n            print(f\"{self.name} evaluating: {description}\")\n            result = await self.process(description)\n\n            # Send the result back\n            self.send_message(\n                receiver=message.sender,\n                content={\"task_id\": task_id, \"result\": result},\n                msg_type=MessageType.RESULT\n            )\n\n# YOUR GROUP TASK: Implement at least one additional specialized agent\n# Examples: Creative Agent, Data Analyst, Writer, Planner, etc.\n</code></pre>"},{"location":"agentic/05_multi_agent_collaboration/#3-multi-agent-system","title":"3. Multi-Agent System","text":"<pre><code>class MultiAgentSystem:\n    \"\"\"A system that coordinates multiple agents to solve complex problems.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the multi-agent system with a shared blackboard.\"\"\"\n        self.blackboard = Blackboard()\n        self.agents = {}\n\n    def add_agent(self, agent: Agent):\n        \"\"\"Add an agent to the system.\"\"\"\n        self.agents[agent.agent_id] = agent\n        print(f\"Added agent: {agent.name} ({agent.role})\")\n\n    async def run(self, task: str) -&gt; str:\n        \"\"\"Run the multi-agent system on a given task.\"\"\"\n        # Find the manager agent\n        manager = next((agent for agent in self.agents.values() if isinstance(agent, Manager)), None)\n\n        if not manager:\n            return \"Error: No manager agent found in the system.\"\n\n        # Process the task\n        result = await manager.process_task(task)\n        return result\n\nasync def main():\n    \"\"\"Set up and run a multi-agent system.\"\"\"\n    # Create the multi-agent system\n    system = MultiAgentSystem()\n\n    # Create and add agents\n    blackboard = system.blackboard\n\n    # Add specialized worker agents\n    researcher = Researcher(\"researcher_1\", \"Alex (Researcher)\", blackboard)\n    critic = Critic(\"critic_1\", \"Taylor (Critic)\", blackboard)\n    # Add your custom agent here\n\n    # Add all worker agents to the system\n    system.add_agent(researcher)\n    system.add_agent(critic)\n    # Add your custom agent to the system\n\n    # Add manager agent (should be added last)\n    manager = Manager(\n        \"manager\",\n        \"Sam (Manager)\",\n        blackboard,\n        worker_ids=[\"researcher_1\", \"critic_1\"]  # Add your custom agent ID here\n    )\n    system.add_agent(manager)\n\n    # Run the system on a task\n    task = \"Evaluate the potential impacts of artificial general intelligence on society in the next decade.\"\n    print(f\"\\nProcessing task: {task}\\n\")\n\n    result = await system.run(task)\n    print(\"\\nFinal Result:\")\n    print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"agentic/05_multi_agent_collaboration/#live-coding-collaboration","title":"Live Coding Collaboration","text":""},{"location":"agentic/05_multi_agent_collaboration/#session-structure-60-75-minutes","title":"Session Structure (60-75 minutes)","text":"<ol> <li> <p>Setup &amp; System Design (15 min)</p> </li> <li> <p>Configure environment</p> </li> <li>Understand agent roles and communication system</li> <li> <p>Plan custom agent implementation</p> </li> <li> <p>Implementation Phase (25 min)</p> </li> <li> <p>Complete the collaborative framework</p> </li> <li>Implement a specialized agent</li> <li> <p>Enhance the manager agent's task assignment logic</p> </li> <li> <p>Testing &amp; Analysis (20 min)</p> </li> <li> <p>Test with complex problems requiring multiple agents</p> </li> <li>Analyze communication patterns</li> <li> <p>Evaluate collaboration effectiveness</p> </li> <li> <p>Extension &amp; Refinement (15 min)</p> </li> <li>Implement an improved task allocation strategy</li> <li>Add more sophisticated agent coordination mechanisms</li> <li>Create specialized roles for specific problem domains</li> </ol>"},{"location":"agentic/05_multi_agent_collaboration/#reflection-checkpoints","title":"Reflection Checkpoints","text":""},{"location":"agentic/05_multi_agent_collaboration/#checkpoint-1-after-agent-implementation","title":"Checkpoint 1: After Agent Implementation","text":"<ul> <li>How does each agent's specialization contribute to the overall system?</li> <li>What communication patterns are most effective for different tasks?</li> <li>What types of tasks benefit most from multi-agent collaboration?</li> <li>How might the system's performance change as more agents are added?</li> </ul>"},{"location":"agentic/05_multi_agent_collaboration/#checkpoint-2-after-system-testing","title":"Checkpoint 2: After System Testing","text":"<ul> <li>How does the system handle task decomposition and reintegration?</li> <li>Where do agents collaborate effectively or struggle to coordinate?</li> <li>How does information flow through the system?</li> <li>What emergent behaviors do you observe in the multi-agent system?</li> </ul>"},{"location":"agentic/05_multi_agent_collaboration/#debugging-and-extension-challenges","title":"Debugging and Extension Challenges","text":""},{"location":"agentic/05_multi_agent_collaboration/#common-issues","title":"Common Issues","text":"<ul> <li>Task allocation imbalances</li> <li>Communication bottlenecks</li> <li>Result integration challenges</li> <li>Circular dependencies between agents</li> <li>Response time variations</li> </ul>"},{"location":"agentic/05_multi_agent_collaboration/#extension-ideas","title":"Extension Ideas","text":"<ol> <li> <p>Dynamic Task Allocation</p> </li> <li> <p>Implement agent capability advertising</p> </li> <li>Create a bidding system for task assignment</li> <li> <p>Add priority-based task scheduling</p> </li> <li> <p>Advanced Communication Protocols</p> </li> <li> <p>Implement request-response patterns</p> </li> <li>Add broadcast messaging capabilities</li> <li> <p>Create agent status notification system</p> </li> <li> <p>Conflict Resolution</p> </li> <li> <p>Add voting mechanisms for decision making</p> </li> <li>Implement negotiation protocols</li> <li> <p>Create a consensus-building process</p> </li> <li> <p>System Monitoring</p> </li> <li>Add performance metrics tracking</li> <li>Create a visualization of agent interactions</li> <li>Implement system health monitoring</li> </ol>"},{"location":"agentic/05_multi_agent_collaboration/#agent-evaluation","title":"Agent Evaluation","text":""},{"location":"agentic/05_multi_agent_collaboration/#effectiveness-metrics","title":"Effectiveness Metrics","text":"<ul> <li>Task completion quality</li> <li>Appropriate specialization usage</li> <li>Knowledge integration</li> <li>Problem-solving versatility</li> </ul>"},{"location":"agentic/05_multi_agent_collaboration/#efficiency-metrics","title":"Efficiency Metrics","text":"<ul> <li>End-to-end processing time</li> <li>Inter-agent communication volume</li> <li>Agent utilization balance</li> <li>Resource consumption</li> </ul>"},{"location":"agentic/05_multi_agent_collaboration/#alignment-metrics","title":"Alignment Metrics","text":"<ul> <li>Consistent goal pursuit across agents</li> <li>Coordination effectiveness</li> <li>System robustness to failures</li> <li>Appropriate task decomposition</li> </ul>"},{"location":"agentic/05_multi_agent_collaboration/#evaluation-exercise","title":"Evaluation Exercise","text":"<p>Design a complex problem that requires diverse expertise and analyze:</p> <ol> <li>How the manager decomposes the problem</li> <li>The quality of specialized contributions</li> <li>The effectiveness of information sharing</li> <li>The coherence of the final integrated solution</li> </ol>"},{"location":"agentic/05_multi_agent_collaboration/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>How does multi-agent collaboration mirror human team dynamics?</li> <li>What cognitive biases might affect different specialized agents?</li> <li>How might you implement learning across the multi-agent system?</li> <li>What are the tradeoffs between centralized vs. decentralized coordination?</li> <li>How would you design the system to handle disagreements between agents?</li> <li>What ethical considerations arise when multiple agents collaborate?</li> </ol>"},{"location":"agentic/05_multi_agent_collaboration/#next-steps","title":"Next Steps","text":"<ul> <li>Explore how self-reflection can improve agent performance within a team</li> <li>Investigate how agent teams can learn from past collaborations</li> <li>Consider how multi-agent systems can adapt their structure to different problems</li> </ul>"},{"location":"agentic/06_self_reflective_agent/","title":"Self-Reflective Agent: Learning From Experience","text":""},{"location":"agentic/06_self_reflective_agent/#objective","title":"Objective","text":"<p>Build an agent capable of evaluating its own performance, identifying strengths and weaknesses, and adjusting its behavior based on past experiences to improve future interactions.</p>"},{"location":"agentic/06_self_reflective_agent/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Create and activate a virtual environment (if not already done) 2. Install dependencies 3. Configure API access <pre><code># Create a virtual environment (if not already created)\npython -m venv agentic-env\nsource agentic-env/bin/activate  # On Windows: agentic-env\\Scripts\\activate\n\n# Install dependencies\npip install openai langchain pydantic python-dotenv\n\n# Create a .env file for your API key (if not already done)\necho \"OPENAI_API_KEY=your_api_key_here\" &gt; .env\n</code></pre>"},{"location":"agentic/06_self_reflective_agent/#verifying-setup","title":"Verifying Setup","text":"<p>Run the following test code to ensure your environment is ready:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\n\n# Load environment variables\nload_dotenv()\n\n# Test API connection\nllm = OpenAI(temperature=0)\nresult = llm.invoke(\"Hello!\")\nprint(f\"LLM Response: {result}\")\n</code></pre>"},{"location":"agentic/06_self_reflective_agent/#core-concept-self-reflective-agents","title":"Core Concept: Self-Reflective Agents","text":"<p>Self-reflective agents elevate capabilities through:</p> <ol> <li>Performance monitoring - Tracking the outcomes of actions</li> <li>Self-evaluation - Assessing strengths and weaknesses</li> <li>Strategic adaptation - Modifying behavior based on insights</li> <li>Continuous improvement - Learning from past experiences</li> </ol> <p>Key characteristics:</p> <ul> <li>Introspective - Analyzes own thought processes</li> <li>Self-aware - Recognizes limitations and capabilities</li> <li>Adaptive - Changes strategies based on feedback</li> <li>Growth-oriented - Improves over time through experience</li> </ul>"},{"location":"agentic/06_self_reflective_agent/#reflection-types","title":"Reflection Types","text":"<ul> <li>Outcome reflection - Evaluating success or failure</li> <li>Process reflection - Analyzing the approach taken</li> <li>Strategy reflection - Considering alternative approaches</li> <li>Knowledge reflection - Identifying gaps in information</li> </ul>"},{"location":"agentic/06_self_reflective_agent/#real-world-examples","title":"Real-world Examples","text":"<ul> <li>AI systems that track error rates to improve</li> <li>Recommendation systems that learn from user feedback</li> <li>Self-improving game-playing agents</li> <li>Learning-based robotics systems</li> </ul>"},{"location":"agentic/06_self_reflective_agent/#modular-code-implementation","title":"Modular Code Implementation","text":""},{"location":"agentic/06_self_reflective_agent/#1-reflection-framework","title":"1. Reflection Framework","text":"<pre><code>import os\nimport json\nimport time\nfrom typing import Dict, List, Any, Optional\nfrom enum import Enum\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\nclass ActionResult(Enum):\n    \"\"\"Possible outcomes of an agent's actions.\"\"\"\n    SUCCESS = \"success\"\n    PARTIAL_SUCCESS = \"partial_success\"\n    FAILURE = \"failure\"\n    UNKNOWN = \"unknown\"\n\nclass ExperienceRecord:\n    \"\"\"A record of an action taken by the agent and its outcome.\"\"\"\n\n    def __init__(self,\n                 task: str,\n                 action: str,\n                 result: ActionResult,\n                 feedback: Optional[str] = None,\n                 context: Optional[Dict] = None):\n        \"\"\"\n        Initialize an experience record.\n\n        Args:\n            task: The task or goal the agent was trying to achieve\n            action: The specific action taken by the agent\n            result: The outcome of the action\n            feedback: Optional feedback provided (by user or environment)\n            context: Optional contextual information relevant to the experience\n        \"\"\"\n        self.task = task\n        self.action = action\n        self.result = result\n        self.feedback = feedback\n        self.context = context or {}\n        self.timestamp = time.time()\n        self.reflection = None  # Will be populated after reflection\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"Convert the experience record to a dictionary.\"\"\"\n        return {\n            \"task\": self.task,\n            \"action\": self.action,\n            \"result\": self.result.value,\n            \"feedback\": self.feedback,\n            \"context\": self.context,\n            \"timestamp\": self.timestamp,\n            \"reflection\": self.reflection\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict) -&gt; 'ExperienceRecord':\n        \"\"\"Create an experience record from a dictionary.\"\"\"\n        record = cls(\n            task=data[\"task\"],\n            action=data[\"action\"],\n            result=ActionResult(data[\"result\"]),\n            feedback=data[\"feedback\"],\n            context=data[\"context\"]\n        )\n        record.timestamp = data[\"timestamp\"]\n        record.reflection = data[\"reflection\"]\n        return record\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of the experience record.\"\"\"\n        return f\"Task: {self.task}\\nAction: {self.action}\\nResult: {self.result.value}\\nFeedback: {self.feedback or 'None'}\"\n\nclass ExperienceMemory:\n    \"\"\"A collection of experience records with retrieval capabilities.\"\"\"\n\n    def __init__(self, max_experiences: int = 100):\n        \"\"\"Initialize the experience memory with a maximum size.\"\"\"\n        self.experiences = []\n        self.max_experiences = max_experiences\n\n    def add(self, experience: ExperienceRecord):\n        \"\"\"Add an experience record to memory.\"\"\"\n        self.experiences.append(experience)\n        # Trim if exceeding maximum size\n        if len(self.experiences) &gt; self.max_experiences:\n            self.experiences = self.experiences[-self.max_experiences:]\n\n    def get_all(self) -&gt; List[ExperienceRecord]:\n        \"\"\"Get all experience records.\"\"\"\n        return self.experiences\n\n    def get_by_result(self, result: ActionResult) -&gt; List[ExperienceRecord]:\n        \"\"\"Get experiences with a specific result.\"\"\"\n        return [exp for exp in self.experiences if exp.result == result]\n\n    def get_by_task_similarity(self, task: str, k: int = 5) -&gt; List[ExperienceRecord]:\n        \"\"\"Get experiences with similar tasks (simplified implementation).\"\"\"\n        # In a real implementation, this would use embeddings and semantic search\n        # For this educational example, we'll use simple string matching\n        # Sort experiences by the length of the longest common substring with the task\n        def common_substring_length(s1: str, s2: str) -&gt; int:\n            # Simple implementation of longest common substring length\n            s1, s2 = s1.lower(), s2.lower()\n            m = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n            longest = 0\n            for i in range(1, len(s1) + 1):\n                for j in range(1, len(s2) + 1):\n                    if s1[i-1] == s2[j-1]:\n                        m[i][j] = m[i-1][j-1] + 1\n                        longest = max(longest, m[i][j])\n            return longest\n\n        sorted_experiences = sorted(\n            self.experiences,\n            key=lambda exp: common_substring_length(exp.task, task),\n            reverse=True\n        )\n        return sorted_experiences[:k]\n\n    def save_to_file(self, filename: str):\n        \"\"\"Save experiences to a file.\"\"\"\n        with open(filename, 'w') as f:\n            json.dump([exp.to_dict() for exp in self.experiences], f, indent=2)\n\n    def load_from_file(self, filename: str):\n        \"\"\"Load experiences from a file.\"\"\"\n        if os.path.exists(filename):\n            with open(filename, 'r') as f:\n                data = json.load(f)\n                self.experiences = [ExperienceRecord.from_dict(item) for item in data]\n</code></pre>"},{"location":"agentic/06_self_reflective_agent/#2-reflection-engine","title":"2. Reflection Engine","text":"<pre><code>class ReflectionEngine:\n    \"\"\"Engine for generating reflections on agent experiences.\"\"\"\n\n    def __init__(self, llm=None):\n        \"\"\"Initialize the reflection engine with an optional language model.\"\"\"\n        load_dotenv()\n        self.llm = llm or OpenAI(temperature=0.7)\n\n        # Define the reflection prompt template\n        self.reflection_template = PromptTemplate(\n            input_variables=[\"experience\", \"similar_experiences\"],\n            template=\"\"\"\n            You are a self-reflective AI analyzing your past performance to improve.\n\n            Current experience to reflect on:\n            {experience}\n\n            Similar past experiences (if any):\n            {similar_experiences}\n\n            Please reflect on this experience by considering:\n            1. What went well and why?\n            2. What could have been improved and how?\n            3. What patterns do you notice across similar experiences?\n            4. What lessons can be applied to future situations?\n            5. What specific changes in approach would lead to better outcomes?\n\n            Provide a thoughtful, specific reflection that will help improve future performance.\n            \"\"\"\n        )\n\n        # Define the meta-reflection prompt template\n        self.meta_reflection_template = PromptTemplate(\n            input_variables=[\"reflections\"],\n            template=\"\"\"\n            You are a self-reflective AI analyzing multiple reflections to identify patterns\n            and extract general principles for improvement.\n\n            Recent reflections:\n            {reflections}\n\n            Based on these reflections, please:\n            1. Identify recurring themes and patterns\n            2. Extract 3-5 key principles or strategies for improvement\n            3. Develop specific, actionable rules to guide future behavior\n            4. Note any areas where your understanding seems incomplete\n\n            Format your response as a structured summary of insights and action items.\n            \"\"\"\n        )\n\n    def reflect_on_experience(self,\n                              experience: ExperienceRecord,\n                              similar_experiences: List[ExperienceRecord] = None) -&gt; str:\n        \"\"\"Generate a reflection on a single experience.\"\"\"\n        # Format the experience for the prompt\n        experience_str = str(experience)\n\n        # Format similar experiences if provided\n        if similar_experiences and len(similar_experiences) &gt; 0:\n            similar_exp_str = \"\\n\\n\".join([str(exp) for exp in similar_experiences])\n        else:\n            similar_exp_str = \"No similar past experiences.\"\n\n        # Generate the reflection\n        prompt = self.reflection_template.format(\n            experience=experience_str,\n            similar_experiences=similar_exp_str\n        )\n\n        reflection = self.llm.invoke(prompt)\n        return reflection\n\n    def meta_reflect(self, experiences: List[ExperienceRecord], k: int = 5) -&gt; str:\n        \"\"\"Generate a meta-reflection across multiple experiences.\"\"\"\n        # Select the k most recent experiences with reflections\n        recent_reflections = [exp for exp in experiences if exp.reflection is not None]\n        recent_reflections = sorted(recent_reflections, key=lambda x: x.timestamp, reverse=True)[:k]\n\n        if not recent_reflections:\n            return \"No reflections available for meta-reflection.\"\n\n        # Format the reflections for the prompt\n        reflections_str = \"\\n\\n\".join([\n            f\"Task: {exp.task}\\nReflection: {exp.reflection}\"\n            for exp in recent_reflections\n        ])\n\n        # Generate the meta-reflection\n        prompt = self.meta_reflection_template.format(reflections=reflections_str)\n        meta_reflection = self.llm.invoke(prompt)\n\n        return meta_reflection\n</code></pre>"},{"location":"agentic/06_self_reflective_agent/#3-self-reflective-agent","title":"3. Self-Reflective Agent","text":"<pre><code>class SelfReflectiveAgent:\n    \"\"\"An agent that learns from experience through reflection.\"\"\"\n\n    def __init__(self, llm=None):\n        \"\"\"Initialize the self-reflective agent with memory and reflection systems.\"\"\"\n        load_dotenv()\n        self.llm = llm or OpenAI(temperature=0.7)\n        self.experience_memory = ExperienceMemory()\n        self.reflection_engine = ReflectionEngine(llm)\n        self.strategies = []  # Learned strategies from meta-reflection\n\n        # Define the decision-making prompt template\n        self.decision_template = PromptTemplate(\n            input_variables=[\"task\", \"context\", \"similar_experiences\", \"strategies\"],\n            template=\"\"\"\n            You are a self-reflective AI that learns from past experiences.\n\n            Current task: {task}\n\n            Context: {context}\n\n            Relevant past experiences:\n            {similar_experiences}\n\n            Strategies learned from reflection:\n            {strategies}\n\n            Based on your past experiences and learned strategies, determine the best\n            approach to complete this task. Explain your reasoning and how it incorporates\n            lessons from previous experiences.\n\n            Your response should be structured as follows:\n            1. Analysis of the task\n            2. Relevant past experiences and lessons\n            3. Approach chosen with explanation\n            4. Step-by-step plan\n            \"\"\"\n        )\n\n    def load_experiences(self, filename: str):\n        \"\"\"Load past experiences from a file.\"\"\"\n        self.experience_memory.load_from_file(filename)\n\n    def save_experiences(self, filename: str):\n        \"\"\"Save experiences to a file.\"\"\"\n        self.experience_memory.save_to_file(filename)\n\n    def _format_experiences(self, experiences: List[ExperienceRecord]) -&gt; str:\n        \"\"\"Format a list of experiences for inclusion in prompts.\"\"\"\n        if not experiences:\n            return \"No relevant past experiences.\"\n\n        return \"\\n\\n\".join([\n            f\"Task: {exp.task}\\nAction: {exp.action}\\nResult: {exp.result.value}\\nReflection: {exp.reflection or 'None'}\"\n            for exp in experiences\n        ])\n\n    def decide_approach(self, task: str, context: str = \"\") -&gt; str:\n        \"\"\"Decide on an approach for a given task based on past experiences.\"\"\"\n        # Retrieve similar past experiences\n        similar_experiences = self.experience_memory.get_by_task_similarity(task)\n\n        # Format the experiences and strategies for the prompt\n        similar_exp_str = self._format_experiences(similar_experiences)\n        strategies_str = \"\\n\".join(self.strategies) if self.strategies else \"No established strategies yet.\"\n\n        # Generate the decision\n        prompt = self.decision_template.format(\n            task=task,\n            context=context,\n            similar_experiences=similar_exp_str,\n            strategies=strategies_str\n        )\n\n        decision = self.llm.invoke(prompt)\n        return decision\n\n    def act(self, task: str, context: str = \"\") -&gt; str:\n        \"\"\"Perform an action based on the decided approach.\"\"\"\n        # In a full implementation, this would execute the action\n        # For this educational example, we'll simply return the planned approach\n        return self.decide_approach(task, context)\n\n    def record_experience(self,\n                         task: str,\n                         action: str,\n                         result: ActionResult,\n                         feedback: Optional[str] = None,\n                         context: Optional[Dict] = None):\n        \"\"\"Record an experience and generate a reflection on it.\"\"\"\n        # Create and store the experience\n        experience = ExperienceRecord(task, action, result, feedback, context)\n\n        # Retrieve similar past experiences for context in reflection\n        similar_experiences = self.experience_memory.get_by_task_similarity(task)\n\n        # Generate a reflection\n        reflection = self.reflection_engine.reflect_on_experience(experience, similar_experiences)\n        experience.reflection = reflection\n\n        # Store the experience with its reflection\n        self.experience_memory.add(experience)\n\n        # If we have enough experiences, perform a meta-reflection\n        if len(self.experience_memory.experiences) &gt;= 5:\n            meta_reflection = self.reflection_engine.meta_reflect(self.experience_memory.experiences)\n\n            # Extract strategies from meta-reflection (in a real implementation,\n            # this would use more sophisticated parsing)\n            strategies = meta_reflection.split(\"\\n\")\n            strategies = [s for s in strategies if s.strip().startswith(\"- \") or s.strip().startswith(\"* \")]\n\n            if strategies:\n                self.strategies = strategies\n\n        return reflection\n</code></pre>"},{"location":"agentic/06_self_reflective_agent/#4-test-harness","title":"4. Test Harness","text":"<pre><code>def test_self_reflective_agent():\n    \"\"\"Test the self-reflective agent with a series of experiences and tasks.\"\"\"\n    agent = SelfReflectiveAgent()\n\n    # First, let's create some simulated past experiences\n    print(\"=== Creating Simulated Past Experiences ===\\n\")\n\n    # Experience 1: A successful explanation\n    agent.record_experience(\n        task=\"Explain the concept of machine learning to a beginner\",\n        action=\"Provided an analogy comparing machine learning to how humans learn from experience\",\n        result=ActionResult.SUCCESS,\n        feedback=\"The explanation was clear and relatable. The analogy really helped.\"\n    )\n\n    # Experience 2: A partial success\n    agent.record_experience(\n        task=\"Explain the difference between supervised and unsupervised learning\",\n        action=\"Gave technical definitions with mathematical formulations\",\n        result=ActionResult.PARTIAL_SUCCESS,\n        feedback=\"The explanation was accurate but too technical for the audience.\"\n    )\n\n    # Experience 3: A failure\n    agent.record_experience(\n        task=\"Explain neural networks to a non-technical manager\",\n        action=\"Used detailed technical descriptions of neuron activations and backpropagation\",\n        result=ActionResult.FAILURE,\n        feedback=\"The explanation was too complex and used too much jargon.\"\n    )\n\n    # Experience 4: Another success with a different approach\n    agent.record_experience(\n        task=\"Explain the concept of overfitting to a data scientist\",\n        action=\"Used technical language with relevant examples from real-world projects\",\n        result=ActionResult.SUCCESS,\n        feedback=\"The explanation was appropriately technical and the examples were helpful.\"\n    )\n\n    # Now, let's test the agent on a new task\n    print(\"\\n=== Testing Agent on New Task ===\\n\")\n\n    new_task = \"Explain how GPT models work to a high school student\"\n    print(f\"Task: {new_task}\\n\")\n\n    # Get the agent's approach\n    approach = agent.decide_approach(new_task)\n    print(\"Agent's Planned Approach:\")\n    print(approach)\n\n    # Simulate the execution of this approach\n    # In a real application, this would be an actual implementation\n    simulated_result = ActionResult.SUCCESS\n    simulated_feedback = \"The student understood the concept well and could explain it back in their own words.\"\n\n    # Record this new experience\n    print(\"\\n=== Recording New Experience and Reflection ===\\n\")\n    reflection = agent.record_experience(\n        task=new_task,\n        action=approach,\n        result=simulated_result,\n        feedback=simulated_feedback\n    )\n\n    print(\"Agent's Reflection:\")\n    print(reflection)\n\n    # Display learned strategies\n    print(\"\\n=== Agent's Learned Strategies ===\\n\")\n    if agent.strategies:\n        for i, strategy in enumerate(agent.strategies, 1):\n            print(f\"{i}. {strategy}\")\n    else:\n        print(\"No strategies learned yet.\")\n\n    # Test on another related but different task\n    print(\"\\n=== Testing Agent on Another Task ===\\n\")\n\n    another_task = \"Explain the ethical implications of AI to a group of policy makers\"\n    print(f\"Task: {another_task}\\n\")\n\n    # Get the agent's approach for this new task\n    approach = agent.decide_approach(another_task)\n    print(\"Agent's Planned Approach:\")\n    print(approach)\n\nif __name__ == \"__main__\":\n    test_self_reflective_agent()\n</code></pre>"},{"location":"agentic/06_self_reflective_agent/#live-coding-collaboration","title":"Live Coding Collaboration","text":""},{"location":"agentic/06_self_reflective_agent/#session-structure-60-75-minutes","title":"Session Structure (60-75 minutes)","text":"<ol> <li> <p>Setup &amp; Framework Design (15 min)</p> </li> <li> <p>Configure environment</p> </li> <li>Understand the reflection architecture</li> <li> <p>Plan implementation approach</p> </li> <li> <p>Implementation Phase (25 min)</p> </li> <li> <p>Complete the agent's reflection capabilities</p> </li> <li>Enhance the decision-making system</li> <li> <p>Implement the experience recording and retrieval</p> </li> <li> <p>Testing &amp; Analysis (20 min)</p> </li> <li> <p>Test with varied scenarios</p> </li> <li>Analyze reflection quality</li> <li> <p>Evaluate how past experiences influence decisions</p> </li> <li> <p>Extension &amp; Refinement (15 min)</p> </li> <li>Implement improved experience retrieval</li> <li>Add more sophisticated reflection analysis</li> <li>Create specialized reflection patterns for different tasks</li> </ol>"},{"location":"agentic/06_self_reflective_agent/#reflection-checkpoints","title":"Reflection Checkpoints","text":""},{"location":"agentic/06_self_reflective_agent/#checkpoint-1-after-framework-implementation","title":"Checkpoint 1: After Framework Implementation","text":"<ul> <li>How does the reflection system capture different aspects of performance?</li> <li>What types of experiences provide the most valuable learning?</li> <li>How should reflections be structured to maximize their utility?</li> <li>What patterns might emerge from different types of experiences?</li> </ul>"},{"location":"agentic/06_self_reflective_agent/#checkpoint-2-after-agent-testing","title":"Checkpoint 2: After Agent Testing","text":"<ul> <li>How do reflections change as the agent accumulates more experiences?</li> <li>When does the agent effectively incorporate past lessons?</li> <li>How does reflection quality impact future decision-making?</li> <li>What meta-patterns emerge from reflecting on reflections?</li> </ul>"},{"location":"agentic/06_self_reflective_agent/#debugging-and-extension-challenges","title":"Debugging and Extension Challenges","text":""},{"location":"agentic/06_self_reflective_agent/#common-issues","title":"Common Issues","text":"<ul> <li>Overgeneralizing from limited experiences</li> <li>Underfitting (not learning from experiences)</li> <li>Reflection paradox (overthinking vs. acting)</li> <li>Context blindness in applying past lessons</li> <li>Strategy conflicts between different task types</li> </ul>"},{"location":"agentic/06_self_reflective_agent/#extension-ideas","title":"Extension Ideas","text":"<ol> <li> <p>Emotion-Informed Reflection</p> </li> <li> <p>Add emotional assessment to experiences</p> </li> <li>Implement reflection on emotional responses</li> <li> <p>Create affect-aware decision making</p> </li> <li> <p>Counterfactual Reflection</p> </li> <li> <p>Add \"what if\" analysis to reflections</p> </li> <li>Implement alternative scenario exploration</li> <li> <p>Create decision trees from counterfactuals</p> </li> <li> <p>Collaborative Reflection</p> </li> <li> <p>Share reflections across multiple agents</p> </li> <li>Implement peer feedback on reflections</li> <li> <p>Create consensus-building for strategies</p> </li> <li> <p>User-Guided Reflection</p> </li> <li>Add user feedback on reflections</li> <li>Implement adjustable reflection parameters</li> <li>Create personalized reflection styles</li> </ol>"},{"location":"agentic/06_self_reflective_agent/#agent-evaluation","title":"Agent Evaluation","text":""},{"location":"agentic/06_self_reflective_agent/#effectiveness-metrics","title":"Effectiveness Metrics","text":"<ul> <li>Improvement rate over time</li> <li>Strategy quality and applicability</li> <li>Adaptation to new situations</li> <li>Learning transfer across domains</li> </ul>"},{"location":"agentic/06_self_reflective_agent/#efficiency-metrics","title":"Efficiency Metrics","text":"<ul> <li>Reflection relevance to future tasks</li> <li>Strategy extraction accuracy</li> <li>Experience retrieval precision</li> <li>Reflection generation time</li> </ul>"},{"location":"agentic/06_self_reflective_agent/#alignment-metrics","title":"Alignment Metrics","text":"<ul> <li>Self-awareness of limitations</li> <li>Appropriate confidence calibration</li> <li>Ethical considerations in reflections</li> <li>Value alignment in strategy formation</li> </ul>"},{"location":"agentic/06_self_reflective_agent/#evaluation-exercise","title":"Evaluation Exercise","text":"<p>Design a sequence of related tasks that test the agent's ability to:</p> <ol> <li>Learn from initial mistakes</li> <li>Apply lessons to similar situations</li> <li>Transfer insights to new domains</li> <li>Integrate potentially conflicting feedback</li> <li>Develop nuanced strategies over time</li> </ol>"},{"location":"agentic/06_self_reflective_agent/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>How does self-reflection in AI agents compare to human metacognition?</li> <li>What biases might affect an agent's reflection process?</li> <li>How might reflection capabilities enhance or hinder agent transparency?</li> <li>What are the ethical implications of agents that continuously self-improve?</li> <li>How might self-reflection impact an agent's explainability?</li> <li>What role should human feedback play in agent reflection?</li> <li>How would you balance reflection time vs. action time in resource-constrained environments?</li> </ol>"},{"location":"agentic/06_self_reflective_agent/#next-steps","title":"Next Steps","text":"<ul> <li>Explore how combining reflection with planning can create more strategic agents</li> <li>Investigate how reflection can enhance multi-agent collaboration</li> <li>Consider how tool-using agents can reflect on tool efficacy</li> </ul>"},{"location":"computer_vision/01_image_loading_and_manipulation/","title":"Image Loading and Manipulation Lab","text":""},{"location":"computer_vision/01_image_loading_and_manipulation/#objective","title":"Objective","text":"<p>Master the fundamentals of loading, inspecting, and manipulating digital images using OpenCV and NumPy. This lab introduces the core building blocks needed for all computer vision projects.</p>"},{"location":"computer_vision/01_image_loading_and_manipulation/#environment-setup","title":"Environment Setup","text":"Dependency Purpose Python 3.10+ Programming language OpenCV Image processing library NumPy Numerical operations for arrays Matplotlib Visualization of images"},{"location":"computer_vision/01_image_loading_and_manipulation/#installation","title":"Installation","text":"<pre><code># Create virtual environment (optional)\npython -m venv cv_env\nsource cv_env/bin/activate  # On Windows: cv_env\\Scripts\\activate\n\n# Install required packages\npip install opencv-python numpy matplotlib\n</code></pre>"},{"location":"computer_vision/01_image_loading_and_manipulation/#verification","title":"Verification","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Verify versions\nprint(f\"OpenCV version: {cv2.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n</code></pre>"},{"location":"computer_vision/01_image_loading_and_manipulation/#key-concepts","title":"Key Concepts","text":""},{"location":"computer_vision/01_image_loading_and_manipulation/#images-as-matrices","title":"Images as Matrices","text":"<p>Digital images are represented as multi-dimensional arrays:</p> <ul> <li>Grayscale: 2D array (height \u00d7 width)</li> <li>Color: 3D array (height \u00d7 width \u00d7 channels)</li> </ul> <p></p>"},{"location":"computer_vision/01_image_loading_and_manipulation/#color-spaces","title":"Color Spaces","text":"<p>Images can be represented in different color systems:</p> <ul> <li>RGB: Red, Green, Blue channels (most common)</li> <li>BGR: Blue, Green, Red (OpenCV's default format)</li> <li>HSV: Hue, Saturation, Value (useful for color filtering)</li> <li>Grayscale: Single channel intensity</li> </ul>"},{"location":"computer_vision/01_image_loading_and_manipulation/#starter-code","title":"Starter Code","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef load_and_display(image_path):\n    \"\"\"\n    Load an image and display it using matplotlib.\n\n    Args:\n        image_path (str): Path to the image file\n\n    Returns:\n        np.ndarray: The loaded image\n    \"\"\"\n    # TODO: Load the image using cv2.imread\n    # Remember that OpenCV loads images in BGR format\n\n    # TODO: Convert BGR to RGB for correct display in matplotlib\n\n    # Display the image\n    plt.figure(figsize=(10, 8))\n    plt.imshow(img_rgb)\n    plt.axis('off')\n    plt.title('Original Image')\n    plt.show()\n\n    return img\n\ndef inspect_image(image):\n    \"\"\"\n    Print basic information about an image.\n\n    Args:\n        image (np.ndarray): Image to inspect\n    \"\"\"\n    # TODO: Print shape, data type, min/max values\n\n    # TODO: If color image, print information for each channel\n\ndef resize_image(image, width=None, height=None, scale=None):\n    \"\"\"\n    Resize an image based on width, height, or scale factor.\n\n    Args:\n        image (np.ndarray): Input image\n        width (int, optional): Target width\n        height (int, optional): Target height\n        scale (float, optional): Scale factor\n\n    Returns:\n        np.ndarray: Resized image\n    \"\"\"\n    # TODO: Implement resizing logic\n    # If scale is provided, resize proportionally\n    # If width and height are provided, resize to those dimensions\n    # If only width or only height is provided, maintain aspect ratio\n\n    return resized_image\n\ndef crop_image(image, x, y, width, height):\n    \"\"\"\n    Crop a region from the image.\n\n    Args:\n        image (np.ndarray): Input image\n        x, y (int): Top-left corner coordinates\n        width, height (int): Width and height of crop region\n\n    Returns:\n        np.ndarray: Cropped image\n    \"\"\"\n    # TODO: Implement cropping logic\n\n    return cropped_image\n\ndef convert_colorspace(image, target_space='rgb'):\n    \"\"\"\n    Convert image between color spaces.\n\n    Args:\n        image (np.ndarray): Input image\n        target_space (str): Target color space ('rgb', 'gray', 'hsv')\n\n    Returns:\n        np.ndarray: Converted image\n    \"\"\"\n    # TODO: Implement color space conversion\n    # Support at least RGB, grayscale, and HSV\n\n    return converted_image\n\ndef flip_image(image, direction='horizontal'):\n    \"\"\"\n    Flip an image horizontally or vertically.\n\n    Args:\n        image (np.ndarray): Input image\n        direction (str): 'horizontal', 'vertical', or 'both'\n\n    Returns:\n        np.ndarray: Flipped image\n    \"\"\"\n    # TODO: Implement image flipping\n\n    return flipped_image\n\ndef rotate_image(image, angle):\n    \"\"\"\n    Rotate an image by the specified angle.\n\n    Args:\n        image (np.ndarray): Input image\n        angle (float): Rotation angle in degrees\n\n    Returns:\n        np.ndarray: Rotated image\n    \"\"\"\n    # TODO: Implement image rotation around the center\n\n    return rotated_image\n\n# Main execution block\nif __name__ == \"__main__\":\n    # Sample image path - replace with your own\n    image_path = \"sample_image.jpg\"\n\n    # Load and display the image\n    img = load_and_display(image_path)\n\n    # Inspect the image\n    inspect_image(img)\n\n    # Demonstrate image manipulations\n    # TODO: Add code to demonstrate each function and visualize results\n</code></pre>"},{"location":"computer_vision/01_image_loading_and_manipulation/#group-implementation-tasks","title":"Group Implementation Tasks","text":""},{"location":"computer_vision/01_image_loading_and_manipulation/#task-1-complete-the-missing-functions","title":"Task 1: Complete the Missing Functions","text":"<p>Working in pairs, complete each TODO section in the starter code. Divide the tasks between team members:</p> <ul> <li>Person A: Implement loading, inspection, and resizing functions</li> <li>Person B: Implement cropping, color conversion, and transformation functions</li> </ul>"},{"location":"computer_vision/01_image_loading_and_manipulation/#task-2-create-a-function-showcase","title":"Task 2: Create a Function Showcase","text":"<p>Develop a demonstration function that:</p> <ol> <li>Loads an image of your choice</li> <li>Displays it alongside a montage of different manipulations</li> <li>Shows the effect of at least 4 different operations</li> </ol> <p>Example showcase layout:</p> <pre><code>[Original Image] | [Resized Image]\n[Grayscale Image] | [Cropped Image]\n[Flipped Image] | [Rotated Image]\n</code></pre>"},{"location":"computer_vision/01_image_loading_and_manipulation/#task-3-image-matrix-investigation","title":"Task 3: Image Matrix Investigation","text":"<ol> <li>Select a small region (e.g., 5\u00d75 pixels) from an image</li> <li>Print the raw pixel values before and after manipulation</li> <li>Discuss how the numerical values change with each operation</li> </ol>"},{"location":"computer_vision/01_image_loading_and_manipulation/#peer-review-questions","title":"Peer Review Questions","text":"<p>After completing the implementation, review each other's code and discuss:</p> <ol> <li> <p>Implementation Approaches:</p> </li> <li> <p>How did your approaches to implementing the functions differ?</p> </li> <li> <p>Which implementation is more efficient or readable?</p> </li> <li> <p>Visual Results Analysis:</p> </li> <li> <p>How does the quality of resized images compare between different methods?</p> </li> <li> <p>What happens to image details when converting between color spaces?</p> </li> <li> <p>Matrix Understanding:</p> </li> <li> <p>What happens to the pixel values when you flip or rotate an image?</p> </li> <li> <p>How does cropping affect the image matrix dimensions?</p> </li> <li> <p>Edge Cases:</p> </li> <li>How does your code handle images of different formats (PNG vs JPG)?</li> <li>What happens if the crop region extends beyond the image boundaries?</li> </ol>"},{"location":"computer_vision/01_image_loading_and_manipulation/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"computer_vision/01_image_loading_and_manipulation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Image Not Loading</p> </li> <li> <p>Check if the file path is correct and the file exists</p> </li> <li> <p>Verify that the file format is supported by OpenCV</p> </li> <li> <p>Color Distortion</p> </li> <li> <p>Remember that OpenCV uses BGR order, not RGB</p> </li> <li> <p>Ensure proper conversion when displaying with matplotlib</p> </li> <li> <p>Dimension Errors</p> </li> <li> <p>Check array shapes before operations</p> </li> <li> <p>Verify that crop coordinates are within image boundaries</p> </li> <li> <p>Memory Issues</p> </li> <li>Be cautious with very large images</li> <li>Release resources with <code>cv2.destroyAllWindows()</code></li> </ol>"},{"location":"computer_vision/01_image_loading_and_manipulation/#mini-project-image-transformation-tool","title":"Mini-Project: Image Transformation Tool","text":"<p>Create a simple command-line tool that:</p> <ol> <li>Takes an input image path and output directory</li> <li>Applies a user-selected sequence of transformations</li> <li>Saves the transformed images with descriptive filenames</li> <li>Generates a visual report showing all transformations</li> </ol>"},{"location":"computer_vision/01_image_loading_and_manipulation/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Add a simple GUI with sliders for transformation parameters</li> <li>Implement batch processing for multiple images</li> <li>Create custom transformation presets (e.g., \"Thumbnail generator\", \"Social media formatter\")</li> </ul>"},{"location":"computer_vision/01_image_loading_and_manipulation/#reflection-questions","title":"Reflection Questions","text":"<ol> <li>How might these basic operations be used in real-world applications?</li> <li>What happens to image quality after multiple transformations?</li> <li>How would you optimize these operations for very large images?</li> <li>What additional metadata might be useful to extract from images?</li> <li>How do digital cameras and smartphones implement similar operations?</li> </ol>"},{"location":"computer_vision/02_filtering_and_convolution/","title":"Filtering and Convolution Lab","text":""},{"location":"computer_vision/02_filtering_and_convolution/#objective","title":"Objective","text":"<p>Understand and implement image filtering and convolution operations using OpenCV. This lab explores how kernels (small matrices) applied to images can create various effects like blurring, sharpening, and edge enhancement.</p>"},{"location":"computer_vision/02_filtering_and_convolution/#environment-setup","title":"Environment Setup","text":"Dependency Purpose Python 3.10+ Programming language OpenCV Image processing library NumPy Numerical operations for arrays Matplotlib Visualization of images"},{"location":"computer_vision/02_filtering_and_convolution/#installation","title":"Installation","text":"<pre><code># Create virtual environment (optional)\npython -m venv cv_env\nsource cv_env/bin/activate  # On Windows: cv_env\\Scripts\\activate\n\n# Install required packages\npip install opencv-python numpy matplotlib\n</code></pre>"},{"location":"computer_vision/02_filtering_and_convolution/#verification","title":"Verification","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Verify versions\nprint(f\"OpenCV version: {cv2.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n</code></pre>"},{"location":"computer_vision/02_filtering_and_convolution/#key-concepts","title":"Key Concepts","text":""},{"location":"computer_vision/02_filtering_and_convolution/#convolution","title":"Convolution","text":"<p>Convolution is a mathematical operation that applies a kernel (small matrix) to an image to produce a new image. The process involves:</p> <ol> <li>Placing the kernel over each pixel in the input image</li> <li>Multiplying overlapping elements</li> <li>Summing the products</li> <li>Replacing the center pixel with the sum</li> </ol> <p></p>"},{"location":"computer_vision/02_filtering_and_convolution/#kernels","title":"Kernels","text":"<p>A kernel (also called a filter or mask) is a small matrix used during convolution. Different kernels produce different effects:</p> Kernel Type Effect Example Application Box Filter Blur/Smooth Noise reduction Gaussian Filter Smooth with weights Reduce detail while preserving edges Sharpen Enhance edges Highlight details Sobel Detect gradients Edge detection Laplacian Detect edges Second-derivative edge detection"},{"location":"computer_vision/02_filtering_and_convolution/#example-kernels","title":"Example Kernels","text":"<pre><code># Box Blur (3x3)\n[1/9, 1/9, 1/9]\n[1/9, 1/9, 1/9]\n[1/9, 1/9, 1/9]\n\n# Sharpen\n[ 0, -1,  0]\n[-1,  5, -1]\n[ 0, -1,  0]\n\n# Sobel (horizontal)\n[-1, 0, 1]\n[-2, 0, 2]\n[-1, 0, 1]\n</code></pre>"},{"location":"computer_vision/02_filtering_and_convolution/#starter-code","title":"Starter Code","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef load_image(image_path):\n    \"\"\"\n    Load an image and convert to RGB for display.\n\n    Args:\n        image_path (str): Path to the image file\n\n    Returns:\n        tuple: (bgr_image, rgb_image)\n    \"\"\"\n    # Load the image\n    img = cv2.imread(image_path)\n    if img is None:\n        raise FileNotFoundError(f\"Could not load image from {image_path}\")\n\n    # Convert to RGB for matplotlib display\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    return img, img_rgb\n\ndef display_images(images, titles, figsize=(15, 10)):\n    \"\"\"\n    Display multiple images in a grid.\n\n    Args:\n        images (list): List of images to display\n        titles (list): List of titles for each image\n        figsize (tuple): Figure size\n    \"\"\"\n    n = len(images)\n    fig, axes = plt.subplots(1, n, figsize=figsize)\n\n    if n == 1:\n        axes = [axes]\n\n    for i, (img, title) in enumerate(zip(images, titles)):\n        axes[i].imshow(img)\n        axes[i].set_title(title)\n        axes[i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndef apply_box_blur(image, kernel_size=3):\n    \"\"\"\n    Apply a box blur filter to an image.\n\n    Args:\n        image (np.ndarray): Input image\n        kernel_size (int): Size of the kernel (3, 5, etc.)\n\n    Returns:\n        np.ndarray: Blurred image\n    \"\"\"\n    # TODO: Implement box blur using cv2.blur()\n\n    # Convert back to RGB if the input was RGB\n    if len(image.shape) == 3:\n        blurred_rgb = cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB)\n        return blurred_rgb\n    return blurred\n\ndef apply_gaussian_blur(image, kernel_size=3, sigma=0):\n    \"\"\"\n    Apply a Gaussian blur filter to an image.\n\n    Args:\n        image (np.ndarray): Input image\n        kernel_size (int): Size of the kernel (must be odd)\n        sigma (float): Standard deviation of the Gaussian\n\n    Returns:\n        np.ndarray: Blurred image\n    \"\"\"\n    # TODO: Implement Gaussian blur using cv2.GaussianBlur()\n\n    # Convert back to RGB if the input was RGB\n    if len(image.shape) == 3:\n        blurred_rgb = cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB)\n        return blurred_rgb\n    return blurred\n\ndef apply_median_blur(image, kernel_size=3):\n    \"\"\"\n    Apply a median blur filter to an image.\n\n    Args:\n        image (np.ndarray): Input image\n        kernel_size (int): Size of the kernel (must be odd)\n\n    Returns:\n        np.ndarray: Blurred image\n    \"\"\"\n    # TODO: Implement median blur using cv2.medianBlur()\n\n    # Convert back to RGB if the input was RGB\n    if len(image.shape) == 3:\n        blurred_rgb = cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB)\n        return blurred_rgb\n    return blurred\n\ndef apply_bilateral_filter(image, diameter=9, sigma_color=75, sigma_space=75):\n    \"\"\"\n    Apply bilateral filter to an image (edge-preserving smoothing).\n\n    Args:\n        image (np.ndarray): Input image\n        diameter (int): Diameter of each pixel neighborhood\n        sigma_color (float): Filter sigma in the color space\n        sigma_space (float): Filter sigma in the coordinate space\n\n    Returns:\n        np.ndarray: Filtered image\n    \"\"\"\n    # TODO: Implement bilateral filter using cv2.bilateralFilter()\n\n    # Convert back to RGB if the input was RGB\n    if len(image.shape) == 3:\n        filtered_rgb = cv2.cvtColor(filtered, cv2.COLOR_BGR2RGB)\n        return filtered_rgb\n    return filtered\n\ndef apply_custom_kernel(image, kernel):\n    \"\"\"\n    Apply a custom kernel to an image using the filter2D function.\n\n    Args:\n        image (np.ndarray): Input image\n        kernel (np.ndarray): Custom kernel (must be odd-sized)\n\n    Returns:\n        np.ndarray: Filtered image\n    \"\"\"\n    # TODO: Implement custom kernel application using cv2.filter2D()\n\n    # Convert back to RGB if the input was RGB\n    if len(image.shape) == 3:\n        filtered_rgb = cv2.cvtColor(filtered, cv2.COLOR_BGR2RGB)\n        return filtered_rgb\n    return filtered\n\ndef create_sharpen_kernel():\n    \"\"\"\n    Create a sharpening kernel.\n\n    Returns:\n        np.ndarray: Sharpen kernel\n    \"\"\"\n    # TODO: Define and return a sharpening kernel\n\n    return kernel\n\ndef create_edge_kernel(kernel_type='sobel_x'):\n    \"\"\"\n    Create an edge detection kernel.\n\n    Args:\n        kernel_type (str): Type of kernel ('sobel_x', 'sobel_y', 'laplacian')\n\n    Returns:\n        np.ndarray: Edge detection kernel\n    \"\"\"\n    # TODO: Define and return the specified edge detection kernel\n\n    return kernel\n\ndef visualize_kernel(kernel, figsize=(5, 5)):\n    \"\"\"\n    Visualize a kernel as a heatmap.\n\n    Args:\n        kernel (np.ndarray): Kernel to visualize\n        figsize (tuple): Figure size\n    \"\"\"\n    plt.figure(figsize=figsize)\n    plt.imshow(kernel, cmap='viridis')\n    plt.colorbar(label='Weight')\n    plt.title(f'Kernel Shape: {kernel.shape}')\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            plt.text(j, i, f'{kernel[i, j]:.2f}',\n                     ha='center', va='center',\n                     color='white' if abs(kernel[i, j]) &lt; 0.5 else 'black')\n    plt.tight_layout()\n    plt.show()\n\ndef compare_blur_methods(image):\n    \"\"\"\n    Compare different blur methods side by side.\n\n    Args:\n        image (np.ndarray): Input image in BGR format\n    \"\"\"\n    # TODO: Apply different blur methods and display results side by side\n\n    pass\n\ndef examine_kernel_effects(image):\n    \"\"\"\n    Examine the effects of different kernels on an image.\n\n    Args:\n        image (np.ndarray): Input image in BGR format\n    \"\"\"\n    # TODO: Apply different custom kernels and display results\n\n    pass\n\n# Main execution\nif __name__ == \"__main__\":\n    # Sample image path - replace with your own\n    image_path = \"sample_image.jpg\"\n\n    # Load image\n    img_bgr, img_rgb = load_image(image_path)\n\n    # Display original image\n    plt.figure(figsize=(10, 8))\n    plt.imshow(img_rgb)\n    plt.title('Original Image')\n    plt.axis('off')\n    plt.show()\n\n    # TODO: Demonstrate kernel visualizations and filtering effects\n</code></pre>"},{"location":"computer_vision/02_filtering_and_convolution/#group-implementation-tasks","title":"Group Implementation Tasks","text":""},{"location":"computer_vision/02_filtering_and_convolution/#task-1-complete-the-missing-functions","title":"Task 1: Complete the Missing Functions","text":"<p>Working in pairs, complete each TODO section in the starter code:</p> <ul> <li>Person A: Implement blur filters (box, Gaussian, median)</li> <li>Person B: Implement bilateral filter and custom kernel application</li> </ul>"},{"location":"computer_vision/02_filtering_and_convolution/#task-2-kernel-exploration","title":"Task 2: Kernel Exploration","text":"<ol> <li>Create at least three different custom kernels (beyond those mentioned)</li> <li>Apply each kernel to the same image</li> <li>Document the visual effect and underlying mathematical reason for each effect</li> </ol> <p>Example table to complete:</p> Kernel Matrix Values Visual Effect Mathematical Explanation Sharpen [0,-1,0][-1,5,-1][0,-1,0] Enhances edges Amplifies center pixel while subtracting neighbors Your Custom Kernel 1 Your Custom Kernel 2 Your Custom Kernel 3"},{"location":"computer_vision/02_filtering_and_convolution/#task-3-noise-reduction-analysis","title":"Task 3: Noise Reduction Analysis","text":"<ol> <li>Add different types of noise to an image (salt-and-pepper, Gaussian)</li> <li>Apply different blur filters to each noisy image</li> <li>Compare the effectiveness of each filter for each noise type</li> <li>Measure quality using visual inspection and a numeric metric</li> </ol>"},{"location":"computer_vision/02_filtering_and_convolution/#peer-review-questions","title":"Peer Review Questions","text":"<p>After completing the implementation, review each other's code and discuss:</p> <ol> <li> <p>Implementation Choices:</p> </li> <li> <p>Which parameters work best for each filter type?</p> </li> <li> <p>How does kernel size affect the output quality and processing time?</p> </li> <li> <p>Filter Comparisons:</p> </li> <li> <p>Which blur filter preserves edges better?</p> </li> <li> <p>For which scenarios would you choose median vs. Gaussian blur?</p> </li> <li> <p>Kernel Understanding:</p> </li> <li> <p>What happens if kernel weights sum to zero? To one? To values greater than one?</p> </li> <li> <p>How do kernel dimensions affect the output?</p> </li> <li> <p>Performance Considerations:</p> </li> <li>Which filters are computationally more expensive? Why?</li> <li>How might filter operations be optimized for large images?</li> </ol>"},{"location":"computer_vision/02_filtering_and_convolution/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"computer_vision/02_filtering_and_convolution/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Kernel Size Errors</p> </li> <li> <p>Kernel sizes must be odd numbers (3, 5, 7, etc.)</p> </li> <li> <p>Larger kernels cause stronger effects but are slower</p> </li> <li> <p>Edge Artifacts</p> </li> <li> <p>Border pixels may show artifacts due to incomplete neighborhood</p> </li> <li> <p>Use border handling techniques like cv2.BORDER_REFLECT</p> </li> <li> <p>Type Conversion Issues</p> </li> <li> <p>Ensure image data types are consistent for operations</p> </li> <li> <p>Check if operation expects uint8 (0-255) or float (0.0-1.0)</p> </li> <li> <p>Performance Problems</p> </li> <li>Large kernels can be computationally expensive</li> <li>Consider using optimized versions or separable kernels</li> </ol>"},{"location":"computer_vision/02_filtering_and_convolution/#debugging-visualization","title":"Debugging Visualization","text":"<p>Create a function that shows:</p> <ol> <li>The original pixel neighborhood (e.g., 5\u00d75 region)</li> <li>The applied kernel</li> <li>The multiplication step</li> <li>The final sum/result</li> </ol>"},{"location":"computer_vision/02_filtering_and_convolution/#mini-project-custom-image-filter-application","title":"Mini-Project: Custom Image Filter Application","text":"<p>Create an application that:</p> <ol> <li>Loads an image from a file or camera</li> <li>Allows the user to design custom kernels</li> <li>Previews the effect of the kernel in real-time</li> <li>Provides a library of common filters with explanations</li> <li>Supports filter chaining (applying multiple filters in sequence)</li> </ol>"},{"location":"computer_vision/02_filtering_and_convolution/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Add noise generation and removal tools</li> <li>Implement frequency domain filtering (FFT-based)</li> <li>Create a \"smart sharpen\" that adapts to image content</li> <li>Develop a tool that suggests optimal kernel parameters</li> </ul>"},{"location":"computer_vision/02_filtering_and_convolution/#reflection-questions","title":"Reflection Questions","text":"<ol> <li>How do kernel operations relate to human visual perception?</li> <li>What's the relationship between kernel size and the amount of information lost?</li> <li>How might convolution be used in other computer vision tasks beyond filtering?</li> <li>What are the limitations of kernel-based operations?</li> <li>How do modern deep learning approaches compare to traditional kernel methods?</li> </ol>"},{"location":"computer_vision/03_edge_detection/","title":"Edge Detection Lab","text":""},{"location":"computer_vision/03_edge_detection/#objective","title":"Objective","text":"<p>Understand and implement various edge detection techniques using OpenCV. This lab explores how edges can be detected, compared, and utilized in computer vision applications.</p>"},{"location":"computer_vision/03_edge_detection/#environment-setup","title":"Environment Setup","text":"Dependency Purpose Python 3.10+ Programming language OpenCV Image processing library NumPy Numerical operations for arrays Matplotlib Visualization of images"},{"location":"computer_vision/03_edge_detection/#installation","title":"Installation","text":"<pre><code># Create virtual environment (optional)\npython -m venv cv_env\nsource cv_env/bin/activate  # On Windows: cv_env\\Scripts\\activate\n\n# Install required packages\npip install opencv-python numpy matplotlib\n</code></pre>"},{"location":"computer_vision/03_edge_detection/#verification","title":"Verification","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Verify versions\nprint(f\"OpenCV version: {cv2.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n</code></pre>"},{"location":"computer_vision/03_edge_detection/#key-concepts","title":"Key Concepts","text":""},{"location":"computer_vision/03_edge_detection/#what-are-edges","title":"What are Edges?","text":"<p>Edges in images are areas with strong intensity contrasts \u2013 a sharp change in intensity from one pixel to the next. They often represent:</p> <ul> <li>Boundaries between objects</li> <li>Changes in surface orientation or material properties</li> <li>Depth discontinuities</li> <li>Changes in scene illumination</li> </ul> <p></p>"},{"location":"computer_vision/03_edge_detection/#edge-detection-process","title":"Edge Detection Process","text":"<p>Edge detection typically involves three stages:</p> <ol> <li>Noise Reduction: Smooth the image to reduce noise (often with Gaussian blur)</li> <li>Gradient Calculation: Compute intensity gradients to find regions of rapid intensity change</li> <li>Edge Tracing/Thresholding: Apply thresholds to determine which gradients represent actual edges</li> </ol>"},{"location":"computer_vision/03_edge_detection/#common-techniques","title":"Common Techniques","text":"Method Description Strengths Weaknesses Sobel Calculates gradients using convolution Simple, detects direction Sensitive to noise Canny Multi-stage algorithm with hysteresis More precise, less noise More complex, parameter tuning needed Laplacian Second derivative operator Detects edges and corners Very sensitive to noise Scharr Modified Sobel with better rotational symmetry Better angular accuracy Similar limitations to Sobel"},{"location":"computer_vision/03_edge_detection/#starter-code","title":"Starter Code","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef load_image(image_path):\n    \"\"\"\n    Load an image and convert to RGB for display.\n\n    Args:\n        image_path (str): Path to the image file\n\n    Returns:\n        tuple: (bgr_image, rgb_image, grayscale_image)\n    \"\"\"\n    # Load the image\n    img = cv2.imread(image_path)\n    if img is None:\n        raise FileNotFoundError(f\"Could not load image from {image_path}\")\n\n    # Convert to RGB for matplotlib display\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Convert to grayscale\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    return img, img_rgb, img_gray\n\ndef display_images(images, titles, figsize=(15, 10), rows=1):\n    \"\"\"\n    Display multiple images in a grid.\n\n    Args:\n        images (list): List of images to display\n        titles (list): List of titles for each image\n        figsize (tuple): Figure size\n        rows (int): Number of rows in the grid\n    \"\"\"\n    n = len(images)\n    cols = (n + rows - 1) // rows  # Ceiling division\n    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n\n    # Make axes a 2D array if it isn't already\n    if rows == 1:\n        axes = axes.reshape(1, -1)\n    elif n == 1:\n        axes = np.array([[axes]])\n\n    # Flatten both arrays for easy iteration\n    axes_flat = axes.flatten()\n\n    for i, (img, title) in enumerate(zip(images, titles)):\n        if i &lt; len(axes_flat):\n            if len(img.shape) == 2:  # Grayscale\n                axes_flat[i].imshow(img, cmap='gray')\n            else:  # Color image\n                axes_flat[i].imshow(img)\n            axes_flat[i].set_title(title)\n            axes_flat[i].axis('off')\n\n    # Hide unused subplots\n    for i in range(n, len(axes_flat)):\n        axes_flat[i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndef apply_gaussian_blur(image, kernel_size=5, sigma=0):\n    \"\"\"\n    Apply Gaussian blur for noise reduction.\n\n    Args:\n        image (np.ndarray): Input image\n        kernel_size (int): Size of the kernel (must be odd)\n        sigma (float): Standard deviation of the Gaussian\n\n    Returns:\n        np.ndarray: Blurred image\n    \"\"\"\n    # TODO: Implement Gaussian blur pre-processing\n\n    return blurred_image\n\ndef sobel_edge_detection(image, ksize=3):\n    \"\"\"\n    Apply Sobel edge detection.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n        ksize (int): Kernel size\n\n    Returns:\n        tuple: (gradient_x, gradient_y, gradient_magnitude, gradient_direction)\n    \"\"\"\n    # TODO: Implement Sobel edge detection\n    # - Calculate gradients in x and y directions\n    # - Calculate gradient magnitude and direction\n\n    return grad_x, grad_y, magnitude, direction\n\ndef scharr_edge_detection(image):\n    \"\"\"\n    Apply Scharr edge detection.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n\n    Returns:\n        tuple: (gradient_x, gradient_y, gradient_magnitude, gradient_direction)\n    \"\"\"\n    # TODO: Implement Scharr edge detection\n    # - Calculate gradients in x and y directions\n    # - Calculate gradient magnitude and direction\n\n    return grad_x, grad_y, magnitude, direction\n\ndef laplacian_edge_detection(image, ksize=3):\n    \"\"\"\n    Apply Laplacian edge detection.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n        ksize (int): Kernel size\n\n    Returns:\n        np.ndarray: Laplacian edges\n    \"\"\"\n    # TODO: Implement Laplacian edge detection\n\n    return laplacian\n\ndef canny_edge_detection(image, low_threshold=50, high_threshold=150, aperture_size=3):\n    \"\"\"\n    Apply Canny edge detection.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n        low_threshold (int): Lower threshold for hysteresis\n        high_threshold (int): Upper threshold for hysteresis\n        aperture_size (int): Aperture size for Sobel operator\n\n    Returns:\n        np.ndarray: Canny edges\n    \"\"\"\n    # TODO: Implement Canny edge detection\n\n    return edges\n\ndef compare_edge_detectors(image):\n    \"\"\"\n    Compare different edge detection methods side by side.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n    \"\"\"\n    # TODO: Apply different edge detection methods and display results\n\n    pass\n\ndef threshold_edges(edges, threshold=127):\n    \"\"\"\n    Apply binary thresholding to edge images.\n\n    Args:\n        edges (np.ndarray): Edge detection output\n        threshold (int): Threshold value\n\n    Returns:\n        np.ndarray: Binary edge image\n    \"\"\"\n    # TODO: Implement thresholding for edge images\n\n    return binary_edges\n\ndef visualize_gradient_directions(magnitude, direction):\n    \"\"\"\n    Visualize gradient directions using color coding.\n\n    Args:\n        magnitude (np.ndarray): Gradient magnitude\n        direction (np.ndarray): Gradient direction in radians\n\n    Returns:\n        np.ndarray: Colorized direction visualization\n    \"\"\"\n    # TODO: Implement gradient direction visualization\n    # Hint: Convert angle to hue, magnitude to value in HSV color space\n\n    return direction_visualization\n\ndef adaptive_edge_detection(image, method='canny', params=None):\n    \"\"\"\n    Apply edge detection with adaptive parameter selection.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n        method (str): Edge detection method ('canny', 'sobel', etc.)\n        params (dict): Optional parameters to override defaults\n\n    Returns:\n        np.ndarray: Edge detection result\n    \"\"\"\n    # TODO: Implement adaptive parameter selection based on image statistics\n\n    return edges\n\n# Main execution\nif __name__ == \"__main__\":\n    # Sample image path - replace with your own\n    image_path = \"sample_image.jpg\"\n\n    # Load image\n    img_bgr, img_rgb, img_gray = load_image(image_path)\n\n    # Display original image\n    plt.figure(figsize=(10, 8))\n    plt.imshow(img_rgb)\n    plt.title('Original Image')\n    plt.axis('off')\n    plt.show()\n\n    # TODO: Demonstrate edge detection methods and comparison\n</code></pre>"},{"location":"computer_vision/03_edge_detection/#group-implementation-tasks","title":"Group Implementation Tasks","text":""},{"location":"computer_vision/03_edge_detection/#task-1-complete-the-missing-functions","title":"Task 1: Complete the Missing Functions","text":"<p>Working in pairs, complete each TODO section in the starter code:</p> <ul> <li>Person A: Implement Sobel and Laplacian edge detection functions</li> <li>Person B: Implement Canny edge detection and gradient visualization functions</li> </ul>"},{"location":"computer_vision/03_edge_detection/#task-2-parameter-experimentation","title":"Task 2: Parameter Experimentation","text":"<ol> <li>Choose one image and apply the Canny edge detector with different parameters</li> <li>Create a matrix of results showing how changing thresholds affects edge detection</li> <li>Document the optimal parameters for:</li> <li>A high-contrast architectural image</li> <li>A low-contrast natural scene</li> <li>A busy, textured image (like fabric or foliage)</li> </ol> <p>Example experiment table:</p> Parameter Set Low Threshold High Threshold Aperture Size Result Image Observations Default 50 150 3 [Image] Baseline detection Low sensitivity 100 200 3 [Image] Missing subtle edges High sensitivity 30 100 3 [Image] More noise, more details ..."},{"location":"computer_vision/03_edge_detection/#task-3-edge-detection-applications","title":"Task 3: Edge Detection Applications","text":"<p>Implement one of these practical applications:</p> <ol> <li>Shape Counter: Count shapes in an image using edge detection and contour finding</li> <li>Document Scanner: Detect document boundaries using edge detection</li> <li>License Plate Finder: Locate rectangular license plates in vehicle images</li> <li>Panorama Stitcher: Find features for image stitching using edge information</li> </ol>"},{"location":"computer_vision/03_edge_detection/#peer-review-questions","title":"Peer Review Questions","text":"<p>After completing the implementation, review each other's code and discuss:</p> <ol> <li> <p>Method Comparison:</p> </li> <li> <p>Which edge detection method produces the cleanest results?</p> </li> <li> <p>When would you choose Sobel over Canny, or vice versa?</p> </li> <li> <p>Parameter Sensitivity:</p> </li> <li> <p>How sensitive is each method to parameter changes?</p> </li> <li> <p>Which parameters had the largest impact on the results?</p> </li> <li> <p>Processing Steps:</p> </li> <li> <p>What effect does pre-processing (blur) have on edge detection?</p> </li> <li> <p>How does post-processing (thresholding) affect the final result?</p> </li> <li> <p>Application Considerations:</p> </li> <li> <p>How well would these edge detectors work in low-light conditions?</p> </li> <li> <p>What pre-processing would help with noisy images?</p> </li> <li> <p>Performance Analysis:</p> </li> <li>Which methods are computationally more efficient?</li> <li>How does image size affect processing time?</li> </ol>"},{"location":"computer_vision/03_edge_detection/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"computer_vision/03_edge_detection/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Too Many Edges Detected</p> </li> <li> <p>Image is too noisy or textured</p> </li> <li>Threshold values are too low</li> <li>Apply Gaussian blur before edge detection</li> <li> <p>Increase threshold values</p> </li> <li> <p>Missing Important Edges</p> </li> <li> <p>Threshold values are too high</p> </li> <li>Pre-processing may be removing important details</li> <li>Decrease threshold values</li> <li> <p>Try adaptive thresholding</p> </li> <li> <p>Broken or Disconnected Edges</p> </li> <li> <p>Typical with simple methods like Sobel</p> </li> <li>Try Canny edge detection which includes hysteresis thresholding</li> <li> <p>Post-process with morphological operations</p> </li> <li> <p>Performance Issues</p> </li> <li>Edge detection can be slow on large images</li> <li>Downscale the image before processing</li> <li>Use optimized implementations (hardware acceleration)</li> </ol>"},{"location":"computer_vision/03_edge_detection/#debugging-tools","title":"Debugging Tools","text":"<p>Create a function that shows:</p> <ol> <li>Original image alongside the edge detection result</li> <li>Gradient magnitude histogram to help select appropriate thresholds</li> <li>Zoomed view of specific regions to examine edge quality</li> </ol>"},{"location":"computer_vision/03_edge_detection/#mini-project-edge-based-feature-detection","title":"Mini-Project: Edge-Based Feature Detection","text":"<p>Create an application that:</p> <ol> <li>Detects and counts objects in an image using edge detection</li> <li>Classifies detected objects by shape (circle, square, triangle, etc.)</li> <li>Measures object dimensions and distances between objects</li> <li>Visualizes results with annotations and statistics</li> </ol>"},{"location":"computer_vision/03_edge_detection/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Implement real-time edge detection using webcam input</li> <li>Create an edge-based image stylization filter (cartoon, sketch)</li> <li>Build a simple optical character recognition (OCR) system for printed text</li> <li>Develop an edge-based image compression technique</li> </ul>"},{"location":"computer_vision/03_edge_detection/#reflection-questions","title":"Reflection Questions","text":"<ol> <li>How do different lighting conditions affect edge detection results?</li> <li>Why might combining multiple edge detection techniques yield better results?</li> <li>How does the human visual system detect edges compared to algorithms?</li> <li>What role does edge detection play in more complex computer vision tasks?</li> <li>How might deep learning approaches to edge detection differ from traditional methods?</li> </ol>"},{"location":"computer_vision/04_contour_detection_and_shapes/","title":"Contour Detection and Shape Analysis Lab","text":""},{"location":"computer_vision/04_contour_detection_and_shapes/#objective","title":"Objective","text":"<p>Learn to detect, analyze, and classify shapes in images using contour detection techniques. This lab builds on edge detection knowledge to identify and measure distinct objects in images.</p>"},{"location":"computer_vision/04_contour_detection_and_shapes/#environment-setup","title":"Environment Setup","text":"Dependency Purpose Python 3.10+ Programming language OpenCV Image processing library NumPy Numerical operations for arrays Matplotlib Visualization of images"},{"location":"computer_vision/04_contour_detection_and_shapes/#installation","title":"Installation","text":"<pre><code># Create virtual environment (optional)\npython -m venv cv_env\nsource cv_env/bin/activate  # On Windows: cv_env\\Scripts\\activate\n\n# Install required packages\npip install opencv-python numpy matplotlib\n</code></pre>"},{"location":"computer_vision/04_contour_detection_and_shapes/#verification","title":"Verification","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Verify versions\nprint(f\"OpenCV version: {cv2.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n</code></pre>"},{"location":"computer_vision/04_contour_detection_and_shapes/#key-concepts","title":"Key Concepts","text":""},{"location":"computer_vision/04_contour_detection_and_shapes/#what-are-contours","title":"What are Contours?","text":"<p>Contours are continuous curves that follow the boundaries of objects in an image. They represent shapes at constant intensity levels and are fundamental for:</p> <ul> <li>Object detection and counting</li> <li>Shape analysis and recognition</li> <li>Feature extraction and measurement</li> <li>Image segmentation</li> </ul> <p></p>"},{"location":"computer_vision/04_contour_detection_and_shapes/#contour-hierarchy","title":"Contour Hierarchy","text":"<p>Contours can have parent-child relationships:</p> <ul> <li>External contours: Outline the outer boundaries of objects</li> <li>Internal contours: Represent holes or nested objects within other contours</li> </ul>"},{"location":"computer_vision/04_contour_detection_and_shapes/#shape-descriptors","title":"Shape Descriptors","text":"<p>Various metrics can be used to analyze and classify shapes:</p> <ul> <li>Area and Perimeter: Basic size measurements</li> <li>Aspect Ratio: Width to height ratio</li> <li>Extent: Ratio of contour area to bounding rectangle area</li> <li>Solidity: Ratio of contour area to convex hull area</li> <li>Moments: Shape statistics used for centroid calculation and more</li> <li>Hu Moments: Shape descriptors invariant to rotation, scale, and reflection</li> </ul>"},{"location":"computer_vision/04_contour_detection_and_shapes/#starter-code","title":"Starter Code","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef load_image(image_path):\n    \"\"\"\n    Load an image and convert to RGB for display.\n\n    Args:\n        image_path (str): Path to the image file\n\n    Returns:\n        tuple: (bgr_image, rgb_image, grayscale_image)\n    \"\"\"\n    # Load the image\n    img = cv2.imread(image_path)\n    if img is None:\n        raise FileNotFoundError(f\"Could not load image from {image_path}\")\n\n    # Convert to RGB for matplotlib display\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Convert to grayscale\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    return img, img_rgb, img_gray\n\ndef display_images(images, titles, figsize=(15, 10), rows=1):\n    \"\"\"\n    Display multiple images in a grid.\n\n    Args:\n        images (list): List of images to display\n        titles (list): List of titles for each image\n        figsize (tuple): Figure size\n        rows (int): Number of rows in the grid\n    \"\"\"\n    n = len(images)\n    cols = (n + rows - 1) // rows  # Ceiling division\n    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n\n    # Make axes a 2D array if it isn't already\n    if rows == 1 and cols == 1:\n        axes = np.array([[axes]])\n    elif rows == 1:\n        axes = axes.reshape(1, -1)\n    elif cols == 1:\n        axes = axes.reshape(-1, 1)\n\n    # Flatten both arrays for easy iteration\n    axes_flat = axes.flatten()\n\n    for i, (img, title) in enumerate(zip(images, titles)):\n        if i &lt; len(axes_flat):\n            if len(img.shape) == 2:  # Grayscale\n                axes_flat[i].imshow(img, cmap='gray')\n            else:  # Color image\n                axes_flat[i].imshow(img)\n            axes_flat[i].set_title(title)\n            axes_flat[i].axis('off')\n\n    # Hide unused subplots\n    for i in range(n, len(axes_flat)):\n        axes_flat[i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndef preprocess_for_contours(image, blur_ksize=5, threshold_method='binary'):\n    \"\"\"\n    Preprocess an image for contour detection.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n        blur_ksize (int): Kernel size for Gaussian blur\n        threshold_method (str): Thresholding method ('binary', 'otsu', 'adaptive')\n\n    Returns:\n        np.ndarray: Binary image ready for contour detection\n    \"\"\"\n    # TODO: Implement image preprocessing for contour detection\n    # 1. Apply Gaussian blur to reduce noise\n    # 2. Apply appropriate thresholding based on method parameter\n\n    return binary_image\n\ndef find_contours(binary_image, retrieval_mode=cv2.RETR_EXTERNAL):\n    \"\"\"\n    Find contours in a binary image.\n\n    Args:\n        binary_image (np.ndarray): Binary input image\n        retrieval_mode (int): Contour retrieval mode\n\n    Returns:\n        tuple: (list of contours, hierarchy)\n    \"\"\"\n    # TODO: Implement contour finding\n    # Use cv2.findContours() with appropriate parameters\n\n    return contours, hierarchy\n\ndef draw_contours(image, contours, color=(0, 255, 0), thickness=2):\n    \"\"\"\n    Draw contours on an image.\n\n    Args:\n        image (np.ndarray): Image to draw on\n        contours (list): Contours to draw\n        color (tuple): BGR color\n        thickness (int): Line thickness\n\n    Returns:\n        np.ndarray: Image with contours drawn\n    \"\"\"\n    # TODO: Implement contour drawing\n    # Create a copy of the image and draw contours on it\n\n    return img_with_contours\n\ndef calculate_shape_descriptors(contour):\n    \"\"\"\n    Calculate various shape descriptors for a contour.\n\n    Args:\n        contour (np.ndarray): Input contour\n\n    Returns:\n        dict: Dictionary of shape descriptors\n    \"\"\"\n    # TODO: Implement shape descriptor calculations\n    # Include area, perimeter, aspect ratio, extent, solidity, etc.\n\n    return descriptors\n\ndef identify_shape(contour, epsilon_factor=0.04):\n    \"\"\"\n    Identify the basic shape of a contour.\n\n    Args:\n        contour (np.ndarray): Input contour\n        epsilon_factor (float): Factor for polygon approximation\n\n    Returns:\n        str: Identified shape ('circle', 'triangle', 'square', 'rectangle', etc.)\n    \"\"\"\n    # TODO: Implement shape identification\n    # Use approxPolyDP to approximate the contour to a polygon\n    # Based on the number of vertices, identify the shape\n\n    return shape_name\n\ndef filter_contours(contours, min_area=100, max_area=None):\n    \"\"\"\n    Filter contours by area.\n\n    Args:\n        contours (list): List of contours\n        min_area (float): Minimum area threshold\n        max_area (float, optional): Maximum area threshold\n\n    Returns:\n        list: Filtered contours\n    \"\"\"\n    # TODO: Implement contour filtering by area\n\n    return filtered_contours\n\ndef extract_shape_features(image, contours):\n    \"\"\"\n    Extract features from each shape in the image.\n\n    Args:\n        image (np.ndarray): Original image\n        contours (list): List of contours\n\n    Returns:\n        list: List of shape features (descriptors, color, etc.)\n    \"\"\"\n    # TODO: Implement feature extraction for each contour\n    # Calculate shape descriptors and extract color information\n\n    return shape_features\n\ndef classify_shapes(contours):\n    \"\"\"\n    Classify contours into shape categories.\n\n    Args:\n        contours (list): List of contours\n\n    Returns:\n        dict: Dictionary mapping shape types to contour indices\n    \"\"\"\n    # TODO: Implement shape classification\n    # Group contours by their identified shapes\n\n    return shape_categories\n\ndef visualize_shape_analysis(image, contours, features):\n    \"\"\"\n    Visualize shape analysis with annotations.\n\n    Args:\n        image (np.ndarray): Original image\n        contours (list): List of contours\n        features (list): List of shape features\n\n    Returns:\n        np.ndarray: Annotated image\n    \"\"\"\n    # TODO: Implement visualization with annotations\n    # Draw contours, labels, and key measurements\n\n    return annotated_image\n\n# Main execution\nif __name__ == \"__main__\":\n    # Sample image path - replace with your own\n    image_path = \"sample_shapes.jpg\"\n\n    # Load image\n    img_bgr, img_rgb, img_gray = load_image(image_path)\n\n    # Display original image\n    plt.figure(figsize=(10, 8))\n    plt.imshow(img_rgb)\n    plt.title('Original Image')\n    plt.axis('off')\n    plt.show()\n\n    # TODO: Demonstrate contour detection and shape analysis\n</code></pre>"},{"location":"computer_vision/04_contour_detection_and_shapes/#group-implementation-tasks","title":"Group Implementation Tasks","text":""},{"location":"computer_vision/04_contour_detection_and_shapes/#task-1-complete-the-missing-functions","title":"Task 1: Complete the Missing Functions","text":"<p>Working in pairs, complete each TODO section in the starter code:</p> <ul> <li>Person A: Implement preprocessing, contour finding, and contour drawing functions</li> <li>Person B: Implement shape descriptor calculation and shape identification functions</li> </ul>"},{"location":"computer_vision/04_contour_detection_and_shapes/#task-2-shape-dataset-creation","title":"Task 2: Shape Dataset Creation","text":"<ol> <li>Create a dataset of basic shapes:</li> <li>Take photos of simple shapes (circles, triangles, squares, etc.)</li> <li>Draw shapes digitally using drawing tools</li> <li>Generate synthetic shapes with Python</li> <li>Test your contour detection and shape classification on this dataset</li> <li>Document the accuracy of your classification</li> </ol>"},{"location":"computer_vision/04_contour_detection_and_shapes/#task-3-real-world-shape-detection","title":"Task 3: Real-World Shape Detection","text":"<p>Choose one of these practical applications and implement it:</p> <ol> <li>Board Game Piece Counter: Detect and count different game pieces by color and shape</li> <li>Coin Counter: Identify and count different coins in an image</li> <li>Logo Detector: Find company logos in images based on shape characteristics</li> <li>Traffic Sign Detector: Detect and classify traffic signs by shape</li> </ol>"},{"location":"computer_vision/04_contour_detection_and_shapes/#peer-review-questions","title":"Peer Review Questions","text":"<p>After completing the implementation, review each other's code and discuss:</p> <ol> <li> <p>Preprocessing Effectiveness:</p> </li> <li> <p>How does the choice of thresholding method affect contour detection?</p> </li> <li> <p>What preprocessing steps worked best for different image types?</p> </li> <li> <p>Shape Descriptor Performance:</p> </li> <li> <p>Which shape descriptors were most reliable for classification?</p> </li> <li> <p>How invariant are your descriptors to rotation and scale changes?</p> </li> <li> <p>Contour Hierarchy:</p> </li> <li> <p>How would you handle nested shapes (shapes within shapes)?</p> </li> <li> <p>What's the best way to represent parent-child relationships?</p> </li> <li> <p>Edge Cases and Challenges:</p> </li> <li> <p>How well does your code handle touching or overlapping shapes?</p> </li> <li> <p>What strategies could improve shape detection in complex images?</p> </li> <li> <p>Optimization Considerations:</p> </li> <li>Which steps in the process are most computationally expensive?</li> <li>How might the algorithm be optimized for real-time applications?</li> </ol>"},{"location":"computer_vision/04_contour_detection_and_shapes/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"computer_vision/04_contour_detection_and_shapes/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Poor Contour Detection</p> </li> <li> <p>Image has low contrast or is noisy</p> </li> <li>Thresholding parameters are not appropriate</li> <li>Try adaptive thresholding or adjust blur parameters</li> <li> <p>Use morphological operations (opening/closing) to clean up binary image</p> </li> <li> <p>Inaccurate Shape Classification</p> </li> <li> <p>Contour approximation epsilon is too large or small</p> </li> <li>Shape criteria are too strict or lenient</li> <li>Adjust epsilon factor for polygon approximation</li> <li> <p>Implement tolerance ranges for shape metrics</p> </li> <li> <p>Missing Small Objects</p> </li> <li> <p>Minimum area threshold is too high</p> </li> <li>Small objects are being merged with noise reduction</li> <li>Lower area threshold or adjust preprocessing parameters</li> <li> <p>Use more targeted noise reduction techniques</p> </li> <li> <p>Merged Objects</p> </li> <li>Objects are touching or overlapping</li> <li>Try watershed segmentation for touching objects</li> <li>Use distance transform and markers for separation</li> <li>Consider convexity defects for splitting merged shapes</li> </ol>"},{"location":"computer_vision/04_contour_detection_and_shapes/#debugging-tools","title":"Debugging Tools","text":"<p>Create functions that visualize:</p> <ol> <li>Each preprocessing step side-by-side</li> <li>Contour hierarchies with color-coding</li> <li>Shape approximation polygons overlaid on original contours</li> <li>Feature values as bar charts for each detected shape</li> </ol>"},{"location":"computer_vision/04_contour_detection_and_shapes/#mini-project-object-counter-and-classifier","title":"Mini-Project: Object Counter and Classifier","text":"<p>Create an application that:</p> <ol> <li>Takes an image containing multiple objects</li> <li>Segments and identifies each distinct object</li> <li>Classifies objects by shape and color</li> <li>Counts objects in each category</li> <li>Generates a summary report with visualizations</li> </ol> <p>Example output:</p> <pre><code>Image Analysis Report:\n- Total objects detected: 15\n- Circles: 5 (3 red, 2 blue)\n- Squares: 4 (2 green, 2 yellow)\n- Triangles: 6 (4 red, 2 green)\n</code></pre>"},{"location":"computer_vision/04_contour_detection_and_shapes/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Add area and perimeter measurements for each object</li> <li>Implement object tracking across video frames</li> <li>Create a spatial relationship analyzer (which objects are near others)</li> <li>Build a simple picking robot simulator that plans grasping based on shape</li> </ul>"},{"location":"computer_vision/04_contour_detection_and_shapes/#reflection-questions","title":"Reflection Questions","text":"<ol> <li>How does lighting affect contour detection and shape analysis?</li> <li>What challenges arise when detecting irregular or complex shapes?</li> <li>How might machine learning improve shape classification compared to geometric rules?</li> <li>What real-world applications could benefit from contour-based shape analysis?</li> <li>How would you approach detecting partially occluded shapes?</li> </ol>"},{"location":"computer_vision/05_face_detection/","title":"Face Detection Lab","text":""},{"location":"computer_vision/05_face_detection/#objective","title":"Objective","text":"<p>Learn to detect and analyze human faces in images using both traditional computer vision techniques and deep learning approaches. This lab explores the foundations of face detection, its applications, and limitations.</p>"},{"location":"computer_vision/05_face_detection/#environment-setup","title":"Environment Setup","text":"Dependency Purpose Python 3.10+ Programming language OpenCV Image processing and computer vision library NumPy Numerical operations for arrays Matplotlib Visualization of images dlib (optional) Additional face detection and facial landmark detection"},{"location":"computer_vision/05_face_detection/#installation","title":"Installation","text":"<pre><code># Create virtual environment (optional)\npython -m venv cv_env\nsource cv_env/bin/activate  # On Windows: cv_env\\Scripts\\activate\n\n# Install required packages\npip install opencv-python numpy matplotlib\n\n# Optional: Install dlib (may require additional system dependencies)\n# pip install dlib\n</code></pre>"},{"location":"computer_vision/05_face_detection/#verification","title":"Verification","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Verify versions\nprint(f\"OpenCV version: {cv2.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n</code></pre>"},{"location":"computer_vision/05_face_detection/#key-concepts","title":"Key Concepts","text":""},{"location":"computer_vision/05_face_detection/#what-is-face-detection","title":"What is Face Detection?","text":"<p>Face detection is the computer vision technology that locates and identifies human faces in digital images. It's a specific case of object detection that focuses on finding instances of human faces, regardless of position, scale, orientation, pose, or lighting.</p> <p></p>"},{"location":"computer_vision/05_face_detection/#main-approaches","title":"Main Approaches","text":"Method Description Strengths Weaknesses Haar Cascades Uses Haar-like features and cascade classifiers Fast, low resource usage Less accurate with varied poses, occlusions HOG + SVM Histogram of Oriented Gradients with Support Vector Machines Better with varied poses More computationally intensive than Haar Deep Learning CNN-based detectors like MTCNN, SSD, YOLO Most accurate, handles varied conditions Higher computational requirements"},{"location":"computer_vision/05_face_detection/#applications","title":"Applications","text":"<ul> <li>Security and surveillance systems</li> <li>Photography (auto-focus, exposure)</li> <li>Biometric authentication</li> <li>Emotion analysis</li> <li>Demographic studies</li> <li>Social media filters and effects</li> </ul>"},{"location":"computer_vision/05_face_detection/#starter-code","title":"Starter Code","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom pathlib import Path\n\ndef load_image(image_path):\n    \"\"\"\n    Load an image and convert to RGB for display.\n\n    Args:\n        image_path (str): Path to the image file\n\n    Returns:\n        tuple: (bgr_image, rgb_image)\n    \"\"\"\n    # Load the image\n    img = cv2.imread(image_path)\n    if img is None:\n        raise FileNotFoundError(f\"Could not load image from {image_path}\")\n\n    # Convert to RGB for matplotlib display\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    return img, img_rgb\n\ndef display_images(images, titles, figsize=(15, 10), rows=1):\n    \"\"\"\n    Display multiple images in a grid.\n\n    Args:\n        images (list): List of images to display\n        titles (list): List of titles for each image\n        figsize (tuple): Figure size\n        rows (int): Number of rows in the grid\n    \"\"\"\n    n = len(images)\n    cols = (n + rows - 1) // rows  # Ceiling division\n    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n\n    # Make axes a 2D array if it isn't already\n    if rows == 1 and cols == 1:\n        axes = np.array([[axes]])\n    elif rows == 1:\n        axes = axes.reshape(1, -1)\n    elif cols == 1:\n        axes = axes.reshape(-1, 1)\n\n    # Flatten both arrays for easy iteration\n    axes_flat = axes.flatten()\n\n    for i, (img, title) in enumerate(zip(images, titles)):\n        if i &lt; len(axes_flat):\n            axes_flat[i].imshow(img)\n            axes_flat[i].set_title(title)\n            axes_flat[i].axis('off')\n\n    # Hide unused subplots\n    for i in range(n, len(axes_flat)):\n        axes_flat[i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndef load_haar_cascade(cascade_type='face'):\n    \"\"\"\n    Load a Haar cascade classifier.\n\n    Args:\n        cascade_type (str): Type of cascade to load ('face', 'eyes', 'smile', etc.)\n\n    Returns:\n        cv2.CascadeClassifier: Loaded cascade classifier\n    \"\"\"\n    # TODO: Implement loading different Haar cascade files\n    # Use cv2.data.haarcascades directory to locate the XML files\n\n    return cascade_classifier\n\ndef detect_faces_haar(image, scale_factor=1.1, min_neighbors=5, min_size=(30, 30)):\n    \"\"\"\n    Detect faces using Haar cascade classifier.\n\n    Args:\n        image (np.ndarray): Input image (grayscale or BGR)\n        scale_factor (float): Scale factor for the detection algorithm\n        min_neighbors (int): Minimum neighbors for detection\n        min_size (tuple): Minimum size of detected faces\n\n    Returns:\n        tuple: (image with detections drawn, list of face rectangles)\n    \"\"\"\n    # TODO: Implement face detection using Haar cascades\n    # 1. Convert to grayscale if needed\n    # 2. Apply face detection\n    # 3. Draw rectangles around detected faces\n\n    return image_with_faces, faces\n\ndef detect_faces_dnn(image, confidence_threshold=0.5):\n    \"\"\"\n    Detect faces using a pre-trained deep neural network.\n\n    Args:\n        image (np.ndarray): Input image in BGR format\n        confidence_threshold (float): Minimum confidence for detections\n\n    Returns:\n        tuple: (image with detections drawn, list of face rectangles with confidences)\n    \"\"\"\n    # TODO: Implement face detection using OpenCV's DNN module\n    # 1. Load the pre-trained model\n    # 2. Prepare the image (create blob)\n    # 3. Run inference\n    # 4. Process detections\n\n    return image_with_faces, faces\n\ndef compare_detection_methods(image):\n    \"\"\"\n    Compare different face detection methods side by side.\n\n    Args:\n        image (np.ndarray): Input image in BGR format\n    \"\"\"\n    # TODO: Apply different face detection methods and display results\n\n    pass\n\ndef detect_facial_features(image, face_rect):\n    \"\"\"\n    Detect facial features (eyes, nose, mouth) within a detected face.\n\n    Args:\n        image (np.ndarray): Input image\n        face_rect (tuple): Face rectangle (x, y, w, h)\n\n    Returns:\n        dict: Dictionary of detected facial features\n    \"\"\"\n    # TODO: Implement facial feature detection\n    # Use appropriate Haar cascades for eyes, nose, mouth\n\n    return facial_features\n\ndef analyze_faces(image, faces):\n    \"\"\"\n    Analyze detected faces for additional information.\n\n    Args:\n        image (np.ndarray): Input image\n        faces (list): List of face rectangles\n\n    Returns:\n        list: List of face analyses (position, size, etc.)\n    \"\"\"\n    # TODO: Implement basic face analysis\n    # Calculate position, size, relative position to image\n\n    return face_analyses\n\ndef draw_detections(image, detections, detection_type='face'):\n    \"\"\"\n    Draw detection results on an image.\n\n    Args:\n        image (np.ndarray): Input image\n        detections (list): List of detections (rectangles, landmarks, etc.)\n        detection_type (str): Type of detection ('face', 'features', etc.)\n\n    Returns:\n        np.ndarray: Image with detections drawn\n    \"\"\"\n    # TODO: Implement visualization for different detection types\n\n    return annotated_image\n\ndef get_model_files(model_type='dnn_face'):\n    \"\"\"\n    Get the model files for the specified model type.\n\n    Args:\n        model_type (str): Type of model ('dnn_face', 'dlib_landmarks', etc.)\n\n    Returns:\n        dict: Dictionary with model file paths\n    \"\"\"\n    # Define model directories and filenames\n    models_dir = Path(\"models\")\n\n    # Ensure models directory exists\n    os.makedirs(models_dir, exist_ok=True)\n\n    # Define model paths for different model types\n    model_paths = {\n        'dnn_face': {\n            'proto': models_dir / \"deploy.prototxt\",\n            'model': models_dir / \"res10_300x300_ssd_iter_140000.caffemodel\",\n            'urls': {\n                'proto': \"https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\",\n                'model': \"https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\"\n            }\n        }\n    }\n\n    # Check if model files exist, if not, provide instructions to download\n    model_info = model_paths.get(model_type)\n    if model_info:\n        for key, path in model_info.items():\n            if key != 'urls' and not path.exists():\n                print(f\"Missing {model_type} {key} file at {path}\")\n                print(f\"Download from: {model_info['urls'][key]}\")\n                print(f\"Or use: curl -o {path} {model_info['urls'][key]}\")\n\n    return model_info\n\n# Main execution\nif __name__ == \"__main__\":\n    # Sample image path - replace with your own\n    image_path = \"sample_faces.jpg\"\n\n    # Load image\n    img_bgr, img_rgb = load_image(image_path)\n\n    # Display original image\n    plt.figure(figsize=(10, 8))\n    plt.imshow(img_rgb)\n    plt.title('Original Image')\n    plt.axis('off')\n    plt.show()\n\n    # Check model files\n    get_model_files()\n\n    # TODO: Demonstrate face detection methods\n</code></pre>"},{"location":"computer_vision/05_face_detection/#group-implementation-tasks","title":"Group Implementation Tasks","text":""},{"location":"computer_vision/05_face_detection/#task-1-complete-the-missing-functions","title":"Task 1: Complete the Missing Functions","text":"<p>Working in pairs, complete each TODO section in the starter code:</p> <ul> <li>Person A: Implement Haar cascade face detection and facial feature detection</li> <li>Person B: Implement DNN-based face detection and comparative analysis</li> </ul>"},{"location":"computer_vision/05_face_detection/#task-2-face-detection-experiments","title":"Task 2: Face Detection Experiments","text":"<ol> <li> <p>Test face detection on a diverse set of images with:</p> </li> <li> <p>Multiple faces at different distances</p> </li> <li>Various poses (profile, tilted)</li> <li>Different lighting conditions</li> <li>Occlusions (glasses, masks, partial faces)</li> <li> <p>Diverse ethnicities and ages</p> </li> <li> <p>Create a performance matrix documenting success rates for each method:</p> </li> </ol> Image Type Haar Cascade DNN Model Notes Frontal faces Profile faces Group photo Low light With occlusions"},{"location":"computer_vision/05_face_detection/#task-3-false-positive-analysis","title":"Task 3: False Positive Analysis","text":"<ol> <li>Run face detection on a set of images containing:</li> <li>No faces</li> <li>Face-like patterns (e.g., electrical outlets, patterns in nature)</li> <li>Cartoon or drawn faces</li> <li>Document false positives and tune parameters to reduce them</li> <li>Analyze what visual patterns trigger false detections</li> </ol>"},{"location":"computer_vision/05_face_detection/#peer-review-questions","title":"Peer Review Questions","text":"<p>After completing the implementation, review each other's code and discuss:</p> <ol> <li> <p>Detection Method Comparison:</p> </li> <li> <p>Which method performed best overall?</p> </li> <li> <p>What were the tradeoffs between speed and accuracy?</p> </li> <li> <p>Parameter Sensitivity:</p> </li> <li> <p>How did changing parameters affect detection results?</p> </li> <li> <p>Which parameters had the largest impact on false positives vs. false negatives?</p> </li> <li> <p>Edge Cases:</p> </li> <li> <p>What types of faces were most challenging to detect?</p> </li> <li> <p>How well did the methods handle occlusions or unusual poses?</p> </li> <li> <p>Computational Requirements:</p> </li> <li> <p>How did processing time compare between methods?</p> </li> <li> <p>What are the memory requirements for each approach?</p> </li> <li> <p>Real-World Applications:</p> </li> <li>For a real-time application, which method would you choose and why?</li> <li>What preprocessing steps might improve detection in challenging conditions?</li> </ol>"},{"location":"computer_vision/05_face_detection/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"computer_vision/05_face_detection/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Missing Model Files</p> </li> <li> <p>Some deep learning models need to be downloaded separately</p> </li> <li>Follow the instructions provided by <code>get_model_files()</code> function</li> <li> <p>Check file paths and permissions</p> </li> <li> <p>Poor Detection Results</p> </li> <li> <p>Adjust parameters (scale_factor, min_neighbors for Haar, confidence threshold for DNN)</p> </li> <li>Try image preprocessing (histogram equalization, normalization)</li> <li> <p>Consider image resolution (too small or too large can affect results)</p> </li> <li> <p>False Positives</p> </li> <li> <p>Increase confidence thresholds or min_neighbors</p> </li> <li>Apply additional validation (e.g., check for facial features within detected faces)</li> <li> <p>Use multiple detection passes with different parameters and take intersections</p> </li> <li> <p>Performance Issues</p> </li> <li>Resize images before processing for faster detection</li> <li>Use hardware acceleration if available (GPU, OpenCL)</li> <li>Consider lighter models for real-time applications</li> </ol>"},{"location":"computer_vision/05_face_detection/#debugging-tools","title":"Debugging Tools","text":"<p>Create functions that:</p> <ol> <li>Visualize detection confidence scores</li> <li>Show step-by-step processing for DNN detection</li> <li>Compare detection time and accuracy across methods</li> </ol>"},{"location":"computer_vision/05_face_detection/#mini-project-face-detection-application","title":"Mini-Project: Face Detection Application","text":"<p>Create an application that:</p> <ol> <li>Processes images from multiple sources (files, camera, or video)</li> <li>Detects faces using the best method for each scenario</li> <li>Tracks detection statistics (confidence, size, position)</li> <li>Implements simple face recognition (optional, using face embeddings)</li> <li>Saves annotated results and provides analysis</li> </ol>"},{"location":"computer_vision/05_face_detection/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Add age and gender estimation</li> <li>Implement emotion detection from facial expressions</li> <li>Create a privacy tool that automatically blurs faces</li> <li>Build a face counting system for crowd analysis</li> <li>Develop a face-based UI control system (e.g., scrolling based on head position)</li> </ul>"},{"location":"computer_vision/05_face_detection/#reflection-questions","title":"Reflection Questions","text":"<ol> <li>What ethical considerations arise when implementing face detection systems?</li> <li>How might biases in training data affect face detection accuracy across different demographics?</li> <li>What advancements in deep learning have improved face detection in recent years?</li> <li>How do face detection systems handle challenging cases like identical twins or face-altering makeup?</li> <li>What privacy controls should be implemented in systems using face detection?</li> </ol>"},{"location":"computer_vision/06_image_segmentation/","title":"Image Segmentation Lab","text":""},{"location":"computer_vision/06_image_segmentation/#objective","title":"Objective","text":"<p>Learn to segment images by dividing them into meaningful regions or objects. This lab covers basic segmentation techniques that form the foundation for more advanced computer vision applications like object detection, scene understanding, and image editing.</p>"},{"location":"computer_vision/06_image_segmentation/#environment-setup","title":"Environment Setup","text":"Dependency Purpose Python 3.10+ Programming language OpenCV Image processing library NumPy Numerical operations for arrays Matplotlib Visualization of images scikit-image Additional segmentation algorithms"},{"location":"computer_vision/06_image_segmentation/#installation","title":"Installation","text":"<pre><code># Create virtual environment (optional)\npython -m venv cv_env\nsource cv_env/bin/activate  # On Windows: cv_env\\Scripts\\activate\n\n# Install required packages\npip install opencv-python numpy matplotlib scikit-image\n</code></pre>"},{"location":"computer_vision/06_image_segmentation/#verification","title":"Verification","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import segmentation\n\n# Verify versions\nprint(f\"OpenCV version: {cv2.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"scikit-image version: {segmentation.__version__}\")\n</code></pre>"},{"location":"computer_vision/06_image_segmentation/#key-concepts","title":"Key Concepts","text":""},{"location":"computer_vision/06_image_segmentation/#what-is-image-segmentation","title":"What is Image Segmentation?","text":"<p>Image segmentation is the process of partitioning an image into multiple segments or regions, each with similar attributes. Unlike edge detection or contour finding, segmentation aims to create a complete division of the image.</p> <p></p>"},{"location":"computer_vision/06_image_segmentation/#main-approaches","title":"Main Approaches","text":"Method Description Best Used For Thresholding Segment based on pixel intensity Simple backgrounds, high contrast Color-based Group similar colors (e.g., K-means) Objects with distinct colors Region-growing Expand from seed points Cohesive regions with gradual transitions Watershed Treat gradient magnitudes as topographic surface Touching objects, cell segmentation GrabCut Interactive segmentation with minimal user input Foreground extraction, detailed objects"},{"location":"computer_vision/06_image_segmentation/#applications","title":"Applications","text":"<ul> <li>Medical image analysis (tumor detection, organ segmentation)</li> <li>Object extraction and background removal</li> <li>Autonomous driving (road/obstacle detection)</li> <li>Video surveillance (foreground/background separation)</li> <li>Image editing and composition</li> </ul>"},{"location":"computer_vision/06_image_segmentation/#starter-code","title":"Starter Code","text":"<pre><code>import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import segmentation, color\nimport random\n\ndef load_image(image_path):\n    \"\"\"\n    Load an image and convert to RGB for display.\n\n    Args:\n        image_path (str): Path to the image file\n\n    Returns:\n        tuple: (bgr_image, rgb_image)\n    \"\"\"\n    # Load the image\n    img = cv2.imread(image_path)\n    if img is None:\n        raise FileNotFoundError(f\"Could not load image from {image_path}\")\n\n    # Convert to RGB for matplotlib display\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    return img, img_rgb\n\ndef display_images(images, titles, figsize=(15, 10), rows=1):\n    \"\"\"\n    Display multiple images in a grid.\n\n    Args:\n        images (list): List of images to display\n        titles (list): List of titles for each image\n        figsize (tuple): Figure size\n        rows (int): Number of rows in the grid\n    \"\"\"\n    n = len(images)\n    cols = (n + rows - 1) // rows  # Ceiling division\n    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n\n    # Make axes a 2D array if it isn't already\n    if rows == 1 and cols == 1:\n        axes = np.array([[axes]])\n    elif rows == 1:\n        axes = axes.reshape(1, -1)\n    elif cols == 1:\n        axes = axes.reshape(-1, 1)\n\n    # Flatten both arrays for easy iteration\n    axes_flat = axes.flatten()\n\n    for i, (img, title) in enumerate(zip(images, titles)):\n        if i &lt; len(axes_flat):\n            if len(img.shape) == 2:  # Grayscale\n                axes_flat[i].imshow(img, cmap='gray')\n            else:  # Color image\n                axes_flat[i].imshow(img)\n            axes_flat[i].set_title(title)\n            axes_flat[i].axis('off')\n\n    # Hide unused subplots\n    for i in range(n, len(axes_flat)):\n        axes_flat[i].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndef apply_binary_thresholding(image, threshold=127, max_value=255):\n    \"\"\"\n    Apply binary thresholding to segment an image.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n        threshold (int): Threshold value\n        max_value (int): Maximum value to use\n\n    Returns:\n        np.ndarray: Binary segmented image\n    \"\"\"\n    # TODO: Implement binary thresholding using cv2.threshold\n\n    return thresholded_image\n\ndef apply_otsu_thresholding(image, max_value=255):\n    \"\"\"\n    Apply Otsu's thresholding to automatically determine the optimal threshold.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n        max_value (int): Maximum value to use\n\n    Returns:\n        tuple: (segmented image, optimal threshold)\n    \"\"\"\n    # TODO: Implement Otsu's thresholding using cv2.threshold with THRESH_OTSU flag\n\n    return thresholded_image, optimal_threshold\n\ndef apply_adaptive_thresholding(image, block_size=11, c=2, max_value=255):\n    \"\"\"\n    Apply adaptive thresholding to handle images with varying illumination.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n        block_size (int): Size of pixel neighborhood for threshold calculation\n        c (int): Constant subtracted from mean or weighted sum\n        max_value (int): Maximum value to use\n\n    Returns:\n        np.ndarray: Segmented image\n    \"\"\"\n    # TODO: Implement adaptive thresholding using cv2.adaptiveThreshold\n\n    return thresholded_image\n\ndef apply_color_based_segmentation(image, n_clusters=5, attempts=10):\n    \"\"\"\n    Apply color-based segmentation using K-means clustering.\n\n    Args:\n        image (np.ndarray): BGR input image\n        n_clusters (int): Number of clusters/segments\n        attempts (int): Number of attempts for K-means\n\n    Returns:\n        np.ndarray: Segmented image with each pixel replaced by the cluster center\n    \"\"\"\n    # TODO: Implement color-based segmentation using K-means\n    # 1. Reshape the image to a 2D array of pixels\n    # 2. Convert to float32\n    # 3. Apply K-means\n    # 4. Replace each pixel with its cluster center\n    # 5. Reshape back to original image dimensions\n\n    return segmented_image\n\ndef apply_watershed_segmentation(image):\n    \"\"\"\n    Apply watershed segmentation to separate touching objects.\n\n    Args:\n        image (np.ndarray): BGR input image\n\n    Returns:\n        np.ndarray: Segmented image with watershed regions\n    \"\"\"\n    # TODO: Implement watershed segmentation\n    # 1. Convert to grayscale\n    # 2. Apply binary thresholding or other preprocessing\n    # 3. Compute distance transform\n    # 4. Find markers (local maxima of distance transform)\n    # 5. Apply watershed\n\n    return segmented_image\n\ndef apply_grabcut_segmentation(image, rect=None):\n    \"\"\"\n    Apply GrabCut algorithm for foreground extraction.\n\n    Args:\n        image (np.ndarray): BGR input image\n        rect (tuple): Rectangle containing foreground (x, y, width, height)\n\n    Returns:\n        np.ndarray: Foreground mask and segmented image\n    \"\"\"\n    # TODO: Implement GrabCut segmentation\n    # 1. Define rectangle for foreground\n    # 2. Create initial mask\n    # 3. Create temporary arrays for algorithm\n    # 4. Run GrabCut algorithm\n    # 5. Create output mask and apply to image\n\n    if rect is None:\n        # If no rectangle provided, use a default centered one\n        height, width = image.shape[:2]\n        margin = min(width, height) // 4\n        rect = (margin, margin, width - 2*margin, height - 2*margin)\n\n    return mask, segmented_image\n\ndef apply_region_growing_segmentation(image, seeds=None, threshold=10):\n    \"\"\"\n    Apply region growing segmentation from seed points.\n\n    Args:\n        image (np.ndarray): Grayscale input image\n        seeds (list): List of seed points [(x1, y1), (x2, y2), ...]\n        threshold (int): Intensity threshold for region growing\n\n    Returns:\n        np.ndarray: Segmented image with regions\n    \"\"\"\n    # TODO: Implement simple region growing algorithm\n    # 1. Initialize mask for output\n    # 2. For each seed, grow region by considering neighbors\n    # 3. Add neighbors to region if within threshold of region mean\n\n    if seeds is None:\n        # If no seeds provided, use a default grid\n        height, width = image.shape[:2]\n        seeds = [(width//4, height//4), (3*width//4, height//4),\n                 (width//4, 3*height//4), (3*width//4, 3*height//4)]\n\n    return segmented_image\n\ndef apply_felzenszwalb_segmentation(image, scale=100, sigma=0.5, min_size=50):\n    \"\"\"\n    Apply Felzenszwalb's segmentation algorithm.\n\n    Args:\n        image (np.ndarray): RGB input image\n        scale (float): Free parameter. Higher means larger clusters.\n        sigma (float): Gaussian kernel width for pre-smoothing\n        min_size (int): Minimum component size\n\n    Returns:\n        np.ndarray: Segmented image with colored regions\n    \"\"\"\n    # TODO: Implement Felzenszwalb's algorithm using skimage\n\n    return segmented_image\n\ndef apply_slic_superpixels(image, n_segments=100, compactness=10):\n    \"\"\"\n    Apply SLIC superpixel segmentation.\n\n    Args:\n        image (np.ndarray): RGB input image\n        n_segments (int): Approximate number of segments\n        compactness (float): Balances color and space proximity\n\n    Returns:\n        np.ndarray: Segmented image with superpixel boundaries\n    \"\"\"\n    # TODO: Implement SLIC superpixel segmentation using skimage\n\n    return segmented_image\n\ndef create_segmentation_comparison(image):\n    \"\"\"\n    Compare different segmentation methods side by side.\n\n    Args:\n        image (np.ndarray): BGR input image\n\n    Returns:\n        list: List of (segmented_image, method_name) tuples\n    \"\"\"\n    # TODO: Apply different segmentation methods and collect results\n\n    return []\n\ndef evaluate_segmentation(segmented_image, ground_truth=None):\n    \"\"\"\n    Evaluate segmentation quality using metrics.\n\n    Args:\n        segmented_image (np.ndarray): Segmented image\n        ground_truth (np.ndarray, optional): Ground truth segmentation\n\n    Returns:\n        dict: Dictionary of evaluation metrics\n    \"\"\"\n    # TODO: Implement segmentation evaluation\n    # If ground truth available, compute overlap metrics\n    # Otherwise, calculate internal evaluation metrics (e.g., segment homogeneity)\n\n    metrics = {\n        'num_segments': 0,\n        'avg_segment_size': 0,\n        'segment_size_std': 0\n    }\n\n    return metrics\n\ndef visualize_segments(segmented_image, original_image=None, random_colors=True):\n    \"\"\"\n    Visualize segmentation results with colored regions.\n\n    Args:\n        segmented_image (np.ndarray): Segmented image with integer labels\n        original_image (np.ndarray, optional): Original image for overlay\n        random_colors (bool): Whether to use random colors or a colormap\n\n    Returns:\n        np.ndarray: Visualization image\n    \"\"\"\n    # TODO: Implement segmentation visualization\n    # 1. Create a color map for segments\n    # 2. Map each segment to a color\n    # 3. Optionally overlay on original image\n\n    return visualization\n\n# Main execution\nif __name__ == \"__main__\":\n    # Sample image path - replace with your own\n    image_path = \"sample_image.jpg\"\n\n    # Load image\n    img_bgr, img_rgb = load_image(image_path)\n\n    # Convert to grayscale for methods that require it\n    img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n\n    # Display original image\n    plt.figure(figsize=(10, 8))\n    plt.imshow(img_rgb)\n    plt.title('Original Image')\n    plt.axis('off')\n    plt.show()\n\n    # TODO: Demonstrate segmentation methods\n</code></pre>"},{"location":"computer_vision/06_image_segmentation/#group-implementation-tasks","title":"Group Implementation Tasks","text":""},{"location":"computer_vision/06_image_segmentation/#task-1-complete-the-missing-functions","title":"Task 1: Complete the Missing Functions","text":"<p>Working in pairs, complete each TODO section in the starter code:</p> <ul> <li>Person A: Implement thresholding methods and color-based segmentation</li> <li>Person B: Implement watershed, region growing, and advanced segmentation methods</li> </ul>"},{"location":"computer_vision/06_image_segmentation/#task-2-segmentation-comparison","title":"Task 2: Segmentation Comparison","text":"<ol> <li> <p>Apply at least 5 different segmentation methods to a set of diverse images:</p> </li> <li> <p>A simple object with clear boundaries</p> </li> <li>A complex scene with multiple objects</li> <li>A natural scene (e.g., landscape)</li> <li>A texture-rich image (e.g., fabric, foliage)</li> <li> <p>A medical image (if available)</p> </li> <li> <p>Create a visual comparison matrix and document which methods work best for each image type:</p> </li> </ol> Image Type Best Method Worst Method Notes Simple object Complex scene Natural scene Texture-rich Medical"},{"location":"computer_vision/06_image_segmentation/#task-3-parameter-experimentation","title":"Task 3: Parameter Experimentation","text":"<ol> <li>Choose one segmentation method (e.g., K-means or watershed)</li> <li>Vary key parameters and document their effect on segmentation quality</li> <li>Create a visual grid showing parameter impact</li> </ol> <p>Example experiment:</p> Parameter Value 1 Value 2 Value 3 K-means clusters (k) k=3 k=5 k=10 [Visual Result] [Image] [Image] [Image]"},{"location":"computer_vision/06_image_segmentation/#peer-review-questions","title":"Peer Review Questions","text":"<p>After completing the implementation, review each other's code and discuss:</p> <ol> <li> <p>Method Selection:</p> </li> <li> <p>Which segmentation method would you choose for medical image analysis?</p> </li> <li> <p>When would you prefer thresholding over clustering-based methods?</p> </li> <li> <p>Parameter Sensitivity:</p> </li> <li> <p>How sensitive is each method to parameter changes?</p> </li> <li> <p>Which parameters had the largest impact on results?</p> </li> <li> <p>Performance Analysis:</p> </li> <li> <p>Which methods are computationally more efficient?</p> </li> <li> <p>How does image size affect processing time for different methods?</p> </li> <li> <p>Result Quality:</p> </li> <li> <p>How would you measure segmentation quality without ground truth?</p> </li> <li> <p>What visual artifacts or common problems did you observe?</p> </li> <li> <p>Practical Applications:</p> </li> <li>How might you combine multiple segmentation techniques for better results?</li> <li>What preprocessing steps improved segmentation quality?</li> </ol>"},{"location":"computer_vision/06_image_segmentation/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"computer_vision/06_image_segmentation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Over-segmentation</p> </li> <li> <p>Too many small regions created</p> </li> <li>Increase clustering parameters (k for K-means)</li> <li>Apply smoothing before segmentation</li> <li> <p>Use region merging as post-processing</p> </li> <li> <p>Under-segmentation</p> </li> <li> <p>Important objects merged together</p> </li> <li>Decrease threshold values</li> <li>Increase number of clusters/segments</li> <li> <p>Apply edge-preserving smoothing</p> </li> <li> <p>Noisy Results</p> </li> <li> <p>Segments are fragmented and inconsistent</p> </li> <li>Apply noise reduction preprocessing</li> <li>Use morphological operations to clean results</li> <li> <p>Consider marker-based watershed</p> </li> <li> <p>Boundary Inaccuracy</p> </li> <li>Segment boundaries don't align with object edges</li> <li>Combine with edge detection</li> <li>Use gradient information in segmentation</li> <li>Try GrabCut or graph-based methods</li> </ol>"},{"location":"computer_vision/06_image_segmentation/#debugging-tools","title":"Debugging Tools","text":"<p>Create functions that:</p> <ol> <li>Highlight segmentation boundaries on the original image</li> <li>Show region properties (size, average color, texture)</li> <li>Compare multiple segmentation results side by side</li> </ol>"},{"location":"computer_vision/06_image_segmentation/#mini-project-interactive-segmentation-tool","title":"Mini-Project: Interactive Segmentation Tool","text":"<p>Create an application that:</p> <ol> <li>Loads images from files</li> <li>Provides a user interface to select and configure segmentation methods</li> <li>Allows parameter adjustment with real-time preview</li> <li>Lets users refine segmentation results manually</li> <li>Extracts segment properties and statistics</li> <li>Exports segmentation results as masks or separate images</li> </ol>"},{"location":"computer_vision/06_image_segmentation/#extension-ideas","title":"Extension Ideas","text":"<ul> <li>Implement segment tracking across multiple frames of video</li> <li>Add automatic object labeling based on segment properties</li> <li>Create a tool that combines multiple segmentation results</li> <li>Develop a segmentation-based image editing tool (e.g., selective color adjustment)</li> <li>Build a medical image analysis tool for specific applications</li> </ul>"},{"location":"computer_vision/06_image_segmentation/#reflection-questions","title":"Reflection Questions","text":"<ol> <li>How do higher-level semantic understanding and lower-level segmentation relate?</li> <li>What challenges remain in image segmentation that deep learning approaches address?</li> <li>How might contextual information improve segmentation results?</li> <li>What are the trade-offs between interactive and fully automatic segmentation?</li> <li>How has image segmentation evolved to handle increasingly complex scenes?</li> </ol>"},{"location":"computer_vision/computer_vision_labs_2024-07-06/","title":"Computer Vision Labs - Chat Session","text":""},{"location":"computer_vision/computer_vision_labs_2024-07-06/#project-overview","title":"Project Overview","text":"<p>Created six comprehensive documentation files for computer vision labs, designed for collaborative learning:</p> <ol> <li>Image Loading and Manipulation - Basic operations and color spaces</li> <li>Filtering and Convolution - Applying kernels for image processing effects</li> <li>Edge Detection - Sobel, Canny, and other edge detection techniques</li> <li>Contour Detection and Shapes - Shape analysis and classification</li> <li>Face Detection - Haar cascades and DNN-based face detection</li> <li>Image Segmentation - Thresholding, clustering, and region-based segmentation</li> </ol>"},{"location":"computer_vision/computer_vision_labs_2024-07-06/#lab-structure","title":"Lab Structure","text":"<p>Each lab includes:</p> <ul> <li>Environmental setup instructions</li> <li>Visual concept explanations</li> <li>Starter code with TODO sections</li> <li>Group implementation tasks</li> <li>Peer review questions</li> <li>Troubleshooting guides</li> <li>Mini-project challenges</li> </ul>"},{"location":"computer_vision/computer_vision_labs_2024-07-06/#created-files","title":"Created Files","text":"<p>All files are stored in: <code>collab/computer_vision/</code></p> <pre><code>01_image_loading_and_manipulation.md\n02_filtering_and_convolution.md\n03_edge_detection.md\n04_contour_detection_and_shapes.md\n05_face_detection.md\n06_image_segmentation.md\n</code></pre>"},{"location":"computer_vision/computer_vision_labs_2024-07-06/#implementation-approach","title":"Implementation Approach","text":"<ul> <li>Followed markdown style guidelines</li> <li>Structured for pair programming with clearly defined roles</li> <li>Included visual feedback opportunities throughout</li> <li>Incorporated both theoretical understanding and practical application</li> <li>Added real-world mini-projects at the end of each lab</li> </ul>"},{"location":"computer_vision/computer_vision_labs_2024-07-06/#next-steps","title":"Next Steps","text":"<p>Consider adding:</p> <ul> <li>Sample images for the labs</li> <li>Example solutions folder</li> <li>Video tutorials to accompany the labs</li> <li>Assessment criteria for each mini-project</li> </ul>"},{"location":"ds_algo/algos/01_breadth_first_search/","title":"Breadth-First Search (BFS) Algorithm","text":""},{"location":"ds_algo/algos/01_breadth_first_search/#objective","title":"Objective","text":"<p>Implement and understand the Breadth-First Search algorithm in a collaborative learning environment. By the end of this session, your team will have coded a working BFS implementation and gained insights into its applications and performance characteristics.</p>"},{"location":"ds_algo/algos/01_breadth_first_search/#environment-setup","title":"Environment Setup","text":"Requirements Python 3.8+ Standard library only <p>Verify your environment:</p> <pre><code>python --version  # Should be 3.8+\n</code></pre>"},{"location":"ds_algo/algos/01_breadth_first_search/#visual-explanation","title":"Visual Explanation","text":"<p>BFS explores a graph level by level, visiting all neighbors at the current depth before moving to the next level.</p> <pre><code>    A\n   / \\\n  B   C\n / \\   \\\nD   E   F\n</code></pre> <p>Traversal order: A \u2192 B \u2192 C \u2192 D \u2192 E \u2192 F</p> <p>BFS uses a queue to track nodes to visit next, following First-In-First-Out (FIFO) order.</p>"},{"location":"ds_algo/algos/01_breadth_first_search/#algorithm-walkthrough","title":"Algorithm Walkthrough","text":""},{"location":"ds_algo/algos/01_breadth_first_search/#pseudocode","title":"Pseudocode","text":"<pre><code>function BFS(graph, start_node):\n    Queue q = new Queue()\n    Set visited = new Set()\n\n    q.enqueue(start_node)\n    visited.add(start_node)\n\n    while q is not empty:\n        current = q.dequeue()\n        process(current)\n\n        for each neighbor of current:\n            if neighbor not in visited:\n                visited.add(neighbor)\n                q.enqueue(neighbor)\n</code></pre>"},{"location":"ds_algo/algos/01_breadth_first_search/#time-and-space-complexity","title":"Time and Space Complexity","text":"<ul> <li>Time Complexity: O(V + E) where V is the number of vertices and E is the number of edges</li> <li>Space Complexity: O(V) for the queue and visited set</li> </ul>"},{"location":"ds_algo/algos/01_breadth_first_search/#annotated-code-template","title":"Annotated Code Template","text":"<pre><code>from collections import deque\n\ndef breadth_first_search(graph, start_node):\n    \"\"\"\n    Performs breadth-first search on a graph starting from start_node.\n\n    Args:\n        graph: Dictionary representing an adjacency list graph\n               {node: [neighbor1, neighbor2, ...]}\n        start_node: Starting node for BFS\n\n    Returns:\n        List of nodes in BFS traversal order\n    \"\"\"\n    # Initialize queue with start node\n    queue = deque([start_node])\n\n    # Track visited nodes to avoid cycles\n    visited = set([start_node])\n\n    # Store traversal order\n    traversal_order = []\n\n    # TODO: Implement the BFS algorithm\n    # While the queue is not empty:\n    #   1. Dequeue a node\n    #   2. Add it to traversal_order\n    #   3. Enqueue all unvisited neighbors\n    #   4. Mark them as visited\n\n    return traversal_order\n</code></pre>"},{"location":"ds_algo/algos/01_breadth_first_search/#live-coding-group-activity","title":"Live Coding Group Activity","text":""},{"location":"ds_algo/algos/01_breadth_first_search/#team-roles","title":"Team Roles","text":"<ul> <li>Driver: Types the code</li> <li>Navigator: Guides the implementation</li> <li>Explainer: Verbalizes what's happening at each step</li> </ul>"},{"location":"ds_algo/algos/01_breadth_first_search/#task","title":"Task","text":"<p>Complete the BFS implementation by filling in the missing code in the template. Test it with the following graph:</p> <pre><code># Example graph as adjacency list\ngraph = {\n    'A': ['B', 'C'],\n    'B': ['A', 'D', 'E'],\n    'C': ['A', 'F'],\n    'D': ['B'],\n    'E': ['B'],\n    'F': ['C']\n}\n\n# Expected output from BFS starting at 'A': ['A', 'B', 'C', 'D', 'E', 'F']\n</code></pre>"},{"location":"ds_algo/algos/01_breadth_first_search/#checkpoint-questions","title":"Checkpoint Questions","text":"<ol> <li>What happens if we use a stack instead of a queue?</li> <li>How would we modify BFS to find the shortest path to a target node?</li> <li>What would be different if we were traversing a binary tree instead of a graph?</li> </ol>"},{"location":"ds_algo/algos/01_breadth_first_search/#complete-implementation","title":"Complete Implementation","text":"<p>After your group discussion, compare your solution with this implementation:</p> <pre><code>from collections import deque\n\ndef breadth_first_search(graph, start_node):\n    \"\"\"\n    Performs breadth-first search on a graph starting from start_node.\n\n    Args:\n        graph: Dictionary representing an adjacency list graph\n               {node: [neighbor1, neighbor2, ...]}\n        start_node: Starting node for BFS\n\n    Returns:\n        List of nodes in BFS traversal order\n    \"\"\"\n    # Initialize queue with start node\n    queue = deque([start_node])\n\n    # Track visited nodes to avoid cycles\n    visited = set([start_node])\n\n    # Store traversal order\n    traversal_order = []\n\n    # BFS loop\n    while queue:\n        # Dequeue the next node\n        current_node = queue.popleft()\n\n        # Add to traversal order\n        traversal_order.append(current_node)\n\n        # Check all neighbors\n        for neighbor in graph[current_node]:\n            # Only process unvisited nodes\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append(neighbor)\n\n    return traversal_order\n\n# Example usage\nif __name__ == \"__main__\":\n    graph = {\n        'A': ['B', 'C'],\n        'B': ['A', 'D', 'E'],\n        'C': ['A', 'F'],\n        'D': ['B'],\n        'E': ['B'],\n        'F': ['C']\n    }\n\n    result = breadth_first_search(graph, 'A')\n    print(f\"BFS traversal: {result}\")\n</code></pre>"},{"location":"ds_algo/algos/01_breadth_first_search/#common-implementation-mistakes","title":"Common Implementation Mistakes","text":"<ol> <li>Forgetting to mark nodes as visited: This can cause infinite loops in graphs with cycles.</li> <li>Using a stack instead of a queue: This would result in DFS, not BFS.</li> <li>Not checking if a node is visited before enqueueing: This can lead to duplicate processing.</li> <li>Missing edge cases: Not handling disconnected graphs or invalid start nodes.</li> </ol>"},{"location":"ds_algo/algos/01_breadth_first_search/#peer-discussion-prompts","title":"Peer Discussion Prompts","text":"<ol> <li>How does BFS differ from other search algorithms?</li> <li>What kind of problems is BFS particularly well-suited for?</li> <li>Explain how BFS naturally finds the shortest path in an unweighted graph.</li> <li>Discuss real-world applications where BFS would be useful.</li> </ol>"},{"location":"ds_algo/algos/01_breadth_first_search/#mini-challenge-bfs-path-finding","title":"Mini-Challenge: BFS Path Finding","text":"<p>Modify your BFS implementation to find the shortest path between two nodes.</p> <pre><code>def bfs_shortest_path(graph, start_node, goal_node):\n    \"\"\"\n    Finds shortest path between start_node and goal_node using BFS.\n\n    Returns:\n        List representing the path from start to goal, or None if no path exists\n    \"\"\"\n    # TODO: Implement shortest path BFS\n    pass\n</code></pre>"},{"location":"ds_algo/algos/01_breadth_first_search/#extension-for-fast-learners","title":"Extension for Fast Learners","text":"<p>Implement a BFS variant that calculates the distance (number of edges) from the start node to every other node in the graph.</p>"},{"location":"ds_algo/algos/01_breadth_first_search/#applications-of-bfs","title":"Applications of BFS","text":"<ul> <li>Finding shortest paths in unweighted graphs</li> <li>Web crawling</li> <li>Social network connections (e.g., finding \"friends of friends\")</li> <li>Level order traversal of trees</li> <li>Finding connected components</li> <li>Solving puzzles like mazes or sliding puzzles</li> </ul>"},{"location":"ds_algo/algos/01_breadth_first_search/#team-reflection","title":"Team Reflection","text":"<p>As a group, discuss:</p> <ol> <li>What was the most challenging part of implementing BFS?</li> <li>How does the queue data structure enable the \"level-by-level\" exploration?</li> <li>What would you need to modify to make this work with different graph representations?</li> <li>Can you think of a real-world problem where BFS would be the optimal solution?</li> </ol>"},{"location":"ds_algo/algos/02_depth_first_search/","title":"Depth-First Search (DFS) Algorithm","text":""},{"location":"ds_algo/algos/02_depth_first_search/#objective","title":"Objective","text":"<p>Master the Depth-First Search algorithm through collaborative coding and analysis. By the end of this session, your team will understand both recursive and iterative DFS implementations and their applications to graphs and trees.</p>"},{"location":"ds_algo/algos/02_depth_first_search/#environment-setup","title":"Environment Setup","text":"Requirements Python 3.8+ Standard library only <p>Verify your environment:</p> <pre><code>python --version  # Should be 3.8+\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#visual-explanation","title":"Visual Explanation","text":"<p>DFS explores a graph by going as deep as possible along each branch before backtracking.</p> <pre><code>    A\n   / \\\n  B   C\n / \\   \\\nD   E   F\n</code></pre> <p>Traversal order: A \u2192 B \u2192 D \u2192 E \u2192 C \u2192 F</p> <p>DFS uses a stack (or recursion) to track nodes to visit next, following Last-In-First-Out (LIFO) order.</p>"},{"location":"ds_algo/algos/02_depth_first_search/#algorithm-walkthrough","title":"Algorithm Walkthrough","text":""},{"location":"ds_algo/algos/02_depth_first_search/#pseudocode-recursive","title":"Pseudocode (Recursive)","text":"<pre><code>function DFS_Recursive(graph, node, visited):\n    if node not in visited:\n        visited.add(node)\n        process(node)\n\n        for each neighbor of node:\n            DFS_Recursive(graph, neighbor, visited)\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#pseudocode-iterative","title":"Pseudocode (Iterative)","text":"<pre><code>function DFS_Iterative(graph, start_node):\n    Stack s = new Stack()\n    Set visited = new Set()\n\n    s.push(start_node)\n\n    while s is not empty:\n        current = s.pop()\n\n        if current not in visited:\n            visited.add(current)\n            process(current)\n\n            for each neighbor of current:\n                if neighbor not in visited:\n                    s.push(neighbor)\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#time-and-space-complexity","title":"Time and Space Complexity","text":"<ul> <li>Time Complexity: O(V + E) where V is the number of vertices and E is the number of edges</li> <li>Space Complexity: O(V) for the stack/recursion and visited set</li> </ul>"},{"location":"ds_algo/algos/02_depth_first_search/#annotated-code-template","title":"Annotated Code Template","text":""},{"location":"ds_algo/algos/02_depth_first_search/#recursive-dfs","title":"Recursive DFS","text":"<pre><code>def dfs_recursive(graph, node, visited=None, traversal_order=None):\n    \"\"\"\n    Performs recursive depth-first search on a graph starting from node.\n\n    Args:\n        graph: Dictionary representing an adjacency list graph\n               {node: [neighbor1, neighbor2, ...]}\n        node: Current node to explore\n        visited: Set of visited nodes (initialized if None)\n        traversal_order: List to track traversal order (initialized if None)\n\n    Returns:\n        List of nodes in DFS traversal order\n    \"\"\"\n    # Initialize visited set and traversal list on first call\n    if visited is None:\n        visited = set()\n    if traversal_order is None:\n        traversal_order = []\n\n    # TODO: Implement recursive DFS\n    # 1. Mark current node as visited\n    # 2. Add to traversal order\n    # 3. Recursively visit all unvisited neighbors\n\n    return traversal_order\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#iterative-dfs","title":"Iterative DFS","text":"<pre><code>def dfs_iterative(graph, start_node):\n    \"\"\"\n    Performs iterative depth-first search on a graph starting from start_node.\n\n    Args:\n        graph: Dictionary representing an adjacency list graph\n               {node: [neighbor1, neighbor2, ...]}\n        start_node: Starting node for DFS\n\n    Returns:\n        List of nodes in DFS traversal order\n    \"\"\"\n    # TODO: Implement iterative DFS using a stack\n    # 1. Initialize stack with start node\n    # 2. Initialize visited set and traversal list\n    # 3. While stack is not empty:\n    #    a. Pop a node\n    #    b. If not visited, mark as visited and add to traversal\n    #    c. Push unvisited neighbors to stack\n\n    traversal_order = []\n    return traversal_order\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#live-coding-group-activity","title":"Live Coding Group Activity","text":""},{"location":"ds_algo/algos/02_depth_first_search/#team-roles","title":"Team Roles","text":"<ul> <li>Driver: Types the code</li> <li>Navigator: Guides the implementation</li> <li>Explainer: Verbalizes what's happening at each step</li> </ul>"},{"location":"ds_algo/algos/02_depth_first_search/#task","title":"Task","text":"<p>Implement both the recursive and iterative DFS algorithms by filling in the missing code. Test with this graph:</p> <pre><code># Example graph as adjacency list\ngraph = {\n    'A': ['B', 'C'],\n    'B': ['A', 'D', 'E'],\n    'C': ['A', 'F'],\n    'D': ['B'],\n    'E': ['B'],\n    'F': ['C']\n}\n\n# Expected output from DFS starting at 'A' (one possible order):\n# ['A', 'B', 'D', 'E', 'C', 'F']\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#tree-dfs-variant","title":"Tree DFS Variant","text":"<p>Also implement a DFS for a binary tree:</p> <pre><code>class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef tree_dfs(root):\n    \"\"\"Perform DFS on a binary tree\"\"\"\n    # TODO: Implement tree DFS\n    pass\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#checkpoint-questions","title":"Checkpoint Questions","text":"<ol> <li>What happens if we push neighbors in reverse order to the stack?</li> <li>How would we modify DFS to detect cycles in a graph?</li> <li>What's the relationship between DFS and topological sorting?</li> </ol>"},{"location":"ds_algo/algos/02_depth_first_search/#complete-implementation","title":"Complete Implementation","text":"<p>After your group discussion, compare your solution with these implementations:</p>"},{"location":"ds_algo/algos/02_depth_first_search/#recursive-dfs_1","title":"Recursive DFS","text":"<pre><code>def dfs_recursive(graph, node, visited=None, traversal_order=None):\n    \"\"\"\n    Performs recursive depth-first search on a graph starting from node.\n\n    Args:\n        graph: Dictionary representing an adjacency list graph\n               {node: [neighbor1, neighbor2, ...]}\n        node: Current node to explore\n        visited: Set of visited nodes (initialized if None)\n        traversal_order: List to track traversal order (initialized if None)\n\n    Returns:\n        List of nodes in DFS traversal order\n    \"\"\"\n    # Initialize visited set and traversal list on first call\n    if visited is None:\n        visited = set()\n    if traversal_order is None:\n        traversal_order = []\n\n    # Mark current node as visited\n    visited.add(node)\n\n    # Add to traversal order\n    traversal_order.append(node)\n\n    # Recursively visit all unvisited neighbors\n    for neighbor in graph[node]:\n        if neighbor not in visited:\n            dfs_recursive(graph, neighbor, visited, traversal_order)\n\n    return traversal_order\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#iterative-dfs_1","title":"Iterative DFS","text":"<pre><code>def dfs_iterative(graph, start_node):\n    \"\"\"\n    Performs iterative depth-first search on a graph starting from start_node.\n\n    Args:\n        graph: Dictionary representing an adjacency list graph\n               {node: [neighbor1, neighbor2, ...]}\n        start_node: Starting node for DFS\n\n    Returns:\n        List of nodes in DFS traversal order\n    \"\"\"\n    # Initialize stack, visited set, and traversal list\n    stack = [start_node]\n    visited = set()\n    traversal_order = []\n\n    # DFS loop\n    while stack:\n        # Pop the next node from the stack\n        current_node = stack.pop()\n\n        # Process unvisited nodes\n        if current_node not in visited:\n            visited.add(current_node)\n            traversal_order.append(current_node)\n\n            # Add unvisited neighbors to stack (in reverse order for same traversal as recursive)\n            for neighbor in reversed(graph[current_node]):\n                if neighbor not in visited:\n                    stack.append(neighbor)\n\n    return traversal_order\n\n# Example usage\nif __name__ == \"__main__\":\n    graph = {\n        'A': ['B', 'C'],\n        'B': ['A', 'D', 'E'],\n        'C': ['A', 'F'],\n        'D': ['B'],\n        'E': ['B'],\n        'F': ['C']\n    }\n\n    recursive_result = dfs_recursive(graph, 'A')\n    iterative_result = dfs_iterative(graph, 'A')\n\n    print(f\"Recursive DFS traversal: {recursive_result}\")\n    print(f\"Iterative DFS traversal: {iterative_result}\")\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#tree-dfs-implementation","title":"Tree DFS Implementation","text":"<pre><code>class TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n\ndef tree_dfs_recursive(root, traversal=None):\n    \"\"\"Perform DFS on a binary tree recursively (preorder traversal)\"\"\"\n    if traversal is None:\n        traversal = []\n\n    if root is None:\n        return traversal\n\n    # Visit root (preorder traversal: root, left, right)\n    traversal.append(root.val)\n\n    # Visit left subtree\n    tree_dfs_recursive(root.left, traversal)\n\n    # Visit right subtree\n    tree_dfs_recursive(root.right, traversal)\n\n    return traversal\n\ndef tree_dfs_iterative(root):\n    \"\"\"Perform DFS on a binary tree iteratively (preorder traversal)\"\"\"\n    if root is None:\n        return []\n\n    traversal = []\n    stack = [root]\n\n    while stack:\n        node = stack.pop()\n        traversal.append(node.val)\n\n        # Push right first so left is processed first (LIFO)\n        if node.right:\n            stack.append(node.right)\n        if node.left:\n            stack.append(node.left)\n\n    return traversal\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#common-implementation-mistakes","title":"Common Implementation Mistakes","text":"<ol> <li>Not tracking visited nodes: Can cause infinite loops in graphs with cycles.</li> <li>Incorrect recursion base case: Missing or improper termination conditions.</li> <li>Wrong order of pushing neighbors: The order affects the traversal sequence.</li> <li>Stack overflow: Very deep graphs can exceed recursion limits.</li> <li>Confusing traversal types: For trees, DFS has three variants (preorder, inorder, postorder).</li> </ol>"},{"location":"ds_algo/algos/02_depth_first_search/#peer-discussion-prompts","title":"Peer Discussion Prompts","text":"<ol> <li>Compare the recursive and iterative implementations. What are the advantages/disadvantages of each?</li> <li>How would you use DFS to detect cycles in a directed graph?</li> <li>Discuss real-world problems where DFS would be more appropriate than BFS.</li> <li>How does tree traversal DFS differ from graph traversal DFS?</li> </ol>"},{"location":"ds_algo/algos/02_depth_first_search/#mini-challenge-path-finding-with-dfs","title":"Mini-Challenge: Path Finding with DFS","text":"<p>Modify your DFS implementation to find any path between two nodes (not necessarily the shortest).</p> <pre><code>def dfs_find_path(graph, start_node, goal_node):\n    \"\"\"\n    Finds a path from start_node to goal_node using DFS.\n\n    Returns:\n        List representing a path from start to goal, or None if no path exists\n    \"\"\"\n    # TODO: Implement path-finding DFS\n    pass\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#extension-for-fast-learners","title":"Extension for Fast Learners","text":"<p>Implement a DFS-based algorithm to detect cycles in a directed graph.</p>"},{"location":"ds_algo/algos/02_depth_first_search/#applications-of-dfs","title":"Applications of DFS","text":"<ul> <li>Topological sorting</li> <li>Finding connected components</li> <li>Maze generation</li> <li>Cycle detection</li> <li>Solving puzzles (e.g., solving Sudoku)</li> <li>Backtracking algorithms</li> <li>Finding strongly connected components (Kosaraju's algorithm)</li> </ul>"},{"location":"ds_algo/algos/02_depth_first_search/#tree-traversal-modes","title":"Tree Traversal Modes","text":"<p>DFS on trees can be performed in three orders:</p> <ol> <li>Preorder: Node, Left, Right</li> <li>Inorder: Left, Node, Right</li> <li>Postorder: Left, Right, Node</li> </ol> <pre><code>def preorder(root, traversal=None):\n    if traversal is None:\n        traversal = []\n    if root:\n        traversal.append(root.val)  # Visit node\n        preorder(root.left, traversal)\n        preorder(root.right, traversal)\n    return traversal\n\ndef inorder(root, traversal=None):\n    if traversal is None:\n        traversal = []\n    if root:\n        inorder(root.left, traversal)\n        traversal.append(root.val)  # Visit node\n        inorder(root.right, traversal)\n    return traversal\n\ndef postorder(root, traversal=None):\n    if traversal is None:\n        traversal = []\n    if root:\n        postorder(root.left, traversal)\n        postorder(root.right, traversal)\n        traversal.append(root.val)  # Visit node\n    return traversal\n</code></pre>"},{"location":"ds_algo/algos/02_depth_first_search/#team-reflection","title":"Team Reflection","text":"<p>As a group, discuss:</p> <ol> <li>How does the stack data structure (or recursion) enable the \"deep\" exploration pattern of DFS?</li> <li>When would you choose DFS over BFS in a real problem?</li> <li>What modifications would you need to make DFS work with weighted graphs?</li> <li>How do the three tree traversal orders relate to different use cases?</li> </ol>"},{"location":"ds_algo/algos/03_binary_search/","title":"Binary Search Algorithm","text":""},{"location":"ds_algo/algos/03_binary_search/#objective","title":"Objective","text":"<p>Master the Binary Search algorithm through collaborative implementation and analysis. By the end of this session, your team will understand both iterative and recursive approaches, when to use Binary Search, and its performance characteristics.</p>"},{"location":"ds_algo/algos/03_binary_search/#environment-setup","title":"Environment Setup","text":"Requirements Python 3.8+ Standard library only <p>Verify your environment:</p> <pre><code>python --version  # Should be 3.8+\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#visual-explanation","title":"Visual Explanation","text":"<p>Binary Search efficiently finds a target value in a sorted array by repeatedly dividing the search range in half.</p> <pre><code>Array: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\nSearch for: 13\n\nStep 1: mid = (0+9)/2 = 4, value = 9 &lt; 13, so search right half\n        [1, 3, 5, 7, 9, | 11, 13, 15, 17, 19]\n\nStep 2: mid = (5+9)/2 = 7, value = 15 &gt; 13, so search left half\n        [11, 13, | 15, 17, 19]\n\nStep 3: mid = (5+6)/2 = 5, value = 11 &lt; 13, so search right half\n        [11, | 13]\n\nStep 4: mid = (6+6)/2 = 6, value = 13 == 13, found at index 6!\n        [13]\n</code></pre> <p>Binary Search works by eliminating half of the remaining elements at each step.</p>"},{"location":"ds_algo/algos/03_binary_search/#algorithm-walkthrough","title":"Algorithm Walkthrough","text":""},{"location":"ds_algo/algos/03_binary_search/#pseudocode-iterative","title":"Pseudocode (Iterative)","text":"<pre><code>function BinarySearch(array, target):\n    left = 0\n    right = length(array) - 1\n\n    while left &lt;= right:\n        mid = (left + right) / 2\n\n        if array[mid] == target:\n            return mid\n        else if array[mid] &lt; target:\n            left = mid + 1\n        else:\n            right = mid - 1\n\n    return -1  # Target not found\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#pseudocode-recursive","title":"Pseudocode (Recursive)","text":"<pre><code>function BinarySearchRecursive(array, target, left, right):\n    if left &gt; right:\n        return -1  # Base case: target not found\n\n    mid = (left + right) / 2\n\n    if array[mid] == target:\n        return mid\n    else if array[mid] &lt; target:\n        return BinarySearchRecursive(array, target, mid + 1, right)\n    else:\n        return BinarySearchRecursive(array, target, left, mid - 1)\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#time-and-space-complexity","title":"Time and Space Complexity","text":"<ul> <li>Time Complexity: O(log n) - each step eliminates half of the remaining elements</li> <li>Space Complexity: O(1) for iterative, O(log n) for recursive due to call stack</li> </ul>"},{"location":"ds_algo/algos/03_binary_search/#annotated-code-template","title":"Annotated Code Template","text":""},{"location":"ds_algo/algos/03_binary_search/#iterative-binary-search","title":"Iterative Binary Search","text":"<pre><code>def binary_search_iterative(arr, target):\n    \"\"\"\n    Performs iterative binary search for target in a sorted array.\n\n    Args:\n        arr: A sorted list of elements\n        target: The element to search for\n\n    Returns:\n        Index of target if found, -1 otherwise\n    \"\"\"\n    # TODO: Implement iterative binary search\n    # 1. Initialize left and right pointers\n    # 2. While left &lt;= right:\n    #    a. Calculate middle index\n    #    b. If element found, return index\n    #    c. If element too small, search right half\n    #    d. If element too large, search left half\n    # 3. Return -1 if not found\n\n    return -1\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#recursive-binary-search","title":"Recursive Binary Search","text":"<pre><code>def binary_search_recursive(arr, target, left=None, right=None):\n    \"\"\"\n    Performs recursive binary search for target in a sorted array.\n\n    Args:\n        arr: A sorted list of elements\n        target: The element to search for\n        left: The left boundary index (default to 0)\n        right: The right boundary index (default to len(arr)-1)\n\n    Returns:\n        Index of target if found, -1 otherwise\n    \"\"\"\n    # Initialize left and right on first call\n    if left is None:\n        left = 0\n    if right is None:\n        right = len(arr) - 1\n\n    # TODO: Implement recursive binary search\n    # 1. Check base case (left &gt; right)\n    # 2. Calculate middle index\n    # 3. If element found, return index\n    # 4. If element too small, search right half\n    # 5. If element too large, search left half\n\n    return -1\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#live-coding-group-activity","title":"Live Coding Group Activity","text":""},{"location":"ds_algo/algos/03_binary_search/#team-roles","title":"Team Roles","text":"<ul> <li>Driver: Types the code</li> <li>Navigator: Guides the implementation</li> <li>Explainer: Verbalizes what's happening at each step</li> </ul>"},{"location":"ds_algo/algos/03_binary_search/#task","title":"Task","text":"<p>Implement both the iterative and recursive versions of Binary Search by filling in the missing code. Test with these examples:</p> <pre><code># Example sorted arrays\nsimple_array = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\nlarge_array = list(range(0, 1000000, 2))  # Even numbers from 0 to 999998\n\n# Test cases\ntest_cases = [\n    (simple_array, 13),    # Should return 6\n    (simple_array, 1),     # Should return 0\n    (simple_array, 20),    # Should return -1\n    (large_array, 99998),  # Should return 49999\n]\n\n# Run tests\nfor arr, target in test_cases:\n    print(f\"Searching for {target} in array of length {len(arr)}\")\n    print(f\"Iterative result: {binary_search_iterative(arr, target)}\")\n    print(f\"Recursive result: {binary_search_recursive(arr, target)}\")\n    print()\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#checkpoint-questions","title":"Checkpoint Questions","text":"<ol> <li>What happens if the array is not sorted?</li> <li>How many comparisons are needed to find an element in an array of size n?</li> <li>When would binary search be inefficient compared to linear search?</li> </ol>"},{"location":"ds_algo/algos/03_binary_search/#complete-implementation","title":"Complete Implementation","text":"<p>After your group discussion, compare your solution with these implementations:</p>"},{"location":"ds_algo/algos/03_binary_search/#iterative-binary-search_1","title":"Iterative Binary Search","text":"<pre><code>def binary_search_iterative(arr, target):\n    \"\"\"\n    Performs iterative binary search for target in a sorted array.\n\n    Args:\n        arr: A sorted list of elements\n        target: The element to search for\n\n    Returns:\n        Index of target if found, -1 otherwise\n    \"\"\"\n    left = 0\n    right = len(arr) - 1\n\n    while left &lt;= right:\n        # Calculate middle index\n        # Note: (left + right) // 2 can cause integer overflow in some languages\n        # Using left + (right - left) // 2 is safer in those cases\n        mid = (left + right) // 2\n\n        # Check if target is present at mid\n        if arr[mid] == target:\n            return mid\n\n        # If target greater, ignore left half\n        elif arr[mid] &lt; target:\n            left = mid + 1\n\n        # If target smaller, ignore right half\n        else:\n            right = mid - 1\n\n    # Target not found\n    return -1\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#recursive-binary-search_1","title":"Recursive Binary Search","text":"<pre><code>def binary_search_recursive(arr, target, left=None, right=None):\n    \"\"\"\n    Performs recursive binary search for target in a sorted array.\n\n    Args:\n        arr: A sorted list of elements\n        target: The element to search for\n        left: The left boundary index (default to 0)\n        right: The right boundary index (default to len(arr)-1)\n\n    Returns:\n        Index of target if found, -1 otherwise\n    \"\"\"\n    # Initialize left and right on first call\n    if left is None:\n        left = 0\n    if right is None:\n        right = len(arr) - 1\n\n    # Base case: element not found\n    if left &gt; right:\n        return -1\n\n    # Calculate middle index\n    mid = (left + right) // 2\n\n    # Check if target is present at mid\n    if arr[mid] == target:\n        return mid\n\n    # If target greater, search right half\n    elif arr[mid] &lt; target:\n        return binary_search_recursive(arr, target, mid + 1, right)\n\n    # If target smaller, search left half\n    else:\n        return binary_search_recursive(arr, target, left, mid - 1)\n\n# Example usage\nif __name__ == \"__main__\":\n    simple_array = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n    print(f\"Array: {simple_array}\")\n\n    target = 13\n    iterative_result = binary_search_iterative(simple_array, target)\n    recursive_result = binary_search_recursive(simple_array, target)\n\n    print(f\"Search for {target}:\")\n    print(f\"Iterative result: index {iterative_result}\")\n    print(f\"Recursive result: index {recursive_result}\")\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#common-implementation-mistakes","title":"Common Implementation Mistakes","text":"<ol> <li>Off-by-one errors: Incorrect boundary conditions (<code>left &lt;= right</code> vs <code>left &lt; right</code>).</li> <li>Integer overflow: When calculating the middle index in languages with fixed-size integers.</li> <li>Not handling duplicates: Binary search finds an occurrence, not necessarily the first/last.</li> <li>Using on unsorted arrays: Binary search requires a sorted array to work correctly.</li> <li>Infinite loop: Incorrectly updating left/right pointers can lead to infinite loops.</li> </ol>"},{"location":"ds_algo/algos/03_binary_search/#peer-discussion-prompts","title":"Peer Discussion Prompts","text":"<ol> <li>Why is binary search so much faster than linear search for large arrays?</li> <li>Can binary search be applied to linked lists? Why or why not?</li> <li>How would you adapt binary search to find the first or last occurrence of a value in an array with duplicates?</li> <li>Discuss real-world applications where binary search is used.</li> </ol>"},{"location":"ds_algo/algos/03_binary_search/#mini-challenge-rotated-binary-search","title":"Mini-Challenge: Rotated Binary Search","text":"<p>Implement a variation of binary search to find an element in a rotated sorted array (an array that is rotated at some pivot unknown to you).</p> <pre><code>def search_rotated_array(arr, target):\n    \"\"\"\n    Searches for target in a rotated sorted array.\n\n    Example:\n        arr = [4, 5, 6, 7, 0, 1, 2] was originally [0, 1, 2, 4, 5, 6, 7]\n        and was rotated at index 3\n\n    Returns:\n        Index of target if found, -1 otherwise\n    \"\"\"\n    # TODO: Implement rotated array search\n    pass\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#extension-for-fast-learners","title":"Extension for Fast Learners","text":"<p>Implement a binary search to find the pivot point (smallest element) in a rotated sorted array.</p>"},{"location":"ds_algo/algos/03_binary_search/#binary-search-variations","title":"Binary Search Variations","text":""},{"location":"ds_algo/algos/03_binary_search/#finding-insertion-point","title":"Finding Insertion Point","text":"<p>When the target is not found, binary search can be modified to return the index where the element should be inserted:</p> <pre><code>def binary_search_insertion_point(arr, target):\n    \"\"\"Returns the index where target should be inserted to maintain sorted order.\"\"\"\n    left = 0\n    right = len(arr)\n\n    while left &lt; right:\n        mid = (left + right) // 2\n\n        if arr[mid] &lt; target:\n            left = mid + 1\n        else:\n            right = mid\n\n    return left\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#finding-bounds","title":"Finding Bounds","text":"<p>To find the first and last occurrence of a value in a sorted array with duplicates:</p> <pre><code>def binary_search_first_occurrence(arr, target):\n    \"\"\"Returns the index of the first occurrence of target in a sorted array.\"\"\"\n    left = 0\n    right = len(arr) - 1\n    result = -1\n\n    while left &lt;= right:\n        mid = (left + right) // 2\n\n        if arr[mid] == target:\n            result = mid  # Save the result\n            right = mid - 1  # Continue searching left\n        elif arr[mid] &lt; target:\n            left = mid + 1\n        else:\n            right = mid - 1\n\n    return result\n\ndef binary_search_last_occurrence(arr, target):\n    \"\"\"Returns the index of the last occurrence of target in a sorted array.\"\"\"\n    left = 0\n    right = len(arr) - 1\n    result = -1\n\n    while left &lt;= right:\n        mid = (left + right) // 2\n\n        if arr[mid] == target:\n            result = mid  # Save the result\n            left = mid + 1  # Continue searching right\n        elif arr[mid] &lt; target:\n            left = mid + 1\n        else:\n            right = mid - 1\n\n    return result\n</code></pre>"},{"location":"ds_algo/algos/03_binary_search/#team-reflection","title":"Team Reflection","text":"<p>As a group, discuss:</p> <ol> <li>Why is the time complexity of binary search O(log n) and not O(n)?</li> <li>When would you use the iterative version versus the recursive version?</li> <li>What are the practical limitations of binary search?</li> <li>Can you think of problems that binary search can help solve, but may not be immediately obvious?</li> </ol>"},{"location":"ds_algo/algos/04_uniform_cost_search/","title":"Uniform Cost Search (UCS) Algorithm","text":""},{"location":"ds_algo/algos/04_uniform_cost_search/#objective","title":"Objective","text":"<p>Master the Uniform Cost Search algorithm through collaborative implementation and analysis. By the end of this session, your team will understand how UCS finds the lowest-cost path in a weighted graph and how it differs from other search algorithms.</p>"},{"location":"ds_algo/algos/04_uniform_cost_search/#environment-setup","title":"Environment Setup","text":"Requirements Python 3.8+ Standard library only (using <code>heapq</code> for priority queue) <p>Verify your environment:</p> <pre><code>python --version  # Should be 3.8+\n</code></pre>"},{"location":"ds_algo/algos/04_uniform_cost_search/#visual-explanation","title":"Visual Explanation","text":"<p>Uniform Cost Search explores a weighted graph by always expanding the lowest-cost path first.</p> <pre><code>    A\n   /|\\\n  / | \\\n /  |  \\\nB   C   D\n \\  |  /\n  \\ | /\n    E\n</code></pre> <p>With costs:</p> <ul> <li>A \u2192 B: 4</li> <li>A \u2192 C: 1</li> <li>A \u2192 D: 5</li> <li>B \u2192 E: 1</li> <li>C \u2192 E: 5</li> <li>D \u2192 E: 2</li> </ul> <p>Shortest path from A to E: A \u2192 C \u2192 D \u2192 E with total cost 8.</p> <p>UCS uses a priority queue to track paths, prioritizing the lowest cumulative cost.</p>"},{"location":"ds_algo/algos/04_uniform_cost_search/#algorithm-walkthrough","title":"Algorithm Walkthrough","text":""},{"location":"ds_algo/algos/04_uniform_cost_search/#pseudocode","title":"Pseudocode","text":"<pre><code>function UniformCostSearch(graph, start, goal):\n    PriorityQueue frontier = new PriorityQueue()\n    frontier.add(start, 0)  // (node, priority/cost)\n\n    Map came_from = {}      // For path reconstruction\n    Map cost_so_far = {}    // To track lowest cost to each node\n\n    came_from[start] = None\n    cost_so_far[start] = 0\n\n    while frontier is not empty:\n        current = frontier.pop()  // Gets lowest cost node\n\n        if current == goal:\n            break\n\n        for each neighbor of current:\n            new_cost = cost_so_far[current] + cost(current, neighbor)\n\n            if neighbor not in cost_so_far OR new_cost &lt; cost_so_far[neighbor]:\n                cost_so_far[neighbor] = new_cost\n                priority = new_cost\n                frontier.add(neighbor, priority)\n                came_from[neighbor] = current\n\n    return came_from, cost_so_far\n</code></pre>"},{"location":"ds_algo/algos/04_uniform_cost_search/#time-and-space-complexity","title":"Time and Space Complexity","text":"<ul> <li>Time Complexity: O(E + V log V) where V is the number of vertices and E is the number of edges</li> <li>Space Complexity: O(V) for the priority queue, visited set, and path tracking</li> </ul>"},{"location":"ds_algo/algos/04_uniform_cost_search/#annotated-code-template","title":"Annotated Code Template","text":"<pre><code>import heapq\n\ndef uniform_cost_search(graph, start_node, goal_node):\n    \"\"\"\n    Finds the lowest-cost path from start_node to goal_node using Uniform Cost Search.\n\n    Args:\n        graph: Dictionary representing an adjacency list with costs\n               {node: [(neighbor1, cost1), (neighbor2, cost2), ...]}\n        start_node: Starting node for UCS\n        goal_node: Target node to reach\n\n    Returns:\n        (path, cost): Tuple containing the path as a list and the total cost\n    \"\"\"\n    # Priority queue for (cost, node, path)\n    # The cost is first in the tuple so heapq prioritizes by cost\n    priority_queue = [(0, start_node, [start_node])]\n\n    # Track visited nodes to avoid cycles\n    visited = set()\n\n    # TODO: Implement the UCS algorithm\n    # While the priority queue is not empty:\n    #   1. Pop the node with lowest cost so far\n    #   2. If it's the goal, return the path and cost\n    #   3. If we've seen it before, skip it\n    #   4. Mark as visited\n    #   5. Add all unvisited neighbors to the queue with cumulative cost\n\n    # If we exit the loop, no path was found\n    return None, float('inf')\n</code></pre>"},{"location":"ds_algo/algos/04_uniform_cost_search/#live-coding-group-activity","title":"Live Coding Group Activity","text":""},{"location":"ds_algo/algos/04_uniform_cost_search/#team-roles","title":"Team Roles","text":"<ul> <li>Driver: Types the code</li> <li>Navigator: Guides the implementation</li> <li>Explainer: Verbalizes what's happening at each step</li> </ul>"},{"location":"ds_algo/algos/04_uniform_cost_search/#task","title":"Task","text":"<p>Complete the UCS implementation by filling in the missing code in the template. Test it with the following graph:</p> <pre><code># Example weighted graph as adjacency list with costs\ngraph = {\n    'A': [('B', 4), ('C', 1), ('D', 5)],\n    'B': [('A', 4), ('E', 1)],\n    'C': [('A', 1), ('D', 3), ('E', 5)],\n    'D': [('A', 5), ('C', 3), ('E', 2)],\n    'E': [('B', 1), ('C', 5), ('D', 2)]\n}\n\n# Expected lowest-cost path from 'A' to 'E': ['A', 'C', 'D', 'E'] with cost 6\n</code></pre>"},{"location":"ds_algo/algos/04_uniform_cost_search/#checkpoint-questions","title":"Checkpoint Questions","text":"<ol> <li>How does UCS differ from BFS?</li> <li>What happens if all edges have the same weight?</li> <li>When would UCS be more appropriate than other search algorithms?</li> </ol>"},{"location":"ds_algo/algos/04_uniform_cost_search/#complete-implementation","title":"Complete Implementation","text":"<p>After your group discussion, compare your solution with this implementation:</p> <pre><code>import heapq\n\ndef uniform_cost_search(graph, start_node, goal_node):\n    \"\"\"\n    Finds the lowest-cost path from start_node to goal_node using Uniform Cost Search.\n\n    Args:\n        graph: Dictionary representing an adjacency list with costs\n               {node: [(neighbor1, cost1), (neighbor2, cost2), ...]}\n        start_node: Starting node for UCS\n        goal_node: Target node to reach\n\n    Returns:\n        (path, cost): Tuple containing the path as a list and the total cost\n    \"\"\"\n    # Priority queue for (cost, node, path)\n    # The cost is first in the tuple so heapq prioritizes by cost\n    priority_queue = [(0, start_node, [start_node])]\n\n    # Track visited nodes to avoid cycles\n    visited = set()\n\n    while priority_queue:\n        # Pop the node with lowest cost so far\n        current_cost, current_node, path = heapq.heappop(priority_queue)\n\n        # If we've reached the goal, return the path and cost\n        if current_node == goal_node:\n            return path, current_cost\n\n        # Skip if we've already visited this node\n        if current_node in visited:\n            continue\n\n        # Mark as visited\n        visited.add(current_node)\n\n        # Explore neighbors\n        for neighbor, cost in graph[current_node]:\n            # Skip visited neighbors\n            if neighbor not in visited:\n                # Calculate new cost and path\n                new_cost = current_cost + cost\n                new_path = path + [neighbor]\n\n                # Add to priority queue\n                heapq.heappush(priority_queue, (new_cost, neighbor, new_path))\n\n    # If we exit the loop, no path was found\n    return None, float('inf')\n\n# Example usage\nif __name__ == \"__main__\":\n    graph = {\n        'A': [('B', 4), ('C', 1), ('D', 5)],\n        'B': [('A', 4), ('E', 1)],\n        'C': [('A', 1), ('D', 3), ('E', 5)],\n        'D': [('A', 5), ('C', 3), ('E', 2)],\n        'E': [('B', 1), ('C', 5), ('D', 2)]\n    }\n\n    start = 'A'\n    goal = 'E'\n\n    path, cost = uniform_cost_search(graph, start, goal)\n\n    if path:\n        print(f\"Lowest-cost path from {start} to {goal}: {' \u2192 '.join(path)}\")\n        print(f\"Total cost: {cost}\")\n    else:\n        print(f\"No path found from {start} to {goal}\")\n</code></pre>"},{"location":"ds_algo/algos/04_uniform_cost_search/#common-implementation-mistakes","title":"Common Implementation Mistakes","text":"<ol> <li>Not using a priority queue: Using a regular queue loses the cost-based ordering.</li> <li>Incorrectly calculating cumulative costs: Forgetting to add the current path cost.</li> <li>Inefficient visited node handling: Checking visited status at the wrong point.</li> <li>Not tracking the path: Only tracking the cost without the actual path.</li> <li>Not handling disconnected graphs: Missing proper termination when no path exists.</li> </ol>"},{"location":"ds_algo/algos/04_uniform_cost_search/#peer-discussion-prompts","title":"Peer Discussion Prompts","text":"<ol> <li>How does UCS relate to Dijkstra's algorithm?</li> <li>In what scenarios would UCS outperform BFS?</li> <li>What modifications would be needed to handle negative edge weights?</li> <li>How does the priority queue implementation affect the algorithm's performance?</li> </ol>"},{"location":"ds_algo/algos/04_uniform_cost_search/#mini-challenge-optimized-ucs","title":"Mini-Challenge: Optimized UCS","text":"<p>Modify your UCS implementation to be more efficient by:</p> <ol> <li>Only adding a node to the priority queue when its cost decreases</li> <li>Using a visited set more effectively</li> </ol> <pre><code>def optimized_ucs(graph, start_node, goal_node):\n    \"\"\"\n    Implements a more efficient version of UCS.\n\n    Returns:\n        (path, cost): Tuple containing the path as a list and the total cost\n    \"\"\"\n    # TODO: Implement optimized UCS\n    # 1. Use a dictionary to track cost to each node\n    # 2. Use a dictionary to track the path to each node\n    # 3. Only add to priority queue when cost decreases\n\n    pass\n</code></pre>"},{"location":"ds_algo/algos/04_uniform_cost_search/#extension-for-fast-learners","title":"Extension for Fast Learners","text":"<p>Implement a bidirectional UCS that searches from both the start and goal nodes simultaneously.</p>"},{"location":"ds_algo/algos/04_uniform_cost_search/#applications-of-ucs","title":"Applications of UCS","text":"<ul> <li>Finding shortest paths in road networks</li> <li>Network routing algorithms</li> <li>Robot path planning</li> <li>Resource allocation</li> <li>Game pathfinding</li> <li>Transportation optimization</li> </ul>"},{"location":"ds_algo/algos/04_uniform_cost_search/#ucs-vs-other-algorithms","title":"UCS vs. Other Algorithms","text":"Algorithm Uses When to Use BFS Queue Unweighted graphs, shortest path in terms of edges DFS Stack Maze solving, puzzle solutions, complete graph exploration UCS Priority Queue Weighted graphs, lowest-cost path Dijkstra Priority Queue Single source shortest paths to all destinations A* Priority Queue w/ Heuristic Guided search with domain knowledge"},{"location":"ds_algo/algos/04_uniform_cost_search/#visualizing-the-search-process","title":"Visualizing the Search Process","text":"<p>To better understand UCS, let's trace through our example:</p> <pre><code>Starting at A:\n- Push (0, 'A', ['A']) to queue\n\nIteration 1:\n- Pop (0, 'A', ['A'])\n- Mark A as visited\n- Push (1, 'C', ['A', 'C']) - Cost from A to C = 1\n- Push (4, 'B', ['A', 'B']) - Cost from A to B = 4\n- Push (5, 'D', ['A', 'D']) - Cost from A to D = 5\n\nIteration 2:\n- Pop (1, 'C', ['A', 'C']) (lowest cost)\n- Mark C as visited\n- Push (4, 'D', ['A', 'C', 'D']) - Cost from A to C to D = 1 + 3 = 4\n- Push (6, 'E', ['A', 'C', 'E']) - Cost from A to C to E = 1 + 5 = 6\n\nIteration 3:\n- Pop (4, 'B', ['A', 'B']) (lowest cost)\n- Mark B as visited\n- Push (5, 'E', ['A', 'B', 'E']) - Cost from A to B to E = 4 + 1 = 5\n\nIteration 4:\n- Pop (4, 'D', ['A', 'C', 'D']) (lowest cost)\n- Mark D as visited\n- Push (6, 'E', ['A', 'C', 'D', 'E']) - Cost from A to C to D to E = 1 + 3 + 2 = 6\n\nIteration 5:\n- Pop (5, 'E', ['A', 'B', 'E']) (lowest cost)\n- E is the goal! Return path ['A', 'B', 'E'] with cost 5\n</code></pre> <p>So the shortest path is A \u2192 B \u2192 E with cost 5.</p>"},{"location":"ds_algo/algos/04_uniform_cost_search/#team-reflection","title":"Team Reflection","text":"<p>As a group, discuss:</p> <ol> <li>Why does UCS always find the optimal (lowest-cost) path?</li> <li>How does the priority queue enable the \"lowest-cost-first\" exploration pattern?</li> <li>What data structures would you use to implement UCS in a production system?</li> <li>Can you think of a real-world problem where UCS would be the optimal solution?</li> </ol>"},{"location":"ds_algo/algos/05_a_star_search/","title":"A* Search Algorithm","text":""},{"location":"ds_algo/algos/05_a_star_search/#objective","title":"Objective","text":"<p>Master the A (A-star) Search algorithm through collaborative implementation and analysis. By the end of this session, your team will understand how A combines path cost with heuristics to find optimal paths efficiently and apply it to grid-based navigation problems.</p>"},{"location":"ds_algo/algos/05_a_star_search/#environment-setup","title":"Environment Setup","text":"Requirements Python 3.8+ Standard library only (using <code>heapq</code> for priority queue) <p>Verify your environment:</p> <pre><code>python --version  # Should be 3.8+\n</code></pre>"},{"location":"ds_algo/algos/05_a_star_search/#visual-explanation","title":"Visual Explanation","text":"<p>A* Search efficiently finds the shortest path by combining:</p> <ul> <li>g(n): The cost from the start node to the current node</li> <li>h(n): A heuristic that estimates the cost from the current node to the goal</li> <li>f(n): The total estimated cost: f(n) = g(n) + h(n)</li> </ul> <p>A* always expands the node with the lowest f(n) value.</p> <pre><code>Grid Navigation Example:\nS = Start, G = Goal, # = Obstacle\n\n+---+---+---+---+---+\n| S |   |   |   |   |\n+---+---+---+---+---+\n|   | # | # |   |   |\n+---+---+---+---+---+\n|   | # |   |   |   |\n+---+---+---+---+---+\n|   | # | # | # |   |\n+---+---+---+---+---+\n|   |   |   |   | G |\n+---+---+---+---+---+\n</code></pre> <p>A* will find the shortest path around obstacles using a combination of actual movement cost and a heuristic (like Manhattan distance).</p>"},{"location":"ds_algo/algos/05_a_star_search/#algorithm-walkthrough","title":"Algorithm Walkthrough","text":""},{"location":"ds_algo/algos/05_a_star_search/#pseudocode","title":"Pseudocode","text":"<pre><code>function A_Star(graph, start, goal, heuristic):\n    PriorityQueue open_set = new PriorityQueue()\n    open_set.add(start, 0)  // (node, f_score)\n\n    Map came_from = {}      // For path reconstruction\n    Map g_score = {}        // Actual cost from start to current node\n    Map f_score = {}        // Estimated total cost\n\n    g_score[start] = 0\n    f_score[start] = heuristic(start, goal)\n\n    while open_set is not empty:\n        current = open_set.pop()  // Gets node with lowest f_score\n\n        if current == goal:\n            return reconstruct_path(came_from, current)\n\n        for each neighbor of current:\n            // tentative_g is the distance from start to neighbor through current\n            tentative_g = g_score[current] + cost(current, neighbor)\n\n            if neighbor not in g_score OR tentative_g &lt; g_score[neighbor]:\n                // This path to neighbor is better than any previous one\n                came_from[neighbor] = current\n                g_score[neighbor] = tentative_g\n                f_score[neighbor] = g_score[neighbor] + heuristic(neighbor, goal)\n\n                if neighbor not in open_set:\n                    open_set.add(neighbor, f_score[neighbor])\n\n    return failure  // No path exists\n</code></pre>"},{"location":"ds_algo/algos/05_a_star_search/#time-and-space-complexity","title":"Time and Space Complexity","text":"<ul> <li>Time Complexity: O(E log V) with a good heuristic, where V is the number of vertices and E is the number of edges</li> <li>Space Complexity: O(V) for the priority queue, visited set, and path tracking</li> </ul>"},{"location":"ds_algo/algos/05_a_star_search/#annotated-code-template","title":"Annotated Code Template","text":"<pre><code>import heapq\n\ndef a_star_search(grid, start, goal):\n    \"\"\"\n    Finds the shortest path from start to goal in a grid using A* Search.\n\n    Args:\n        grid: 2D array where 0 is open space and 1 is obstacle\n        start: Tuple (row, col) representing start position\n        goal: Tuple (row, col) representing goal position\n\n    Returns:\n        List of positions representing the path, or None if no path exists\n    \"\"\"\n    # Define directions (up, right, down, left)\n    directions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n\n    # TODO: Implement heuristic function (Manhattan distance)\n    def heuristic(a, b):\n        pass\n\n    # TODO: Implement A* algorithm\n    # 1. Initialize open set (priority queue), closed set, g_score, f_score\n    # 2. While open set is not empty:\n    #    a. Get node with lowest f_score\n    #    b. If at goal, reconstruct path and return\n    #    c. Move current node to closed set\n    #    d. For each neighbor:\n    #       i. Skip if in closed set or obstacle\n    #       ii. Calculate tentative g_score\n    #       iii. If new path is better, update g_score, f_score, and came_from\n\n    return None  # No path found\n</code></pre>"},{"location":"ds_algo/algos/05_a_star_search/#live-coding-group-activity","title":"Live Coding Group Activity","text":""},{"location":"ds_algo/algos/05_a_star_search/#team-roles","title":"Team Roles","text":"<ul> <li>Driver: Types the code</li> <li>Navigator: Guides the implementation</li> <li>Explainer: Verbalizes what's happening at each step</li> </ul>"},{"location":"ds_algo/algos/05_a_star_search/#task","title":"Task","text":"<p>Complete the A* Search implementation by filling in the missing code in the template. Test it with the following grid:</p> <pre><code># Example grid (0 = open, 1 = obstacle)\ngrid = [\n    [0, 0, 0, 0, 0],\n    [0, 1, 1, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 1, 1, 1, 0],\n    [0, 0, 0, 0, 0]\n]\n\nstart = (0, 0)  # Top-left\ngoal = (4, 4)   # Bottom-right\n\n# Expected path: [(0,0), (1,0), (2,0), (3,0), (4,0), (4,1), (4,2), (4,3), (4,4)]\n# or another valid shortest path\n</code></pre>"},{"location":"ds_algo/algos/05_a_star_search/#checkpoint-questions","title":"Checkpoint Questions","text":"<ol> <li>How does the heuristic function affect A* performance?</li> <li>What happens if h(n) is always 0? How does A* behave?</li> <li>What makes a heuristic \"admissible\" and why is that important?</li> </ol>"},{"location":"ds_algo/algos/05_a_star_search/#complete-implementation","title":"Complete Implementation","text":"<p>After your group discussion, compare your solution with this implementation:</p> <pre><code>import heapq\n\ndef a_star_search(grid, start, goal):\n    \"\"\"\n    Finds the shortest path from start to goal in a grid using A* Search.\n\n    Args:\n        grid: 2D array where 0 is open space and 1 is obstacle\n        start: Tuple (row, col) representing start position\n        goal: Tuple (row, col) representing goal position\n\n    Returns:\n        List of positions representing the path, or None if no path exists\n    \"\"\"\n    rows, cols = len(grid), len(grid[0])\n\n    # Define directions (up, right, down, left)\n    directions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n\n    # Manhattan distance heuristic\n    def heuristic(a, b):\n        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n\n    # Check if a position is valid\n    def is_valid(pos):\n        r, c = pos\n        return 0 &lt;= r &lt; rows and 0 &lt;= c &lt; cols and grid[r][c] == 0\n\n    # Initialize data structures\n    open_set = []  # Priority queue\n    heapq.heappush(open_set, (0, start))  # (f_score, position)\n\n    came_from = {}  # To reconstruct path\n    g_score = {start: 0}  # Cost from start to current\n    f_score = {start: heuristic(start, goal)}  # Estimated total cost\n\n    # For the priority queue, we need to track positions we've seen\n    open_set_hash = {start}\n\n    while open_set:\n        # Get node with lowest f_score\n        _, current = heapq.heappop(open_set)\n        open_set_hash.remove(current)\n\n        # If we reached the goal, reconstruct and return path\n        if current == goal:\n            path = []\n            while current in came_from:\n                path.append(current)\n                current = came_from[current]\n            path.append(start)\n            return path[::-1]  # Reverse to get path from start to goal\n\n        # Check neighbors\n        for dr, dc in directions:\n            neighbor = (current[0] + dr, current[1] + dc)\n\n            # Skip invalid or obstacle positions\n            if not is_valid(neighbor):\n                continue\n\n            # Calculate tentative g_score (cost is 1 for each step)\n            tentative_g = g_score[current] + 1\n\n            # If this path to neighbor is better than any previous one\n            if neighbor not in g_score or tentative_g &lt; g_score[neighbor]:\n                came_from[neighbor] = current\n                g_score[neighbor] = tentative_g\n                f_score[neighbor] = tentative_g + heuristic(neighbor, goal)\n\n                if neighbor not in open_set_hash:\n                    heapq.heappush(open_set, (f_score[neighbor], neighbor))\n                    open_set_hash.add(neighbor)\n\n    return None  # No path found\n\n# Example usage\nif __name__ == \"__main__\":\n    grid = [\n        [0, 0, 0, 0, 0],\n        [0, 1, 1, 0, 0],\n        [0, 1, 0, 0, 0],\n        [0, 1, 1, 1, 0],\n        [0, 0, 0, 0, 0]\n    ]\n\n    start = (0, 0)  # Top-left\n    goal = (4, 4)   # Bottom-right\n\n    path = a_star_search(grid, start, goal)\n\n    if path:\n        print(f\"Path found: {path}\")\n\n        # Visualize the path on the grid\n        visual_grid = [['\u25a1' if cell == 0 else '\u25a0' for cell in row] for row in grid]\n        visual_grid[start[0]][start[1]] = 'S'\n        visual_grid[goal[0]][goal[1]] = 'G'\n\n        for r, c in path:\n            if (r, c) != start and (r, c) != goal:\n                visual_grid[r][c] = '\u25cf'\n\n        print(\"\\nGrid Visualization:\")\n        for row in visual_grid:\n            print(' '.join(row))\n    else:\n        print(\"No path found\")\n</code></pre>"},{"location":"ds_algo/algos/05_a_star_search/#common-implementation-mistakes","title":"Common Implementation Mistakes","text":"<ol> <li>Incorrect heuristic: Using a non-admissible heuristic that overestimates costs.</li> <li>Inefficient priority queue updates: Not handling updates to nodes already in the open set.</li> <li>Missing edge cases: Not checking grid boundaries or obstacles properly.</li> <li>Not tracking visited nodes: Causing redundant exploration.</li> <li>Incorrect path reconstruction: Failing to properly reconstruct the shortest path.</li> </ol>"},{"location":"ds_algo/algos/05_a_star_search/#peer-discussion-prompts","title":"Peer Discussion Prompts","text":"<ol> <li>How does A* compare to Uniform Cost Search?</li> <li>What makes a good heuristic for A*?</li> <li>How would you adapt A* for navigation in a real-world map?</li> <li>In what scenarios might A* not be the best algorithm?</li> </ol>"},{"location":"ds_algo/algos/05_a_star_search/#mini-challenge-diagonal-movement","title":"Mini-Challenge: Diagonal Movement","text":"<p>Modify your A* implementation to allow diagonal movement (8 directions instead of 4).</p> <pre><code>def a_star_with_diagonals(grid, start, goal):\n    \"\"\"\n    A* search with diagonal movement allowed.\n\n    Returns:\n        List of positions representing the path, or None if no path exists\n    \"\"\"\n    # TODO: Implement A* with diagonal movement\n    # 1. Add diagonal directions\n    # 2. Update cost calculation (diagonal movement usually costs \u221a2)\n    # 3. Consider whether to allow \"corner cutting\"\n\n    pass\n</code></pre>"},{"location":"ds_algo/algos/05_a_star_search/#extension-for-fast-learners","title":"Extension for Fast Learners","text":"<p>Implement A with a weighted heuristic, where f(n) = g(n) + wh(n), to see how different weights affect the search.</p>"},{"location":"ds_algo/algos/05_a_star_search/#heuristic-functions","title":"Heuristic Functions","text":"<p>Different heuristics can be used depending on the problem:</p>"},{"location":"ds_algo/algos/05_a_star_search/#manhattan-distance-l1-norm","title":"Manhattan Distance (L1 Norm)","text":"<pre><code>def manhattan_distance(a, b):\n    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n</code></pre>"},{"location":"ds_algo/algos/05_a_star_search/#euclidean-distance-l2-norm","title":"Euclidean Distance (L2 Norm)","text":"<pre><code>def euclidean_distance(a, b):\n    return ((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2) ** 0.5\n</code></pre>"},{"location":"ds_algo/algos/05_a_star_search/#diagonal-distance-chebyshev-distance","title":"Diagonal Distance (Chebyshev Distance)","text":"<pre><code>def diagonal_distance(a, b):\n    return max(abs(a[0] - b[0]), abs(a[1] - b[1]))\n</code></pre>"},{"location":"ds_algo/algos/05_a_star_search/#applications-of-a","title":"Applications of A*","text":"<ul> <li>Pathfinding in video games</li> <li>Robot navigation</li> <li>GPS routing systems</li> <li>Network packet routing</li> <li>Puzzles like Sliding Puzzle or Tower of Hanoi</li> <li>Motion planning in robotics</li> </ul>"},{"location":"ds_algo/algos/05_a_star_search/#grid-navigation-with-weighted-costs","title":"Grid Navigation With Weighted Costs","text":"<p>In many real scenarios, different terrain types have different movement costs:</p> <pre><code># Grid with terrain costs (e.g., road=1, grass=2, swamp=5)\nterrain_grid = [\n    [1, 1, 1, 2, 2],\n    [1, 5, 5, 2, 2],\n    [1, 5, 1, 1, 2],\n    [1, 5, 5, 5, 1],\n    [1, 1, 1, 1, 1]\n]\n\ndef a_star_weighted_terrain(terrain_grid, start, goal):\n    \"\"\"A* search that takes terrain costs into account\"\"\"\n    # Similar to regular A*, but use terrain costs for g-score calculation\n    pass\n</code></pre>"},{"location":"ds_algo/algos/05_a_star_search/#team-reflection","title":"Team Reflection","text":"<p>As a group, discuss:</p> <ol> <li>How does the choice of heuristic affect both the efficiency and the optimality of A*?</li> <li>When is A* guaranteed to find the optimal path?</li> <li>How would you handle scenarios where the environment changes during execution?</li> <li>What are the trade-offs between speed and accuracy when implementing A* in real-world applications?</li> </ol>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/","title":"Iterative Deepening Depth-First Search (IDDFS) Algorithm","text":""},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#objective","title":"Objective","text":"<p>Master the Iterative Deepening Depth-First Search algorithm through collaborative implementation and analysis. By the end of this session, your team will understand how IDDFS combines the space efficiency of DFS with the level-by-level search benefits of BFS.</p>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#environment-setup","title":"Environment Setup","text":"Requirements Python 3.8+ Standard library only <p>Verify your environment:</p> <pre><code>python --version  # Should be 3.8+\n</code></pre>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#visual-explanation","title":"Visual Explanation","text":"<p>Iterative Deepening DFS performs multiple depth-limited DFS searches, incrementing the depth limit with each iteration until the goal is found.</p> <pre><code>    A\n   / \\\n  B   C\n / \\   \\\nD   E   F\n</code></pre> <p>IDDFS with increasing depth limits:</p> <p>Depth 0: A Depth 1: A \u2192 B, A \u2192 C Depth 2: A \u2192 B \u2192 D, A \u2192 B \u2192 E, A \u2192 C \u2192 F</p> <p>It combines the completeness and optimality of BFS with the space efficiency of DFS.</p>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#algorithm-walkthrough","title":"Algorithm Walkthrough","text":""},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#pseudocode","title":"Pseudocode","text":"<pre><code>function IDDFS(graph, start, goal):\n    for depth from 0 to \u221e:\n        result = DepthLimitedSearch(graph, start, goal, depth)\n        if result != \"cutoff\":\n            return result\n\nfunction DepthLimitedSearch(graph, node, goal, depth_limit):\n    if node == goal:\n        return node\n    if depth_limit == 0:\n        return \"cutoff\"\n\n    cutoff_occurred = false\n\n    for each neighbor of node:\n        result = DepthLimitedSearch(graph, neighbor, goal, depth_limit - 1)\n\n        if result == \"cutoff\":\n            cutoff_occurred = true\n        else if result != \"failure\":\n            return result\n\n    if cutoff_occurred:\n        return \"cutoff\"\n    else:\n        return \"failure\"\n</code></pre>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#time-and-space-complexity","title":"Time and Space Complexity","text":"<ul> <li>Time Complexity: O(b^d) where b is the branching factor and d is the depth of the shallowest solution</li> <li>Space Complexity: O(d) for the depth-limited search, much better than BFS</li> </ul>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#annotated-code-template","title":"Annotated Code Template","text":"<pre><code>def depth_limited_search(graph, node, goal, depth_limit, visited=None, path=None):\n    \"\"\"\n    Performs depth-limited search from node to goal with a specified depth limit.\n\n    Args:\n        graph: Dictionary representing an adjacency list graph\n               {node: [neighbor1, neighbor2, ...]}\n        node: Current node to explore\n        goal: Target node to find\n        depth_limit: Maximum depth to search\n        visited: Set of visited nodes (initialized if None)\n        path: Current path (initialized if None)\n\n    Returns:\n        Tuple (found, path):\n            - found: True if goal found, False otherwise\n            - path: Path to goal if found, None otherwise\n    \"\"\"\n    if visited is None:\n        visited = set()\n    if path is None:\n        path = [node]\n\n    # TODO: Implement depth-limited search\n    # 1. Check if we've found the goal\n    # 2. Check if we've reached the depth limit\n    # 3. Mark current node as visited\n    # 4. Recursively visit all unvisited neighbors with reduced depth limit\n\n    return False, None\n\ndef iterative_deepening_dfs(graph, start, goal, max_depth=float('inf')):\n    \"\"\"\n    Performs iterative deepening DFS from start to goal up to a maximum depth.\n\n    Args:\n        graph: Dictionary representing an adjacency list graph\n               {node: [neighbor1, neighbor2, ...]}\n        start: Starting node\n        goal: Target node to find\n        max_depth: Maximum depth to search (default: infinity)\n\n    Returns:\n        Tuple (found, path, depth):\n            - found: True if goal found, False otherwise\n            - path: Path to goal if found, None otherwise\n            - depth: Depth at which the goal was found\n    \"\"\"\n    # TODO: Implement iterative deepening DFS\n    # 1. For each depth from 0 to max_depth:\n    #    a. Call depth_limited_search\n    #    b. If goal found, return the result\n\n    return False, None, -1\n</code></pre>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#live-coding-group-activity","title":"Live Coding Group Activity","text":""},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#team-roles","title":"Team Roles","text":"<ul> <li>Driver: Types the code</li> <li>Navigator: Guides the implementation</li> <li>Explainer: Verbalizes what's happening at each step</li> </ul>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#task","title":"Task","text":"<p>Complete the IDDFS implementation by filling in the missing code in the template. Test it with the following graph:</p> <pre><code># Example graph as adjacency list\ngraph = {\n    'A': ['B', 'C'],\n    'B': ['A', 'D', 'E'],\n    'C': ['A', 'F'],\n    'D': ['B'],\n    'E': ['B'],\n    'F': ['C', 'G'],\n    'G': ['F']\n}\n\n# Test various starting points and goals\ntest_cases = [\n    ('A', 'G'),  # Should find at depth 3: A\u2192C\u2192F\u2192G\n    ('A', 'E'),  # Should find at depth 2: A\u2192B\u2192E\n    ('D', 'C'),  # Should find at depth 2: D\u2192B\u2192A\u2192C\n    ('E', 'G'),  # Should find at depth 4: E\u2192B\u2192A\u2192C\u2192F\u2192G\n]\n\nfor start, goal in test_cases:\n    found, path, depth = iterative_deepening_dfs(graph, start, goal)\n    if found:\n        print(f\"Path from {start} to {goal}: {' \u2192 '.join(path)} (found at depth {depth})\")\n    else:\n        print(f\"No path found from {start} to {goal}\")\n</code></pre>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#checkpoint-questions","title":"Checkpoint Questions","text":"<ol> <li>How does IDDFS combine the benefits of BFS and DFS?</li> <li>Why is the space complexity of IDDFS better than BFS?</li> <li>In what scenarios would IDDFS be preferable to either BFS or DFS?</li> </ol>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#complete-implementation","title":"Complete Implementation","text":"<p>After your group discussion, compare your solution with this implementation:</p> <pre><code>def depth_limited_search(graph, node, goal, depth_limit, visited=None, path=None):\n    \"\"\"\n    Performs depth-limited search from node to goal with a specified depth limit.\n\n    Args:\n        graph: Dictionary representing an adjacency list graph\n               {node: [neighbor1, neighbor2, ...]}\n        node: Current node to explore\n        goal: Target node to find\n        depth_limit: Maximum depth to search\n        visited: Set of visited nodes (initialized if None)\n        path: Current path (initialized if None)\n\n    Returns:\n        Tuple (found, path):\n            - found: True if goal found, False otherwise\n            - path: Path to goal if found, None otherwise\n    \"\"\"\n    if visited is None:\n        visited = set([node])\n    if path is None:\n        path = [node]\n\n    # Base case: found the goal\n    if node == goal:\n        return True, path\n\n    # Base case: reached depth limit\n    if depth_limit == 0:\n        return False, None\n\n    # Explore neighbors\n    for neighbor in graph[node]:\n        if neighbor not in visited:\n            # Mark as visited and update path\n            visited.add(neighbor)\n            result, new_path = depth_limited_search(\n                graph, neighbor, goal, depth_limit - 1,\n                visited.copy(), path + [neighbor]\n            )\n\n            # If goal found in this branch, return the result\n            if result:\n                return True, new_path\n\n    # Goal not found within depth limit\n    return False, None\n\ndef iterative_deepening_dfs(graph, start, goal, max_depth=float('inf')):\n    \"\"\"\n    Performs iterative deepening DFS from start to goal up to a maximum depth.\n\n    Args:\n        graph: Dictionary representing an adjacency list graph\n               {node: [neighbor1, neighbor2, ...]}\n        start: Starting node\n        goal: Target node to find\n        max_depth: Maximum depth to search (default: infinity)\n\n    Returns:\n        Tuple (found, path, depth):\n            - found: True if goal found, False otherwise\n            - path: Path to goal if found, None otherwise\n            - depth: Depth at which the goal was found\n    \"\"\"\n    # Iterate through increasing depth limits\n    for depth in range(max_depth + 1):\n        found, path = depth_limited_search(graph, start, goal, depth)\n\n        # If goal found, return the result along with the depth\n        if found:\n            return True, path, depth\n\n    # Goal not found within max_depth\n    return False, None, -1\n\n# Example usage\nif __name__ == \"__main__\":\n    graph = {\n        'A': ['B', 'C'],\n        'B': ['A', 'D', 'E'],\n        'C': ['A', 'F'],\n        'D': ['B'],\n        'E': ['B'],\n        'F': ['C', 'G'],\n        'G': ['F']\n    }\n\n    start = 'A'\n    goal = 'G'\n\n    found, path, depth = iterative_deepening_dfs(graph, start, goal)\n\n    if found:\n        print(f\"Path from {start} to {goal}: {' \u2192 '.join(path)}\")\n        print(f\"Found at depth: {depth}\")\n    else:\n        print(f\"No path found from {start} to {goal}\")\n</code></pre>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#visualizing-the-algorithm","title":"Visualizing the Algorithm","text":"<p>Let's trace through the execution of IDDFS for finding a path from 'A' to 'G' in our example graph:</p> <p>Depth 0:</p> <ul> <li>Explore A</li> <li>Goal not found</li> </ul> <p>Depth 1:</p> <ul> <li>Explore A</li> <li>Explore B (neighbor of A)</li> <li>Explore C (neighbor of A)</li> <li>Goal not found</li> </ul> <p>Depth 2:</p> <ul> <li>Explore A</li> <li>Explore B (neighbor of A)</li> <li>Explore D (neighbor of B)</li> <li>Explore E (neighbor of B)</li> <li>Explore C (neighbor of A)</li> <li>Explore F (neighbor of C)</li> <li>Goal not found</li> </ul> <p>Depth 3:</p> <ul> <li>Explore A</li> <li>Explore B (neighbor of A)</li> <li>Explore D (neighbor of B)</li> <li>Explore E (neighbor of B)</li> <li>Explore C (neighbor of A)</li> <li>Explore F (neighbor of C)<ul> <li>Explore G (neighbor of F) - Goal found!</li> </ul> </li> <li>Return path: A \u2192 C \u2192 F \u2192 G</li> </ul>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#common-implementation-mistakes","title":"Common Implementation Mistakes","text":"<ol> <li>Incorrect cycle detection: IDDFS needs proper cycle detection to avoid infinite loops.</li> <li>Not resetting visited sets: Each depth iteration needs a fresh visited set.</li> <li>Incorrect path tracking: Ensure the path is properly updated during the search.</li> <li>Inefficient re-exploration: The algorithm re-explores shallow nodes multiple times.</li> <li>Neglecting early termination: Should stop as soon as the goal is found.</li> </ol>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#peer-discussion-prompts","title":"Peer Discussion Prompts","text":"<ol> <li>How does IDDFS compare to BFS in terms of finding the shortest path?</li> <li>What are the trade-offs between IDDFS, BFS, and DFS?</li> <li>Can IDDFS be adapted to handle weighted graphs?</li> <li>In what real-world scenarios would IDDFS be particularly useful?</li> </ol>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#mini-challenge-path-finding-with-limited-memory","title":"Mini-Challenge: Path Finding with Limited Memory","text":"<p>Modify your IDDFS implementation to work effectively when memory is limited by using an iterative (non-recursive) approach.</p> <pre><code>def iterative_iddfs(graph, start, goal, max_depth=float('inf')):\n    \"\"\"\n    Implements IDDFS without recursion to save memory.\n\n    Returns:\n        Tuple (found, path, depth)\n    \"\"\"\n    # TODO: Implement iterative IDDFS\n    # 1. Use a stack instead of recursion\n    # 2. Keep track of node depth explicitly\n\n    pass\n</code></pre>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#extension-for-fast-learners","title":"Extension for Fast Learners","text":"<p>Implement a bidirectional IDDFS that searches from both the start and goal nodes simultaneously.</p>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#applications-of-iddfs","title":"Applications of IDDFS","text":"<ul> <li>Game playing algorithms</li> <li>Puzzle solving</li> <li>Route finding with limited memory</li> <li>Network packet routing</li> <li>Web crawling with depth limits</li> <li>Hierarchical data exploration</li> </ul>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#iddfs-space-optimization","title":"IDDFS Space Optimization","text":"<p>One of the primary benefits of IDDFS is its space efficiency. Let's compare:</p> Algorithm Time Complexity Space Complexity Complete? Optimal Path? BFS O(b^d) O(b^d) Yes Yes (unweighted) DFS O(b^d) O(d) No No IDDFS O(b^d) O(d) Yes Yes (unweighted) <p>Where:</p> <ul> <li>b = branching factor</li> <li>d = depth of shallowest solution</li> </ul> <p>IDDFS may seem inefficient due to repeated exploration, but in practice, most of the work happens at the deepest level, making the overhead manageable.</p>"},{"location":"ds_algo/algos/06_iterative_deepening_dfs/#team-reflection","title":"Team Reflection","text":"<p>As a group, discuss:</p> <ol> <li>In what scenarios would the re-exploration overhead of IDDFS be worth the space savings?</li> <li>How does the branching factor of the graph affect the efficiency of IDDFS?</li> <li>Could IDDFS be combined with heuristics like A* to create a more efficient algorithm?</li> <li>What modifications would be needed to adapt IDDFS for trees instead of graphs?</li> </ol>"},{"location":"ds_algo/algos/ds_algo_prompt/","title":"Documentation Request Template","text":""},{"location":"ds_algo/algos/ds_algo_prompt/#project-type","title":"Project Type","text":"<p>Algorithm</p>"},{"location":"ds_algo/algos/ds_algo_prompt/#documentation-goal","title":"Documentation Goal","text":"<p>Create six standalone documentation files focused on active, collaborative learning of core search algorithms through hands-on coding and structured group work.</p>"},{"location":"ds_algo/algos/ds_algo_prompt/#target-audience","title":"Target Audience","text":"<p>Students, beginner developers, and coding bootcamp learners working in small groups or peer-learning environments.</p>"},{"location":"ds_algo/algos/ds_algo_prompt/#technical-requirements","title":"Technical Requirements","text":"<ul> <li>Environment: Any OS with Python 3.8+</li> <li>Prerequisites: Intro-level programming and understanding of basic data structures (lists, sets, queues)</li> <li>Dependencies: Python standard library only</li> </ul>"},{"location":"ds_algo/algos/ds_algo_prompt/#desired-sections-for-each-of-the-6-files","title":"Desired Sections (for each of the 6 files)","text":"<ul> <li>Environment setup</li> <li>Visual explanation of the algorithm</li> <li>Pseudocode</li> <li>Annotated code template (incomplete)</li> <li>Live coding group activity</li> <li>Peer discussion prompts</li> <li>Time and space complexity walkthrough</li> <li>Common implementation mistakes</li> <li>Mini-challenge to reinforce learning</li> </ul>"},{"location":"ds_algo/algos/ds_algo_prompt/#special-focus-areas","title":"Special Focus Areas","text":"<ul> <li>Encourage students to compare and contrast algorithm performance</li> <li>Assign roles in group work (driver, navigator, explainer)</li> <li>Include checkpoints to test and reflect as a team</li> <li>Offer optional extensions for fast learners</li> </ul>"},{"location":"ds_algo/algos/ds_algo_prompt/#code-examples-needed","title":"Code Examples Needed","text":"<p>Each file focuses on one algorithm:</p> <ol> <li> <p>Breadth-First Search (BFS)</p> </li> <li> <p>Use queue</p> </li> <li> <p>Apply to graph traversal</p> </li> <li> <p>Depth-First Search (DFS)</p> </li> <li> <p>Use stack or recursion</p> </li> <li> <p>Include graph and tree traversal examples</p> </li> <li> <p>Binary Search</p> </li> <li> <p>Iterative and recursive</p> </li> <li> <p>Explain sorted array requirement</p> </li> <li> <p>Uniform Cost Search</p> </li> <li> <p>Use priority queue</p> </li> <li> <p>Include cost-annotated graph</p> </li> <li> <p>A* Search</p> </li> <li> <p>Combine heuristic with path cost</p> </li> <li> <p>Apply to grid map navigation</p> </li> <li> <p>Iterative Deepening DFS</p> </li> <li>Hybrid of DFS and BFS</li> <li>Emphasize space/time benefits</li> </ol>"},{"location":"ds_algo/algos/ds_algo_prompt/#additional-notes","title":"Additional Notes","text":"<p>Each file must:</p> <ul> <li>Be self-contained and runnable in under 2 minutes</li> <li>Include at least one pair programming activity</li> <li>Have a peer-review code prompt</li> <li>Include \u201ccheckpoint questions\u201d to guide group reflection</li> </ul>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/","title":"Arrays and Lists: Fundamental Sequential Data Structures","text":""},{"location":"ds_algo/data_structures/01_arrays_and_lists/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the differences between arrays (fixed-size) and lists (dynamic)</li> <li>Implement key operations: indexing, appending, slicing, and insertion</li> <li>Analyze time and space complexity of different operations</li> <li>Make informed decisions about when to use arrays vs lists</li> </ul>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Ensure Python 3.8+ is installed (<code>python --version</code>) 2. No external packages required (uses Python's built-in data structures) 3. Create a working directory for your code 4. Save code examples as <code>.py</code> files 5. Run with <code>python filename.py</code>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#concept-overview","title":"Concept Overview","text":"<p>Arrays and lists are sequential data structures that store elements in contiguous memory locations.</p>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#arrays-vs-lists-in-python","title":"Arrays vs Lists in Python","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Python Lists                \u2502 Traditional Arrays    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500-\u2502\n\u2502 Dynamic size                \u2502 Fixed size           \u2502\n\u2502 Mixed data types            \u2502 Homogeneous elements \u2502\n\u2502 Built-in methods            \u2502 Basic operations     \u2502\n\u2502 Memory overhead             \u2502 Memory efficient     \u2502\n\u2502 Implemented as array lists  \u2502 Direct memory access \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In Python, the built-in <code>list</code> type is actually an array list - a dynamic array that resizes automatically. For true fixed-size arrays, you can use the <code>array</code> module or NumPy arrays.</p>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#starter-code-with-gaps","title":"Starter Code with Gaps","text":"<p>Save this as <code>array_list_operations.py</code>:</p> <pre><code>\"\"\"\nArray and List Operations - Collaborative Learning Exercise\n\"\"\"\nimport array\n\ndef main():\n    # PART 1: Python Lists (Dynamic Arrays)\n    print(\"===== PYTHON LISTS =====\")\n\n    # TODO: Create an empty list\n    my_list = []\n\n    # TODO: Append elements to the list (implement with your team)\n    # Add code here\n\n    # TODO: Demonstrate indexing (positive and negative)\n    # Add code here\n\n    # TODO: Implement list slicing examples\n    # Add code here\n\n    # TODO: Insert elements at specific positions\n    # Add code here\n\n    # PART 2: Fixed-size Arrays using array module\n    print(\"\\n===== FIXED-SIZE ARRAYS =====\")\n\n    # TODO: Create a fixed-size integer array\n    # Hint: Use array.array('i', [...])\n    # Add code here\n\n    # TODO: Try operations and observe differences from lists\n    # Add code here\n\n    # PART 3: Implement a simple ArrayList class (similar to Java's ArrayList)\n    print(\"\\n===== CUSTOM ARRAYLIST IMPLEMENTATION =====\")\n\nclass ArrayList:\n    \"\"\"A simplified implementation of a dynamic array (similar to ArrayList in Java)\"\"\"\n\n    def __init__(self, capacity=10):\n        \"\"\"Initialize with a default capacity of 10 elements\"\"\"\n        # TODO: Create a fixed-size array with initial capacity\n        # Hint: Use [None] * capacity\n        self.size = 0\n\n    def append(self, element):\n        \"\"\"Add an element to the end of the array\"\"\"\n        # TODO: Implement append with resizing when needed\n        pass\n\n    def get(self, index):\n        \"\"\"Get element at index\"\"\"\n        # TODO: Implement with bounds checking\n        pass\n\n    def insert(self, index, element):\n        \"\"\"Insert element at specific index\"\"\"\n        # TODO: Implement insert with shifting elements\n        pass\n\n    def remove(self, index):\n        \"\"\"Remove element at index\"\"\"\n        # TODO: Implement remove with shifting elements\n        pass\n\n    def __str__(self):\n        \"\"\"String representation of the array\"\"\"\n        # TODO: Implement string representation\n        pass\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#live-coding-collaboration-tasks","title":"Live Coding Collaboration Tasks","text":""},{"location":"ds_algo/data_structures/01_arrays_and_lists/#task-1-implement-missing-list-operations","title":"Task 1: Implement Missing List Operations","text":"<p>Working in pairs, fill in the TODOs for the list operations in the main function. Take turns writing code while the other reviews and suggests improvements.</p>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#task-2-complete-the-arraylist-class","title":"Task 2: Complete the ArrayList Class","text":"<p>In teams of 2-3:</p> <ol> <li>First coder implements <code>__init__</code> and <code>append</code></li> <li>Second coder implements <code>get</code> and <code>__str__</code></li> <li>Third coder (or back to first) implements <code>insert</code> and <code>remove</code></li> <li>All review the implementation together</li> </ol>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#peer-discussion-questions","title":"Peer Discussion Questions","text":"<ol> <li> <p>Complexity Analysis: What is the time complexity of:</p> </li> <li> <p>Accessing an element by index in a list vs. ArrayList?</p> </li> <li>Appending an element to a list? What about when resizing occurs?</li> <li> <p>Inserting at the beginning of a list? How could you optimize this?</p> </li> <li> <p>Design Decisions:</p> </li> <li> <p>When would you use a fixed-size array vs. a dynamic list?</p> </li> <li>What factors affect the performance of arrays/lists?</li> <li> <p>How does memory layout affect performance for large datasets?</p> </li> <li> <p>Testing Edge Cases:</p> </li> <li>What edge cases should we test for our ArrayList implementation?</li> <li>How might our implementation fail with very large datasets?</li> </ol>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#time-and-space-complexity","title":"Time and Space Complexity","text":"Operation Python List (Average) Python List (Worst) Fixed Array Access O(1) O(1) O(1) Append O(1) O(n) [resizing] N/A Insert O(n) O(n) O(n) Delete O(n) O(n) O(n) Search O(n) O(n) O(n) <p>Space complexity: O(n) where n is the number of elements</p>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#common-bugs-and-debugging-tips","title":"Common Bugs and Debugging Tips","text":"<ol> <li> <p>Index Out of Range</p> </li> <li> <p>Error: <code>IndexError: list index out of range</code></p> </li> <li> <p>Debug: Check bounds before accessing. Use try/except or explicit bounds checking.</p> </li> <li> <p>Modifying While Iterating</p> </li> <li> <p>Error: Unexpected behavior when modifying a list during iteration</p> </li> <li> <p>Debug: Create a copy of the list or iterate backward when removing items.</p> </li> <li> <p>Aliasing vs Copying</p> </li> </ol> <pre><code>list1 = [1, 2, 3]\nlist2 = list1          # Aliasing - both variables refer to the same list\nlist3 = list1.copy()   # Creates a new copy of the list\n\nlist1[0] = 99          # Modifies both list1 and list2, but not list3\n</code></pre> <ol> <li>Inefficient Appending</li> <li>Error: Slow performance when building a list</li> <li>Debug: Use <code>append()</code> instead of concatenation (<code>list += [item]</code>)</li> </ol>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#group-challenge-custom-list-implementation","title":"Group Challenge: Custom List Implementation","text":""},{"location":"ds_algo/data_structures/01_arrays_and_lists/#challenge-task","title":"Challenge Task","text":"<p>Extend the ArrayList implementation to include:</p> <ol> <li>A <code>capacity()</code> method that returns the current capacity</li> <li>An efficient <code>extend()</code> method that adds all elements from another list</li> <li>A method to shrink the internal array when it's significantly larger than needed</li> <li>A performance test comparing your implementation to Python's built-in list</li> </ol>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#approach-instructions","title":"Approach Instructions","text":"<ol> <li>Plan together: Draw the internal state of the ArrayList during operations</li> <li>Rotate roles:</li> <li>Implementer: Writes the code</li> <li>Tester: Develops test cases</li> <li>Reviewer: Evaluates performance and correctness</li> <li>Checkpoints:</li> <li>After implementing each method, run tests and discuss complexity</li> <li>When all features are complete, compare performance with Python's built-in list</li> </ol>"},{"location":"ds_algo/data_structures/01_arrays_and_lists/#reflection-questions","title":"Reflection Questions","text":"<p>After completing the exercises:</p> <ol> <li>How does Python's list implementation differ from your ArrayList?</li> <li>What were the most challenging aspects of the implementation?</li> <li>In what scenarios would your custom implementation be better/worse than using a built-in list?</li> <li>How would you improve your implementation for better performance?</li> </ol>"},{"location":"ds_algo/data_structures/02_stacks/","title":"Stacks: Last-In-First-Out (LIFO) Data Structures","text":""},{"location":"ds_algo/data_structures/02_stacks/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the LIFO (Last-In-First-Out) principle of stacks</li> <li>Implement a stack using Python lists and a custom class</li> <li>Apply stack operations: push, pop, peek, and isEmpty</li> <li>Solve common problems using stacks</li> <li>Analyze time and space complexity of stack operations</li> </ul>"},{"location":"ds_algo/data_structures/02_stacks/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Ensure Python 3.8+ is installed (<code>python --version</code>) 2. No external packages required (uses Python's built-in data structures) 3. Create a working directory for your code 4. Save code examples as <code>.py</code> files 5. Run with <code>python filename.py</code>"},{"location":"ds_algo/data_structures/02_stacks/#concept-overview","title":"Concept Overview","text":"<p>A stack is a linear data structure that follows the LIFO principle - the last element added is the first one removed.</p>"},{"location":"ds_algo/data_structures/02_stacks/#stack-visualization","title":"Stack Visualization","text":"<pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   TOP      \u2502 \u2190 Most recently added (will be removed first)\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502   Item 3   \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502   Item 2   \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502   Item 1   \u2502 \u2190 First added (will be removed last)\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ds_algo/data_structures/02_stacks/#core-operations","title":"Core Operations","text":"<ul> <li>push(item): Add an item to the top of the stack</li> <li>pop(): Remove and return the top item from the stack</li> <li>peek() or top(): Return the top item without removing it</li> <li>isEmpty(): Check if the stack is empty</li> </ul>"},{"location":"ds_algo/data_structures/02_stacks/#starter-code-with-gaps","title":"Starter Code with Gaps","text":"<p>Save this as <code>stack_implementations.py</code>:</p> <pre><code>\"\"\"\nStack Implementations - Collaborative Learning Exercise\n\"\"\"\n\ndef main():\n    print(\"===== STACK USING PYTHON LIST =====\")\n    # TODO: Implement list-based stack operations\n    # Hint: Use list methods like append() and pop()\n\n    # PART 1: Create a stack using Python list\n    stack = []\n\n    # TODO: Push elements to the stack\n\n    # TODO: Implement peek functionality\n\n    # TODO: Pop elements from the stack\n\n    # TODO: Check if stack is empty\n\n    print(\"\\n===== CUSTOM STACK IMPLEMENTATION =====\")\n    # Create a stack with our custom implementation\n    my_stack = Stack()\n\n    # TODO: Use the stack methods\n\n    print(\"\\n===== STACK APPLICATIONS =====\")\n\n    # TODO: Implement bracket matching function\n    text = \"((3 + 5) * [10 - 2]) / {7}\"\n    print(f\"Brackets balanced in '{text}': {is_balanced(text)}\")\n\n    # TODO: Implement reverse string function using stack\n    text = \"Hello, World!\"\n    print(f\"Original: {text}\")\n    print(f\"Reversed: {reverse_string(text)}\")\n\n\nclass Stack:\n    \"\"\"A custom Stack implementation\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize an empty stack\"\"\"\n        # TODO: Initialize the internal data structure\n        # Hint: Use a Python list\n\n    def push(self, item):\n        \"\"\"Add item to the top of the stack\"\"\"\n        # TODO: Implement push operation\n        pass\n\n    def pop(self):\n        \"\"\"Remove and return the top item from the stack\"\"\"\n        # TODO: Implement pop operation with error handling\n        pass\n\n    def peek(self):\n        \"\"\"Return the top item without removing it\"\"\"\n        # TODO: Implement peek operation with error handling\n        pass\n\n    def is_empty(self):\n        \"\"\"Check if the stack is empty\"\"\"\n        # TODO: Implement isEmpty check\n        pass\n\n    def size(self):\n        \"\"\"Return the number of items in the stack\"\"\"\n        # TODO: Return stack size\n        pass\n\n    def __str__(self):\n        \"\"\"Return a string representation of the stack\"\"\"\n        # TODO: Implement string representation\n        pass\n\n\ndef is_balanced(text):\n    \"\"\"Check if brackets in the given text are balanced using a stack\"\"\"\n    # TODO: Implement a bracket matching algorithm using stack\n    # Hint: Push opening brackets, pop and check when closing brackets are found\n    pass\n\n\ndef reverse_string(text):\n    \"\"\"Reverse a string using a stack\"\"\"\n    # TODO: Implement string reversal using stack\n    # Hint: Push each character, then pop them all to get reversed order\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ds_algo/data_structures/02_stacks/#live-coding-collaboration-tasks","title":"Live Coding Collaboration Tasks","text":""},{"location":"ds_algo/data_structures/02_stacks/#task-1-list-based-stack-implementation","title":"Task 1: List-Based Stack Implementation","text":"<p>Working in pairs, implement the list-based stack operations in the main function:</p> <ol> <li>First person implements push and peek operations</li> <li>Second person implements pop and isEmpty operations</li> <li>Test the operations together</li> </ol>"},{"location":"ds_algo/data_structures/02_stacks/#task-2-complete-the-stack-class","title":"Task 2: Complete the Stack Class","text":"<p>In teams of 2-3:</p> <ol> <li>First coder implements <code>__init__</code>, <code>push</code>, and <code>is_empty</code></li> <li>Second coder implements <code>pop</code> and <code>peek</code></li> <li>Third coder (or back to first) implements <code>size</code> and <code>__str__</code></li> <li>All review the implementation together</li> </ol>"},{"location":"ds_algo/data_structures/02_stacks/#task-3-stack-applications","title":"Task 3: Stack Applications","text":"<p>Divide the <code>is_balanced</code> and <code>reverse_string</code> functions between team members and implement them using the Stack class.</p>"},{"location":"ds_algo/data_structures/02_stacks/#peer-discussion-questions","title":"Peer Discussion Questions","text":"<ol> <li> <p>Implementation Choices:</p> </li> <li> <p>What are the advantages/disadvantages of using a Python list vs. a custom Stack class?</p> </li> <li>How would you implement a stack with a maximum capacity?</li> <li> <p>Could you implement a stack using a linked list instead? What changes?</p> </li> <li> <p>Application Analysis:</p> </li> <li> <p>What other problems could be solved effectively with a stack?</p> </li> <li>When is a stack more appropriate than a queue?</li> <li> <p>How would you implement an \"undo\" feature in a text editor using stacks?</p> </li> <li> <p>Performance Consideration:</p> </li> <li>What is the time complexity of each stack operation?</li> <li>How does memory usage compare between different implementations?</li> <li>Are there any operations that could be optimized further?</li> </ol>"},{"location":"ds_algo/data_structures/02_stacks/#time-and-space-complexity","title":"Time and Space Complexity","text":"Operation Time Complexity Space Complexity Push O(1)* O(1) Pop O(1) O(1) Peek O(1) O(1) isEmpty O(1) O(1) Size O(1) O(1) <p>*Note: Push can be O(n) in rare cases when the internal array needs to resize.</p>"},{"location":"ds_algo/data_structures/02_stacks/#common-bugs-and-debugging-tips","title":"Common Bugs and Debugging Tips","text":"<ol> <li> <p>Stack Underflow</p> </li> <li> <p>Error: Trying to pop from an empty stack</p> </li> <li>Debug: Always check if the stack is empty before popping</li> </ol> <pre><code>if not stack.is_empty():\n    item = stack.pop()\nelse:\n    print(\"Cannot pop from empty stack\")\n</code></pre> <ol> <li> <p>Reference vs. Value</p> </li> <li> <p>Issue: Pushing reference types (like lists) can lead to unexpected behavior if modified later</p> </li> <li>Debug: Use deep copying when pushing mutable objects if you need to preserve their state</li> </ol> <pre><code>import copy\nstack.push(copy.deepcopy(my_list))\n</code></pre> <ol> <li> <p>Missing Boundary Checks</p> </li> <li> <p>Error: Index out of range errors</p> </li> <li>Debug: Ensure all stack operations check boundaries</li> </ol> <pre><code>def peek(self):\n    if self.is_empty():\n        raise IndexError(\"Cannot peek at an empty stack\")\n    return self.items[-1]\n</code></pre> <ol> <li>Stack Size Tracking</li> <li>Issue: Incorrect size tracking in custom implementations</li> <li>Debug: Ensure size is updated correctly in all operations</li> </ol>"},{"location":"ds_algo/data_structures/02_stacks/#group-challenge-evaluate-expressions-with-stacks","title":"Group Challenge: Evaluate Expressions with Stacks","text":""},{"location":"ds_algo/data_structures/02_stacks/#challenge-task","title":"Challenge Task","text":"<p>Implement a function to evaluate simple arithmetic expressions using two stacks:</p> <ol> <li>One stack for operators</li> <li>One stack for operands</li> </ol> <p>Example expressions:</p> <ul> <li>\"3 + 4 * 2\"</li> <li>\"( 1 + 2 ) * 3\"</li> <li>\"5 + ( 8 * 3 - 2 )\"</li> </ul>"},{"location":"ds_algo/data_structures/02_stacks/#approach-instructions","title":"Approach Instructions","text":"<ol> <li> <p>Planning Phase (10 minutes):</p> </li> <li> <p>Draw the state of both stacks at each step</p> </li> <li>Define operator precedence</li> <li> <p>Decide how to handle parentheses</p> </li> <li> <p>Implementation Phase (15 minutes):</p> </li> <li> <p>Person 1: Parse the expression into tokens</p> </li> <li>Person 2: Implement operator precedence logic</li> <li> <p>Person 3: Implement the evaluation algorithm</p> </li> <li> <p>Testing Phase (5 minutes):</p> </li> <li>Create test cases with different operators and parentheses</li> <li>Verify the results against expected outputs</li> <li>Test edge cases (empty expression, division by zero)</li> </ol>"},{"location":"ds_algo/data_structures/02_stacks/#starter-code-for-expression-evaluator","title":"Starter Code for Expression Evaluator","text":"<pre><code>def evaluate_expression(expression):\n    \"\"\"\n    Evaluate arithmetic expression using two stacks\n    \"\"\"\n    # TODO: Implement using operator and operand stacks\n    operators = Stack()  # Stack for operators\n    operands = Stack()   # Stack for numbers\n\n    # TODO: Tokenize the expression\n\n    # TODO: Process each token based on type (number, operator, parenthesis)\n\n    # TODO: Final evaluation and return result\n\n    return result\n</code></pre>"},{"location":"ds_algo/data_structures/02_stacks/#reflection-questions","title":"Reflection Questions","text":"<p>After completing the exercises:</p> <ol> <li>How do stacks help solve problems with a naturally recursive structure?</li> <li>What were the most challenging aspects of implementing the expression evaluator?</li> <li>How would your implementation change if you needed to support more complex operations (like exponents or functions)?</li> <li>In what real-world applications have you encountered stacks (explicitly or implicitly)?</li> </ol>"},{"location":"ds_algo/data_structures/03_queues/","title":"Queues: First-In-First-Out (FIFO) Data Structures","text":""},{"location":"ds_algo/data_structures/03_queues/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the FIFO (First-In-First-Out) principle of queues</li> <li>Implement standard, circular, and priority queues in Python</li> <li>Apply queue operations: enqueue, dequeue, peek, and isEmpty</li> <li>Analyze time and space complexity of queue operations</li> <li>Recognize application scenarios for different queue types</li> </ul>"},{"location":"ds_algo/data_structures/03_queues/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Ensure Python 3.8+ is installed (<code>python --version</code>) 2. No external packages required (uses Python's built-in data structures) 3. Create a working directory for your code 4. Save code examples as <code>.py</code> files 5. Run with <code>python filename.py</code>"},{"location":"ds_algo/data_structures/03_queues/#concept-overview","title":"Concept Overview","text":"<p>A queue is a linear data structure that follows the First-In-First-Out (FIFO) principle - the first element added is the first one removed.</p>"},{"location":"ds_algo/data_structures/03_queues/#queue-visualization","title":"Queue Visualization","text":"<pre><code>    \u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2510\n    \u2502Item1\u2502Item2\u2502Item3\u2502Item4\u2502    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\n      \u2191                 \u2191\n    Front             Rear\n   (Dequeue)        (Enqueue)\n</code></pre>"},{"location":"ds_algo/data_structures/03_queues/#types-of-queues","title":"Types of Queues","text":"<ol> <li>Standard Queue: Basic FIFO structure</li> <li>Circular Queue: Efficient use of fixed-size array by wrapping around</li> <li>Priority Queue: Elements have associated priorities and are dequeued in priority order</li> </ol>"},{"location":"ds_algo/data_structures/03_queues/#core-operations","title":"Core Operations","text":"<ul> <li>enqueue(item): Add an item to the rear of the queue</li> <li>dequeue(): Remove and return the front item from the queue</li> <li>peek() or front(): Return the front item without removing it</li> <li>isEmpty(): Check if the queue is empty</li> </ul>"},{"location":"ds_algo/data_structures/03_queues/#starter-code-with-gaps","title":"Starter Code with Gaps","text":"<p>Save this as <code>queue_implementations.py</code>:</p> <pre><code>\"\"\"\nQueue Implementations - Collaborative Learning Exercise\n\"\"\"\nfrom collections import deque\n\ndef main():\n    print(\"===== STANDARD QUEUE USING LIST =====\")\n    # Warning: Using list as a queue is not efficient due to O(n) time complexity for dequeue\n    queue = []\n\n    # TODO: Implement standard queue operations using list\n    # Enqueue: append(), Dequeue: pop(0)\n\n    print(\"\\n===== STANDARD QUEUE USING COLLECTIONS.DEQUE =====\")\n    # More efficient implementation using deque\n    queue = deque()\n\n    # TODO: Implement queue operations using deque\n    # Enqueue: append(), Dequeue: popleft()\n\n    print(\"\\n===== CUSTOM QUEUE IMPLEMENTATION =====\")\n    # Using our custom implementation\n    my_queue = Queue()\n\n    # TODO: Use custom queue methods\n\n    print(\"\\n===== CIRCULAR QUEUE =====\")\n    # Fixed-size circular queue\n    circular_queue = CircularQueue(5)  # Size 5\n\n    # TODO: Demonstrate circular queue operations\n\n    print(\"\\n===== PRIORITY QUEUE =====\")\n    # Queue where items have priorities\n    priority_queue = PriorityQueue()\n\n    # TODO: Demonstrate priority queue operations\n\n    print(\"\\n===== QUEUE APPLICATIONS =====\")\n    # TODO: Implement breadth-first traversal of a simple graph\n    print(\"Breadth-First Traversal:\")\n    graph = {\n        'A': ['B', 'C'],\n        'B': ['A', 'D', 'E'],\n        'C': ['A', 'F'],\n        'D': ['B'],\n        'E': ['B', 'F'],\n        'F': ['C', 'E']\n    }\n    breadth_first_traversal(graph, 'A')\n\n\nclass Queue:\n    \"\"\"A custom Queue implementation using a Python list\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize an empty queue\"\"\"\n        # TODO: Initialize the internal data structure\n\n    def enqueue(self, item):\n        \"\"\"Add an item to the rear of the queue\"\"\"\n        # TODO: Implement enqueue operation\n        pass\n\n    def dequeue(self):\n        \"\"\"Remove and return the front item from the queue\"\"\"\n        # TODO: Implement dequeue operation with error handling\n        pass\n\n    def peek(self):\n        \"\"\"Return the front item without removing it\"\"\"\n        # TODO: Implement peek operation with error handling\n        pass\n\n    def is_empty(self):\n        \"\"\"Check if the queue is empty\"\"\"\n        # TODO: Implement isEmpty check\n        pass\n\n    def size(self):\n        \"\"\"Return the number of items in the queue\"\"\"\n        # TODO: Return queue size\n        pass\n\n    def __str__(self):\n        \"\"\"Return a string representation of the queue\"\"\"\n        # TODO: Implement string representation\n        pass\n\n\nclass CircularQueue:\n    \"\"\"A circular queue implementation with fixed size\"\"\"\n\n    def __init__(self, capacity):\n        \"\"\"Initialize an empty circular queue with given capacity\"\"\"\n        # TODO: Initialize the circular queue with fixed capacity\n        # Hint: Use an array of given size, plus front and rear pointers\n        self.capacity = capacity\n        self.queue = [None] * capacity\n        self.front = self.rear = -1\n\n    def enqueue(self, item):\n        \"\"\"Add an item to the queue\"\"\"\n        # TODO: Implement circular enqueue with full queue detection\n        # Remember to handle the case when queue is empty (front = rear = -1)\n        # And the case when rear needs to wrap around to 0\n        pass\n\n    def dequeue(self):\n        \"\"\"Remove and return the front item from the queue\"\"\"\n        # TODO: Implement circular dequeue\n        # Remember to handle the case when queue becomes empty after dequeue\n        # And the case when front needs to wrap around to 0\n        pass\n\n    def peek(self):\n        \"\"\"Return the front item without removing it\"\"\"\n        # TODO: Implement peek operation with error handling\n        pass\n\n    def is_empty(self):\n        \"\"\"Check if the queue is empty\"\"\"\n        # TODO: Implement isEmpty check\n        pass\n\n    def is_full(self):\n        \"\"\"Check if the queue is full\"\"\"\n        # TODO: Implement isFull check\n        # Hint: Queue is full when (rear + 1) % capacity == front\n        pass\n\n    def size(self):\n        \"\"\"Return the number of items in the queue\"\"\"\n        # TODO: Calculate size in a circular queue\n        pass\n\n    def __str__(self):\n        \"\"\"Return a string representation of the circular queue\"\"\"\n        # TODO: Implement string representation\n        pass\n\n\nclass PriorityQueue:\n    \"\"\"A priority queue implementation using a list of (item, priority) tuples\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize an empty priority queue\"\"\"\n        # TODO: Initialize the internal data structure\n        # Hint: Use a list of (item, priority) tuples\n\n    def enqueue(self, item, priority):\n        \"\"\"Add an item with a priority (lower number = higher priority)\"\"\"\n        # TODO: Implement priority enqueue\n        # Hint: Either insert in order or use a simple list and sort on dequeue\n        pass\n\n    def dequeue(self):\n        \"\"\"Remove and return the highest priority item\"\"\"\n        # TODO: Implement priority dequeue\n        # Return the item with the highest priority (lowest priority number)\n        pass\n\n    def peek(self):\n        \"\"\"Return the highest priority item without removing it\"\"\"\n        # TODO: Implement priority peek\n        pass\n\n    def is_empty(self):\n        \"\"\"Check if the priority queue is empty\"\"\"\n        # TODO: Implement isEmpty check\n        pass\n\n    def size(self):\n        \"\"\"Return the number of items in the priority queue\"\"\"\n        # TODO: Return priority queue size\n        pass\n\n    def __str__(self):\n        \"\"\"Return a string representation of the priority queue\"\"\"\n        # TODO: Implement string representation\n        pass\n\n\ndef breadth_first_traversal(graph, start_node):\n    \"\"\"Traverse a graph in breadth-first order using a queue\"\"\"\n    # TODO: Implement BFS traversal using a queue\n    # Hint: Use a queue to keep track of nodes to visit\n    # and a set to keep track of visited nodes\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ds_algo/data_structures/03_queues/#live-coding-collaboration-tasks","title":"Live Coding Collaboration Tasks","text":""},{"location":"ds_algo/data_structures/03_queues/#task-1-standard-queue-implementation","title":"Task 1: Standard Queue Implementation","text":"<p>Working in pairs, implement the standard queue operations in the main function:</p> <ol> <li>First person implements list-based queue operations</li> <li>Second person implements deque-based queue operations</li> <li>Together, compare the efficiency and discuss the differences</li> </ol>"},{"location":"ds_algo/data_structures/03_queues/#task-2-complete-the-queue-classes","title":"Task 2: Complete the Queue Classes","text":"<p>In teams of 3 (or rotate roles in smaller teams):</p> <ol> <li>Person 1: Complete the basic <code>Queue</code> class</li> <li>Person 2: Complete the <code>CircularQueue</code> class</li> <li>Person 3: Complete the <code>PriorityQueue</code> class</li> <li>All review implementations together and test them</li> </ol>"},{"location":"ds_algo/data_structures/03_queues/#task-3-implement-bfs-algorithm","title":"Task 3: Implement BFS Algorithm","text":"<p>As a team, implement the <code>breadth_first_traversal</code> function using the queue data structure.</p>"},{"location":"ds_algo/data_structures/03_queues/#peer-discussion-questions","title":"Peer Discussion Questions","text":"<ol> <li> <p>Implementation Comparisons:</p> </li> <li> <p>What are the advantages of using deque over a list for a queue?</p> </li> <li>Compare the advantages and disadvantages of each queue implementation.</li> <li> <p>How does the circular queue optimize space usage?</p> </li> <li> <p>Use Case Analysis:</p> </li> <li> <p>When would you use a circular queue instead of a standard queue?</p> </li> <li>What real-world scenarios call for a priority queue?</li> <li> <p>How do queues enable breadth-first algorithms?</p> </li> <li> <p>Design Considerations:</p> </li> <li>How would you implement a bounded queue with a maximum size?</li> <li>What are the trade-offs of different priority queue implementations?</li> <li>How would you implement a double-ended queue (deque)?</li> </ol>"},{"location":"ds_algo/data_structures/03_queues/#time-and-space-complexity","title":"Time and Space Complexity","text":"Operation List-based Queue deque-based Queue Circular Queue Priority Queue Enqueue O(1) O(1) O(1) O(log n) or O(n)* Dequeue O(n) O(1) O(1) O(log n) or O(1)* Peek O(1) O(1) O(1) O(1) isEmpty O(1) O(1) O(1) O(1) Space O(n) O(n) O(n) O(n) <p>*Note: Complexity for Priority Queue depends on implementation (heap vs. sorted list)</p>"},{"location":"ds_algo/data_structures/03_queues/#common-bugs-and-debugging-tips","title":"Common Bugs and Debugging Tips","text":"<ol> <li> <p>Empty Queue Operations</p> </li> <li> <p>Error: Trying to dequeue or peek from an empty queue</p> </li> <li>Debug: Always check if the queue is empty before operations</li> </ol> <pre><code>if not queue.is_empty():\n    item = queue.dequeue()\nelse:\n    print(\"Cannot dequeue from empty queue\")\n</code></pre> <ol> <li> <p>Circular Queue Pointer Management</p> </li> <li> <p>Issue: Incorrect management of front and rear pointers</p> </li> <li>Debug: Carefully handle boundary conditions</li> </ol> <pre><code># When enqueueing to a circular queue\nself.rear = (self.rear + 1) % self.capacity\n\n# When dequeueing from a circular queue\nself.front = (self.front + 1) % self.capacity\n</code></pre> <ol> <li> <p>Queue Full Condition</p> </li> <li> <p>Issue: Unable to detect when circular queue is full</p> </li> <li>Debug: Implement is_full method correctly</li> </ol> <pre><code>def is_full(self):\n    return (self.rear + 1) % self.capacity == self.front\n</code></pre> <ol> <li>Priority Confusion</li> <li>Issue: Confusion about priority ordering (higher number = higher priority or vice versa)</li> <li>Debug: Clearly document and consistently implement priority ordering</li> </ol>"},{"location":"ds_algo/data_structures/03_queues/#group-challenge-implementing-a-service-queue-system","title":"Group Challenge: Implementing a Service Queue System","text":""},{"location":"ds_algo/data_structures/03_queues/#challenge-task","title":"Challenge Task","text":"<p>Build a simple customer service queue simulation with multiple priority levels:</p> <ol> <li>VIP customers (highest priority)</li> <li>Regular customers with appointments (medium priority)</li> <li>Walk-in customers (lowest priority)</li> </ol> <p>The system should:</p> <ul> <li>Allow adding customers of different types</li> <li>Process customers in priority order</li> <li>Handle situations when the queue reaches capacity</li> <li>Provide estimated wait times based on processing speed</li> <li>Allow emergency priority override</li> </ul>"},{"location":"ds_algo/data_structures/03_queues/#approach-instructions","title":"Approach Instructions","text":"<ol> <li> <p>Design Phase (10 minutes):</p> </li> <li> <p>Decide on the queue implementation to use</p> </li> <li>Define customer object structure</li> <li> <p>Plan the simulation workflow</p> </li> <li> <p>Implementation Phase (15 minutes):</p> </li> <li> <p>Person 1: Implement the customer class and priority queue</p> </li> <li>Person 2: Build the queue processing system</li> <li> <p>Person 3: Create simulation controls and statistics tracking</p> </li> <li> <p>Testing Phase (5 minutes):</p> </li> <li>Run simulations with different customer arrival patterns</li> <li>Test edge cases (empty queue, full queue, priority ties)</li> <li>Compare metrics like average wait time by priority level</li> </ol>"},{"location":"ds_algo/data_structures/03_queues/#starter-code-for-service-queue","title":"Starter Code for Service Queue","text":"<pre><code>class Customer:\n    def __init__(self, name, customer_type, arrival_time):\n        \"\"\"Initialize a customer with type determining priority\"\"\"\n        # TODO: Implement customer initialization\n        # customer_type can be \"vip\", \"appointment\", or \"walkin\"\n        pass\n\nclass ServiceQueue:\n    def __init__(self, capacity=50, processing_time=5):\n        \"\"\"Initialize service queue with capacity and avg processing time\"\"\"\n        # TODO: Implement service queue using a priority queue\n        pass\n\n    def add_customer(self, customer):\n        \"\"\"Add a customer to the queue based on their priority\"\"\"\n        # TODO: Implement customer addition with priority based on type\n        pass\n\n    def process_next_customer(self):\n        \"\"\"Process the next highest priority customer\"\"\"\n        # TODO: Implement customer processing\n        pass\n\n    def get_wait_time_estimate(self, customer_type):\n        \"\"\"Estimate wait time for a new customer of given type\"\"\"\n        # TODO: Implement wait time estimation\n        pass\n\ndef run_simulation(arrival_rate, service_time, simulation_duration):\n    \"\"\"Run a simulation of the customer service queue\"\"\"\n    # TODO: Implement queue simulation\n    pass\n</code></pre>"},{"location":"ds_algo/data_structures/03_queues/#reflection-questions","title":"Reflection Questions","text":"<p>After completing the exercises:</p> <ol> <li>How does the choice of queue implementation affect the performance of the BFS algorithm?</li> <li>What were the most challenging aspects of implementing the circular queue?</li> <li>How would your priority queue implementation scale with very large numbers of items?</li> <li>In what scenarios might you need to combine different queue types or implement a custom hybrid queue?</li> </ol>"},{"location":"ds_algo/data_structures/04_linked_lists/","title":"Linked Lists: Node-Based Sequential Data Structures","text":""},{"location":"ds_algo/data_structures/04_linked_lists/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the structure and behavior of singly and doubly linked lists</li> <li>Implement node creation, traversal, insertion, and deletion operations</li> <li>Compare linked lists with array-based structures for different use cases</li> <li>Analyze time and space complexity of linked list operations</li> <li>Solve common problems using linked list techniques</li> </ul>"},{"location":"ds_algo/data_structures/04_linked_lists/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Ensure Python 3.8+ is installed (<code>python --version</code>) 2. No external packages required (uses Python's built-in data structures) 3. Create a working directory for your code 4. Save code examples as <code>.py</code> files 5. Run with <code>python filename.py</code>"},{"location":"ds_algo/data_structures/04_linked_lists/#concept-overview","title":"Concept Overview","text":"<p>A linked list is a linear data structure where elements are stored in nodes, each containing data and a reference (pointer) to the next node.</p>"},{"location":"ds_algo/data_structures/04_linked_lists/#types-of-linked-lists","title":"Types of Linked Lists","text":"<ol> <li>Singly Linked List: Each node points to the next node</li> <li>Doubly Linked List: Each node points to both next and previous nodes</li> <li>Circular Linked List: Last node points back to the first node</li> </ol>"},{"location":"ds_algo/data_structures/04_linked_lists/#linked-list-visualization","title":"Linked List Visualization","text":"<p>Singly Linked List:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Data\u2502 Next\u2502\u2500\u2500\u2500\u25ba\u2502 Data\u2502 Next\u2502\u2500\u2500\u2500\u25ba\u2502 Data\u2502 Next\u2502\u2500\u2500\u2500\u25ba\u2502 Data\u2502 NULL\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n   Head                                               Tail\n</code></pre> <p>Doubly Linked List:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502NULL \u2502 Data\u2502 Next\u2502\u25c4\u2500\u2500\u25ba\u2502 Prev\u2502 Data\u2502 Next\u2502\u25c4\u2500\u2500\u25ba\u2502 Prev\u2502 Data\u2502 NULL\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n   Head                                           Tail\n</code></pre>"},{"location":"ds_algo/data_structures/04_linked_lists/#starter-code-with-gaps","title":"Starter Code with Gaps","text":"<p>Save this as <code>linked_list_implementations.py</code>:</p> <pre><code>\"\"\"\nLinked List Implementations - Collaborative Learning Exercise\n\"\"\"\n\ndef main():\n    print(\"===== SINGLY LINKED LIST =====\")\n    # Create a singly linked list\n    sll = SinglyLinkedList()\n\n    # TODO: Add elements to the singly linked list\n\n    # TODO: Demonstrate traversal, insertion, and deletion\n\n    print(\"\\n===== DOUBLY LINKED LIST =====\")\n    # Create a doubly linked list\n    dll = DoublyLinkedList()\n\n    # TODO: Add elements to the doubly linked list\n\n    # TODO: Demonstrate traversal (forward and backward), insertion, and deletion\n\n    print(\"\\n===== LINKED LIST APPLICATIONS =====\")\n    # TODO: Implement and test linked list applications\n\n\nclass Node:\n    \"\"\"A basic node for singly linked list\"\"\"\n\n    def __init__(self, data):\n        \"\"\"Initialize a node with data and null next pointer\"\"\"\n        self.data = data\n        self.next = None\n\n\nclass DoublyNode:\n    \"\"\"A node for doubly linked list\"\"\"\n\n    def __init__(self, data):\n        \"\"\"Initialize a node with data, null next, and null prev pointers\"\"\"\n        self.data = data\n        self.next = None\n        self.prev = None\n\n\nclass SinglyLinkedList:\n    \"\"\"A singly linked list implementation\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize an empty linked list\"\"\"\n        self.head = None\n        self.tail = None\n        self.size = 0\n\n    def is_empty(self):\n        \"\"\"Check if the list is empty\"\"\"\n        # TODO: Implement is_empty check\n        pass\n\n    def append(self, data):\n        \"\"\"Add a node with given data to the end of the list\"\"\"\n        # TODO: Implement append operation\n        # Remember to update both head and tail pointers\n        pass\n\n    def prepend(self, data):\n        \"\"\"Add a node with given data to the beginning of the list\"\"\"\n        # TODO: Implement prepend operation\n        pass\n\n    def insert_after(self, target_data, data):\n        \"\"\"Insert a new node with data after the first node containing target_data\"\"\"\n        # TODO: Implement insert_after operation\n        pass\n\n    def delete(self, data):\n        \"\"\"Delete the first node containing the given data\"\"\"\n        # TODO: Implement delete operation\n        # Remember to handle special cases (empty list, deleting head, etc.)\n        pass\n\n    def find(self, data):\n        \"\"\"Find and return the first node containing the given data\"\"\"\n        # TODO: Implement find operation\n        pass\n\n    def display(self):\n        \"\"\"Display all elements in the list\"\"\"\n        # TODO: Implement display operation\n        pass\n\n    def get_size(self):\n        \"\"\"Get the number of nodes in the list\"\"\"\n        # TODO: Implement get_size (could use a counter or stored size)\n        pass\n\n    def reverse(self):\n        \"\"\"Reverse the linked list in-place\"\"\"\n        # TODO: Implement reversal operation\n        # This is a common interview question!\n        pass\n\n\nclass DoublyLinkedList:\n    \"\"\"A doubly linked list implementation\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize an empty doubly linked list\"\"\"\n        self.head = None\n        self.tail = None\n        self.size = 0\n\n    def is_empty(self):\n        \"\"\"Check if the list is empty\"\"\"\n        # TODO: Implement is_empty check\n        pass\n\n    def append(self, data):\n        \"\"\"Add a node with given data to the end of the list\"\"\"\n        # TODO: Implement append operation\n        # Make sure to update prev pointers too!\n        pass\n\n    def prepend(self, data):\n        \"\"\"Add a node with given data to the beginning of the list\"\"\"\n        # TODO: Implement prepend operation\n        pass\n\n    def insert_after(self, target_data, data):\n        \"\"\"Insert a new node with data after the first node containing target_data\"\"\"\n        # TODO: Implement insert_after operation\n        pass\n\n    def delete(self, data):\n        \"\"\"Delete the first node containing the given data\"\"\"\n        # TODO: Implement delete operation\n        # Make sure to update prev pointers too!\n        pass\n\n    def display_forward(self):\n        \"\"\"Display all elements in forward direction\"\"\"\n        # TODO: Implement forward display\n        pass\n\n    def display_backward(self):\n        \"\"\"Display all elements in backward direction (using prev pointers)\"\"\"\n        # TODO: Implement backward display (starting from tail)\n        pass\n\n    def get_size(self):\n        \"\"\"Get the number of nodes in the list\"\"\"\n        # TODO: Implement get_size\n        pass\n\n\n# Examples of common linked list problems\ndef detect_cycle(linked_list):\n    \"\"\"Detect if a linked list has a cycle using Floyd's cycle-finding algorithm\"\"\"\n    # TODO: Implement cycle detection using slow and fast pointers\n    # Return True if cycle exists, False otherwise\n    pass\n\n\ndef find_middle(linked_list):\n    \"\"\"Find the middle node of a linked list using the slow and fast pointer technique\"\"\"\n    # TODO: Implement middle node finding\n    # Return the middle node (or the second middle node if even length)\n    pass\n\n\ndef merge_sorted_lists(list1, list2):\n    \"\"\"Merge two sorted linked lists into a single sorted linked list\"\"\"\n    # TODO: Implement sorted list merging\n    # Return the merged list\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ds_algo/data_structures/04_linked_lists/#live-coding-collaboration-tasks","title":"Live Coding Collaboration Tasks","text":""},{"location":"ds_algo/data_structures/04_linked_lists/#task-1-singly-linked-list-implementation","title":"Task 1: Singly Linked List Implementation","text":"<p>Working in pairs, implement the core singly linked list operations:</p> <ol> <li>First person implements <code>is_empty</code>, <code>append</code>, and <code>prepend</code></li> <li>Second person implements <code>insert_after</code>, <code>delete</code>, and <code>find</code></li> <li>Together implement <code>display</code> and <code>get_size</code></li> <li>Discuss and implement <code>reverse</code> collaboratively</li> </ol>"},{"location":"ds_algo/data_structures/04_linked_lists/#task-2-doubly-linked-list-implementation","title":"Task 2: Doubly Linked List Implementation","text":"<p>Working in pairs or small teams:</p> <ol> <li>First person implements the doubly linked list node operations (<code>append</code>, <code>prepend</code>)</li> <li>Second person implements traversal operations (<code>display_forward</code>, <code>display_backward</code>)</li> <li>Third person (or back to first) implements insertion and deletion operations</li> </ol>"},{"location":"ds_algo/data_structures/04_linked_lists/#task-3-solving-linked-list-problems","title":"Task 3: Solving Linked List Problems","text":"<p>As a team, implement the three common linked list algorithms:</p> <ol> <li>Cycle detection</li> <li>Finding the middle node</li> <li>Merging sorted lists</li> </ol>"},{"location":"ds_algo/data_structures/04_linked_lists/#peer-discussion-questions","title":"Peer Discussion Questions","text":"<ol> <li> <p>Implementation Considerations:</p> </li> <li> <p>What are the trade-offs between singly and doubly linked lists?</p> </li> <li>When would you choose a linked list over an array or vice versa?</li> <li> <p>How does maintaining a tail pointer affect the complexity of operations?</p> </li> <li> <p>Algorithmic Thinking:</p> </li> <li> <p>How does the slow/fast pointer technique work in cycle detection?</p> </li> <li>What are the edge cases when implementing linked list operations?</li> <li> <p>How would you implement a linked list from scratch if pointers were not available?</p> </li> <li> <p>Real-world Applications:</p> </li> <li>Where are linked lists used in real systems?</li> <li>How might linked lists be used in an undo/redo feature?</li> <li>What types of applications benefit most from linked list structures?</li> </ol>"},{"location":"ds_algo/data_structures/04_linked_lists/#time-and-space-complexity","title":"Time and Space Complexity","text":"Operation Singly Linked List Doubly Linked List Array (for comparison) Access by index O(n) O(n) O(1) Insert at front O(1) O(1) O(n) Insert at end O(1)* O(1) O(1)* or O(n) Insert in middle O(n) O(n) O(n) Delete from front O(1) O(1) O(n) Delete from end O(n) or O(1)* O(1) O(1)* or O(n) Delete in middle O(n) O(n) O(n) Search O(n) O(n) O(n) Space per element O(1) + extra ptr O(1) + 2 extra ptrs O(1) <p>*With tail pointer for linked lists or dynamic arrays</p>"},{"location":"ds_algo/data_structures/04_linked_lists/#common-bugs-and-debugging-tips","title":"Common Bugs and Debugging Tips","text":"<ol> <li> <p>Null Pointer Exceptions</p> </li> <li> <p>Error: Trying to access fields of a null node</p> </li> <li>Debug: Always check if a node is null before accessing its fields</li> </ol> <pre><code>if current is not None:\n    current = current.next\n</code></pre> <ol> <li> <p>Lost Nodes</p> </li> <li> <p>Issue: Nodes becoming unreachable due to improper pointer updates</p> </li> <li>Debug: Be careful when updating pointers, especially in delete operations</li> </ol> <pre><code># Save references before updating pointers\ntemp = current.next\ncurrent.next = current.next.next\n# Now temp can be safely deleted or recycled\n</code></pre> <ol> <li> <p>Infinite Loops in Cycles</p> </li> <li> <p>Issue: Getting stuck in infinite loops when traversing a circular linked list</p> </li> <li>Debug: Use the slow/fast pointer technique or keep track of visited nodes</li> </ol> <pre><code>visited = set()\nwhile current:\n    if current in visited:\n        print(\"Cycle detected\")\n        break\n    visited.add(current)\n    current = current.next\n</code></pre> <ol> <li>Boundary Conditions</li> <li>Issue: Not handling edge cases like empty lists or single-element lists</li> <li>Debug: Always test your code with empty lists, single-element lists, and boundary cases</li> </ol>"},{"location":"ds_algo/data_structures/04_linked_lists/#group-challenge-implementing-a-playlist-manager","title":"Group Challenge: Implementing a Playlist Manager","text":""},{"location":"ds_algo/data_structures/04_linked_lists/#challenge-task","title":"Challenge Task","text":"<p>Build a music playlist manager using linked lists that supports:</p> <ol> <li>Adding songs to the front, end, or after a specific song</li> <li>Removing songs by name</li> <li>Moving songs up or down in the playlist</li> <li>Creating a \"shuffle\" feature that randomizes the order</li> <li>Implementing \"repeat one\", \"repeat all\", and \"no repeat\" modes using circular and non-circular linked lists</li> </ol>"},{"location":"ds_algo/data_structures/04_linked_lists/#approach-instructions","title":"Approach Instructions","text":"<ol> <li> <p>Design Phase (10 minutes):</p> </li> <li> <p>Decide on data structure (singly vs. doubly linked list)</p> </li> <li>Define node structure (what information to store about each song)</li> <li> <p>Plan the user interface for playlist operations</p> </li> <li> <p>Implementation Phase (15 minutes):</p> </li> <li> <p>Person 1: Implement basic playlist operations (add, remove, display)</p> </li> <li>Person 2: Implement movement operations (move up, move down)</li> <li> <p>Person 3: Implement special features (shuffle, repeat modes)</p> </li> <li> <p>Testing Phase (5 minutes):</p> </li> <li>Create test playlists with sample songs</li> <li>Test all operations individually and in combination</li> <li>Verify edge cases (empty playlist, single song, etc.)</li> </ol>"},{"location":"ds_algo/data_structures/04_linked_lists/#starter-code-for-playlist-manager","title":"Starter Code for Playlist Manager","text":"<pre><code>class Song:\n    def __init__(self, title, artist, duration):\n        self.title = title\n        self.artist = artist\n        self.duration = duration  # in seconds\n\n    def __str__(self):\n        minutes = self.duration // 60\n        seconds = self.duration % 60\n        return f\"{self.title} by {self.artist} ({minutes}:{seconds:02d})\"\n\n\nclass Playlist:\n    def __init__(self, name):\n        self.name = name\n        # TODO: Initialize the appropriate linked list structure\n        # Hint: Consider what operations will be most efficient\n\n    def add_song(self, song, position=\"end\"):\n        \"\"\"Add a song to the playlist at the specified position (start, end, or after a song)\"\"\"\n        # TODO: Implement add_song functionality\n        pass\n\n    def remove_song(self, title):\n        \"\"\"Remove a song from the playlist by title\"\"\"\n        # TODO: Implement remove_song functionality\n        pass\n\n    def move_up(self, title):\n        \"\"\"Move a song up one position in the playlist\"\"\"\n        # TODO: Implement move_up functionality\n        pass\n\n    def move_down(self, title):\n        \"\"\"Move a song down one position in the playlist\"\"\"\n        # TODO: Implement move_down functionality\n        pass\n\n    def shuffle(self):\n        \"\"\"Randomly reorder the songs in the playlist\"\"\"\n        # TODO: Implement shuffle functionality\n        # Hint: Consider converting to a list, shuffling, then rebuilding the linked list\n        pass\n\n    def set_repeat_mode(self, mode):\n        \"\"\"Set repeat mode (none, one, all)\"\"\"\n        # TODO: Implement repeat mode setting\n        # Hint: \"all\" could use a circular linked list\n        pass\n\n    def display(self):\n        \"\"\"Display all songs in the playlist\"\"\"\n        # TODO: Implement display functionality\n        pass\n</code></pre>"},{"location":"ds_algo/data_structures/04_linked_lists/#reflection-questions","title":"Reflection Questions","text":"<p>After completing the exercises:</p> <ol> <li>How did the choice between singly and doubly linked lists affect your implementation?</li> <li>What were the most challenging operations to implement and why?</li> <li>How would you improve your implementation for better performance or more features?</li> <li>In what real-world scenarios would linked lists be preferable to arrays or other data structures?</li> </ol>"},{"location":"ds_algo/data_structures/05_hash_tables/","title":"Hash Tables: Efficient Key-Value Data Structures","text":""},{"location":"ds_algo/data_structures/05_hash_tables/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the principles behind hash tables and hash functions</li> <li>Implement hash tables with different collision resolution strategies</li> <li>Analyze the time and space complexity of hash table operations</li> <li>Explore real-world applications of hash tables</li> <li>Compare hash table implementations with other data structures</li> </ul>"},{"location":"ds_algo/data_structures/05_hash_tables/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Ensure Python 3.8+ is installed (<code>python --version</code>) 2. No external packages required (uses Python's built-in data structures) 3. Create a working directory for your code 4. Save code examples as <code>.py</code> files 5. Run with <code>python filename.py</code>"},{"location":"ds_algo/data_structures/05_hash_tables/#concept-overview","title":"Concept Overview","text":"<p>A hash table (hash map) is a data structure that implements an associative array, which maps keys to values using a hash function.</p>"},{"location":"ds_algo/data_structures/05_hash_tables/#hash-table-visualization","title":"Hash Table Visualization","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Hash Function: h(key) \u2192 index     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502\n            \u25bc\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510\n\u2502 0 \u2502 1 \u2502 2 \u2502 3 \u2502 4 \u2502 5 \u2502 6 \u2502 7 \u2502...\u2502  \u2190 Buckets/Slots\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524\n\u2502   \u2502 \u25bc \u2502   \u2502 \u25bc \u2502   \u2502 \u25bc \u2502   \u2502   \u2502   \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\n      \u2502       \u2502       \u2502\n      \u25bc       \u25bc       \u25bc\n    (k,v)   (k,v)\u2192(k,v)  \u2190 Entries (key-value pairs)\n             Collision handling\n             (e.g., chaining)\n</code></pre>"},{"location":"ds_algo/data_structures/05_hash_tables/#key-concepts","title":"Key Concepts","text":"<ol> <li>Hash Function: Converts keys into array indices</li> <li>Collision Resolution: Handling when different keys hash to the same index</li> <li>Chaining: Store multiple key-value pairs in the same slot using a linked list</li> <li>Open Addressing: Find another empty slot (linear probing, quadratic probing, double hashing)</li> <li>Load Factor: Ratio of filled slots to total slots (affects performance)</li> <li>Rehashing: Increasing table size and redistributing entries when load factor gets too high</li> </ol>"},{"location":"ds_algo/data_structures/05_hash_tables/#starter-code-with-gaps","title":"Starter Code with Gaps","text":"<p>Save this as <code>hash_table_implementations.py</code>:</p> <pre><code>\"\"\"\nHash Table Implementations - Collaborative Learning Exercise\n\"\"\"\nimport hashlib\n\ndef main():\n    print(\"===== PYTHON DICTIONARY =====\")\n    # Python's built-in dict is a hash table implementation\n    py_dict = {}\n\n    # TODO: Demonstrate dictionary operations\n\n    print(\"\\n===== CHAINING HASH TABLE =====\")\n    # Hash table using separate chaining for collision resolution\n    chaining_table = HashTableChaining(size=10)\n\n    # TODO: Demonstrate chaining hash table operations\n\n    print(\"\\n===== OPEN ADDRESSING HASH TABLE =====\")\n    # Hash table using linear probing for collision resolution\n    open_addr_table = HashTableOpenAddressing(size=10)\n\n    # TODO: Demonstrate open addressing hash table operations\n\n    print(\"\\n===== HASH TABLE APPLICATIONS =====\")\n    # TODO: Implement example applications of hash tables\n\n\nclass HashTableChaining:\n    \"\"\"\n    Hash table implementation using separate chaining for collision resolution.\n    Each slot contains a linked list of key-value pairs that hash to that slot.\n    \"\"\"\n\n    class Node:\n        \"\"\"A node in a linked list for chaining\"\"\"\n        def __init__(self, key, value):\n            self.key = key\n            self.value = value\n            self.next = None\n\n    def __init__(self, size=10):\n        \"\"\"Initialize a hash table with given size\"\"\"\n        self.size = size\n        self.buckets = [None] * size\n        self.count = 0\n\n    def _hash(self, key):\n        \"\"\"Hash function to convert key to index\"\"\"\n        # TODO: Implement a hash function\n        # Hint: Use built-in hash() or hashlib for strings, or implement your own\n        # Make sure to handle different key types\n        pass\n\n    def put(self, key, value):\n        \"\"\"Insert or update a key-value pair\"\"\"\n        # TODO: Implement insertion with chaining\n        # If key exists, update its value\n        # If key doesn't exist, add a new node to the chain\n        pass\n\n    def get(self, key):\n        \"\"\"Get value for a key\"\"\"\n        # TODO: Implement retrieval\n        # Return the value if key exists, otherwise return None\n        pass\n\n    def remove(self, key):\n        \"\"\"Remove a key-value pair\"\"\"\n        # TODO: Implement removal\n        # Return True if removed, False if key not found\n        pass\n\n    def contains(self, key):\n        \"\"\"Check if key exists\"\"\"\n        # TODO: Implement contains check\n        pass\n\n    def keys(self):\n        \"\"\"Return all keys in the hash table\"\"\"\n        # TODO: Implement keys collection\n        pass\n\n    def values(self):\n        \"\"\"Return all values in the hash table\"\"\"\n        # TODO: Implement values collection\n        pass\n\n    def size(self):\n        \"\"\"Return number of key-value pairs\"\"\"\n        # TODO: Return size\n        pass\n\n    def load_factor(self):\n        \"\"\"Calculate and return the load factor\"\"\"\n        # TODO: Implement load factor calculation\n        pass\n\n    def _resize(self, new_size):\n        \"\"\"Resize the hash table\"\"\"\n        # TODO: Implement resizing\n        # Create a new larger table and rehash all existing entries\n        pass\n\n    def __str__(self):\n        \"\"\"String representation of the hash table\"\"\"\n        # TODO: Implement string representation\n        pass\n\n\nclass HashTableOpenAddressing:\n    \"\"\"\n    Hash table implementation using open addressing with linear probing\n    for collision resolution.\n    \"\"\"\n\n    # Special marker for deleted slots\n    _DELETED = object()\n\n    def __init__(self, size=10):\n        \"\"\"Initialize a hash table with given size\"\"\"\n        self.size = size\n        self.keys = [None] * size\n        self.values = [None] * size\n        self.count = 0\n\n    def _hash(self, key):\n        \"\"\"Hash function to convert key to index\"\"\"\n        # TODO: Implement a hash function\n        pass\n\n    def _find_slot(self, key):\n        \"\"\"Find the slot for a key using linear probing\"\"\"\n        # TODO: Implement slot finding\n        # Return the index where key is found or should be inserted\n        # Handle collisions with linear probing\n        pass\n\n    def put(self, key, value):\n        \"\"\"Insert or update a key-value pair\"\"\"\n        # TODO: Implement insertion with linear probing\n        # If load factor is too high, resize before insertion\n        pass\n\n    def get(self, key):\n        \"\"\"Get value for a key\"\"\"\n        # TODO: Implement retrieval with linear probing\n        pass\n\n    def remove(self, key):\n        \"\"\"Remove a key-value pair\"\"\"\n        # TODO: Implement removal\n        # Use a special marker to indicate deleted slots\n        pass\n\n    def contains(self, key):\n        \"\"\"Check if key exists\"\"\"\n        # TODO: Implement contains check\n        pass\n\n    def keys(self):\n        \"\"\"Return all keys in the hash table\"\"\"\n        # TODO: Implement keys collection\n        pass\n\n    def values(self):\n        \"\"\"Return all values in the hash table\"\"\"\n        # TODO: Implement values collection\n        pass\n\n    def size(self):\n        \"\"\"Return number of key-value pairs\"\"\"\n        # TODO: Return size\n        pass\n\n    def load_factor(self):\n        \"\"\"Calculate and return the load factor\"\"\"\n        # TODO: Implement load factor calculation\n        pass\n\n    def _resize(self, new_size):\n        \"\"\"Resize the hash table\"\"\"\n        # TODO: Implement resizing\n        pass\n\n    def __str__(self):\n        \"\"\"String representation of the hash table\"\"\"\n        # TODO: Implement string representation\n        pass\n\n\ndef count_word_frequency(text):\n    \"\"\"Count frequency of each word in text using a hash table\"\"\"\n    # TODO: Implement word frequency counter\n    # Hint: Split text into words, use a hash table to count occurrences\n    pass\n\n\ndef check_anagrams(word1, word2):\n    \"\"\"Check if two words are anagrams using a hash table\"\"\"\n    # TODO: Implement anagram checker\n    # Hint: Use a hash table to count character frequencies\n    pass\n\n\ndef first_non_repeating_char(text):\n    \"\"\"Find the first non-repeating character in a string using a hash table\"\"\"\n    # TODO: Implement first non-repeating character finder\n    # Hint: First count all characters, then find first with count 1\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ds_algo/data_structures/05_hash_tables/#live-coding-collaboration-tasks","title":"Live Coding Collaboration Tasks","text":""},{"location":"ds_algo/data_structures/05_hash_tables/#task-1-python-dictionary-operations","title":"Task 1: Python Dictionary Operations","text":"<p>Working in pairs, implement the dictionary operations in the main function:</p> <ol> <li>First person implements basic operations (put, get, remove)</li> <li>Second person implements advanced operations (iteration, dictionary comprehensions)</li> <li>Compare Python's built-in dictionary performance characteristics</li> </ol>"},{"location":"ds_algo/data_structures/05_hash_tables/#task-2-implement-hash-table-with-chaining","title":"Task 2: Implement Hash Table with Chaining","text":"<p>In teams of 2-3:</p> <ol> <li>First coder implements <code>_hash</code> method and <code>put</code> operation</li> <li>Second coder implements <code>get</code> and <code>contains</code> operations</li> <li>Third coder (or back to first) implements <code>remove</code> and collection methods</li> <li>All implement resizing together</li> <li>Test with various input types and edge cases</li> </ol>"},{"location":"ds_algo/data_structures/05_hash_tables/#task-3-implement-hash-table-with-open-addressing","title":"Task 3: Implement Hash Table with Open Addressing","text":"<p>In teams of 2-3:</p> <ol> <li>First coder implements <code>_hash</code>, <code>_find_slot</code>, and <code>put</code> methods</li> <li>Second coder implements <code>get</code> and <code>contains</code> operations</li> <li>Third coder (or back to first) implements <code>remove</code> and resizing</li> <li>Test with various load factors and observe performance</li> </ol>"},{"location":"ds_algo/data_structures/05_hash_tables/#task-4-implement-hash-table-applications","title":"Task 4: Implement Hash Table Applications","text":"<p>Divide the utility functions among team members and implement them using hash tables.</p>"},{"location":"ds_algo/data_structures/05_hash_tables/#peer-discussion-questions","title":"Peer Discussion Questions","text":"<ol> <li> <p>Hash Function Design:</p> </li> <li> <p>What makes a good hash function? What properties should it have?</p> </li> <li>How do hash collisions affect performance?</li> <li> <p>How can we create hash functions for custom objects?</p> </li> <li> <p>Collision Resolution:</p> </li> <li> <p>Compare the advantages and disadvantages of chaining vs. open addressing.</p> </li> <li>How does the load factor affect each collision resolution strategy?</li> <li> <p>When would you choose one strategy over another?</p> </li> <li> <p>Performance Analysis:</p> </li> <li>What is the average and worst-case time complexity for hash table operations?</li> <li>How does the load factor affect performance?</li> <li>What trade-offs exist between memory usage and performance?</li> </ol>"},{"location":"ds_algo/data_structures/05_hash_tables/#time-and-space-complexity","title":"Time and Space Complexity","text":"Operation Average Case Worst Case (without resize) Amortized (with resize) Insert O(1) O(n) O(1) Lookup O(1) O(n) O(1) Delete O(1) O(n) O(1) Space O(n) O(n) O(n) <p>*Worst case occurs when all keys hash to the same bucket or when there are many collisions</p>"},{"location":"ds_algo/data_structures/05_hash_tables/#common-bugs-and-debugging-tips","title":"Common Bugs and Debugging Tips","text":"<ol> <li> <p>Poor Hash Distribution</p> </li> <li> <p>Issue: Too many collisions due to a poorly designed hash function</p> </li> <li> <p>Debug:</p> <ul> <li>Test your hash function with various inputs to ensure even distribution</li> <li>Visualize the distribution of keys across buckets  <pre><code>def analyze_distribution(hash_table):\n    distribution = [0] * hash_table.size\n    for i in range(hash_table.size):\n        bucket = hash_table.buckets[i]\n        count = 0\n        while bucket:\n            count += 1\n            bucket = bucket.next\n        distribution[i] = count\n    return distribution\n</code></pre></li> </ul> </li> <li> <p>Hash Collisions</p> </li> <li> <p>Issue: Same hash value for different keys</p> </li> <li>Debug: Ensure your collision resolution strategy works correctly</li> </ol> <pre><code># Test with keys that you know will collide\nkey1 = \"abc\"\nkey2 = \"cba\"\n# Force collision by using a simple hash function like sum of character codes\n</code></pre> <ol> <li> <p>Incorrect Removal in Open Addressing</p> </li> <li> <p>Issue: Simply setting a slot to None breaks the search algorithm</p> </li> <li>Debug: Use a special marker for deleted slots rather than None</li> </ol> <pre><code># Check if proper tombstone marking is used\ndef test_delete_and_find(table):\n    table.put(\"key1\", \"value1\")\n    table.put(\"key2\", \"value2\")\n    table.remove(\"key1\")  # Should mark as deleted, not None\n    assert table.get(\"key2\") == \"value2\"  # Should still find key2\n</code></pre> <ol> <li>Forgetting to Resize</li> <li>Issue: Performance degradation as table fills up</li> <li>Debug: Monitor load factor and resize when it exceeds a threshold    <pre><code>def put(self, key, value):\n    if self.load_factor() &gt; 0.7:  # Threshold commonly used\n        self._resize(self.size * 2)\n    # Rest of put implementation\n</code></pre></li> </ol>"},{"location":"ds_algo/data_structures/05_hash_tables/#group-challenge-building-a-document-indexer","title":"Group Challenge: Building a Document Indexer","text":""},{"location":"ds_algo/data_structures/05_hash_tables/#challenge-task","title":"Challenge Task","text":"<p>Implement a simple document indexing and search system using hash tables that:</p> <ol> <li>Indexes multiple text documents by creating an inverted index</li> <li>Allows searching for documents containing specific words</li> <li>Ranks documents by relevance (frequency of search terms)</li> <li>Supports basic boolean operators (AND, OR) in search queries</li> <li>Implements a simple word stemming algorithm (e.g., removing common suffixes)</li> </ol>"},{"location":"ds_algo/data_structures/05_hash_tables/#approach-instructions","title":"Approach Instructions","text":"<ol> <li> <p>Design Phase (10 minutes):</p> </li> <li> <p>Plan the structure of the inverted index</p> </li> <li>Define the document and term storage approach</li> <li> <p>Outline search algorithms for different query types</p> </li> <li> <p>Implementation Phase (15 minutes):</p> </li> <li> <p>Person 1: Implement document parsing and word extraction</p> </li> <li>Person 2: Build the inverted index using hash tables</li> <li> <p>Person 3: Implement search functionality with ranking</p> </li> <li> <p>Testing Phase (5 minutes):</p> </li> <li>Create a small corpus of test documents</li> <li>Try different search queries (single word, multiple words, boolean)</li> <li>Evaluate whether document ranking makes sense</li> </ol>"},{"location":"ds_algo/data_structures/05_hash_tables/#starter-code-for-document-indexer","title":"Starter Code for Document Indexer","text":"<pre><code>class DocumentIndexer:\n    def __init__(self):\n        \"\"\"Initialize the indexer with an empty inverted index\"\"\"\n        # TODO: Initialize data structures\n        # Inverted index: word -&gt; {doc_id -&gt; [positions]}\n        self.inverted_index = {}\n        self.documents = {}  # doc_id -&gt; original text\n\n    def add_document(self, doc_id, text):\n        \"\"\"Add a document to the index\"\"\"\n        # TODO: Implement document addition\n        # 1. Store the original document\n        # 2. Tokenize the text into words\n        # 3. Update the inverted index with word positions\n        pass\n\n    def search(self, query):\n        \"\"\"Search for documents matching the query\"\"\"\n        # TODO: Implement search functionality\n        # Parse query, find matching documents, and rank them\n        pass\n\n    def search_boolean(self, query):\n        \"\"\"Support AND/OR queries like 'word1 AND word2 OR word3'\"\"\"\n        # TODO: Implement boolean search\n        pass\n\n    def _tokenize(self, text):\n        \"\"\"Convert text to tokens (words)\"\"\"\n        # TODO: Implement tokenization\n        # Remove punctuation, convert to lowercase, split into words\n        pass\n\n    def _stem(self, word):\n        \"\"\"Perform simple word stemming\"\"\"\n        # TODO: Implement basic stemming\n        # Remove common suffixes like -ing, -ed, -s\n        pass\n\n    def _rank_documents(self, matching_docs, query_terms):\n        \"\"\"Rank matching documents by relevance\"\"\"\n        # TODO: Implement document ranking\n        # Consider term frequency, document frequency, positions\n        pass\n\n\n# Utility functions for testing the index\ndef create_test_corpus():\n    \"\"\"Create a small test corpus of documents\"\"\"\n    # TODO: Create 5-10 short documents about different topics\n    pass\n\ndef test_indexer(indexer):\n    \"\"\"Test the indexer with various queries\"\"\"\n    # TODO: Try different types of searches and print results\n    pass\n</code></pre>"},{"location":"ds_algo/data_structures/05_hash_tables/#reflection-questions","title":"Reflection Questions","text":"<p>After completing the exercises:</p> <ol> <li>How do the performance characteristics of your hash table implementations compare to Python's built-in dictionaries?</li> <li>What were the most challenging aspects of implementing collision resolution strategies?</li> <li>How would you scale your hash table implementation to handle millions of entries efficiently?</li> <li>In what real-world applications have you encountered hash tables, and how might your implementation be adapted for those scenarios?</li> </ol>"},{"location":"ds_algo/data_structures/06_trees/","title":"Trees: Hierarchical Data Structures","text":""},{"location":"ds_algo/data_structures/06_trees/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the structure and terminology of tree data structures</li> <li>Implement binary trees and tree traversal algorithms</li> <li>Build trees from arrays and interact with tree structures</li> <li>Analyze time and space complexity of tree operations</li> <li>Explore common applications of tree data structures</li> </ul>"},{"location":"ds_algo/data_structures/06_trees/#environment-setup","title":"Environment Setup","text":"Setup Steps 1. Ensure Python 3.8+ is installed (<code>python --version</code>) 2. No external packages required (uses Python's built-in data structures) 3. Create a working directory for your code 4. Save code examples as <code>.py</code> files 5. Run with <code>python filename.py</code>"},{"location":"ds_algo/data_structures/06_trees/#concept-overview","title":"Concept Overview","text":"<p>A tree is a hierarchical data structure consisting of nodes connected by edges, with a single root node and no cycles.</p>"},{"location":"ds_algo/data_structures/06_trees/#tree-terminology","title":"Tree Terminology","text":"<ul> <li>Node: An element in the tree containing data and references to child nodes</li> <li>Root: The topmost node in the tree</li> <li>Parent/Child: Relationship between connected nodes</li> <li>Leaf: A node with no children</li> <li>Depth: Length of path from root to a node</li> <li>Height: Length of the longest path from a node to a leaf</li> <li>Subtree: Tree formed by a node and its descendants</li> </ul>"},{"location":"ds_algo/data_structures/06_trees/#binary-tree-visualization","title":"Binary Tree Visualization","text":"<pre><code>         \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  A  \u2502           Root (Level 0)\n         \u2514\u2500\u2500\u252c\u2500\u2500\u2518\n      \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n   \u250c\u2500\u2500\u2534\u2500\u2500\u2510     \u250c\u2500\u2500\u2534\u2500\u2500\u2510\n   \u2502  B  \u2502     \u2502  C  \u2502    Level 1\n   \u2514\u2500\u2500\u252c\u2500\u2500\u2518     \u2514\u2500\u2500\u252c\u2500\u2500\u2518\n  \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510       \u2514\u2500\u2500\u2500\u2510\n\u250c\u2500\u2534\u2500\u2510   \u250c\u2500\u2534\u2500\u2510      \u250c\u2500\u2500\u2534\u2500\u2510\n\u2502 D \u2502   \u2502 E \u2502      \u2502 F  \u2502  Level 2 (D, E are leaf nodes)\n\u2514\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ds_algo/data_structures/06_trees/#types-of-trees","title":"Types of Trees","text":"<ol> <li>Binary Tree: Each node has at most two children</li> <li>Binary Search Tree (BST): Binary tree where left child &lt; parent &lt; right child</li> <li>AVL Tree: Self-balancing BST</li> <li>B-Tree: Self-balancing tree with multiple keys per node (not covered in this lab)</li> </ol>"},{"location":"ds_algo/data_structures/06_trees/#starter-code-with-gaps","title":"Starter Code with Gaps","text":"<p>Save this as <code>tree_implementations.py</code>:</p> <pre><code>\"\"\"\nTree Implementations - Collaborative Learning Exercise\n\"\"\"\nimport queue\n\ndef main():\n    print(\"===== BINARY TREE =====\")\n    # Create a binary tree manually\n    root = TreeNode(\"A\")\n    root.left = TreeNode(\"B\")\n    root.right = TreeNode(\"C\")\n    root.left.left = TreeNode(\"D\")\n    root.left.right = TreeNode(\"E\")\n    root.right.right = TreeNode(\"F\")\n\n    # Build a tree from the above illustration\n    print(\"Tree Structure:\")\n    print_tree(root)\n\n    print(\"\\n===== TREE TRAVERSALS =====\")\n    # TODO: Implement and demonstrate tree traversals\n\n    print(\"\\n===== BINARY SEARCH TREE =====\")\n    bst = BinarySearchTree()\n    # TODO: Implement and demonstrate BST operations\n\n    print(\"\\n===== TREE APPLICATIONS =====\")\n    # TODO: Implement and demonstrate tree applications\n\n\nclass TreeNode:\n    \"\"\"Basic node for a binary tree\"\"\"\n\n    def __init__(self, data):\n        \"\"\"Initialize a node with data and null children\"\"\"\n        self.data = data\n        self.left = None\n        self.right = None\n\n\ndef print_tree(root, level=0, prefix=\"Root: \"):\n    \"\"\"Print a visual representation of the tree\"\"\"\n    if root is not None:\n        print(\" \" * (level * 4) + prefix + str(root.data))\n        if root.left is not None or root.right is not None:\n            if root.left:\n                print_tree(root.left, level + 1, \"L\u2500\u2500 \")\n            else:\n                print(\" \" * ((level + 1) * 4) + \"L\u2500\u2500 None\")\n            if root.right:\n                print_tree(root.right, level + 1, \"R\u2500\u2500 \")\n            else:\n                print(\" \" * ((level + 1) * 4) + \"R\u2500\u2500 None\")\n\n\ndef in_order_traversal(root):\n    \"\"\"Traverse tree in-order (left, root, right)\"\"\"\n    # TODO: Implement in-order traversal\n    # This can be done recursively or iteratively\n    pass\n\n\ndef pre_order_traversal(root):\n    \"\"\"Traverse tree pre-order (root, left, right)\"\"\"\n    # TODO: Implement pre-order traversal\n    pass\n\n\ndef post_order_traversal(root):\n    \"\"\"Traverse tree post-order (left, right, root)\"\"\"\n    # TODO: Implement post-order traversal\n    pass\n\n\ndef level_order_traversal(root):\n    \"\"\"Traverse tree level by level (breadth-first)\"\"\"\n    # TODO: Implement level-order traversal\n    # Hint: Use a queue to keep track of nodes at each level\n    pass\n\n\ndef build_tree_from_list(elements):\n    \"\"\"Build a binary tree from a list (array representation)\"\"\"\n    # TODO: Implement tree building from a list\n    # For a list [1,2,3,4,5], create a complete binary tree\n    pass\n\n\nclass BinarySearchTree:\n    \"\"\"Binary Search Tree implementation\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize an empty BST\"\"\"\n        self.root = None\n\n    def insert(self, data):\n        \"\"\"Insert a value into the BST\"\"\"\n        # TODO: Implement insertion for BST\n        # Remember BST property: left &lt; root &lt; right\n        pass\n\n    def search(self, data):\n        \"\"\"Search for a value in the BST\"\"\"\n        # TODO: Implement search for BST\n        pass\n\n    def delete(self, data):\n        \"\"\"Delete a value from the BST\"\"\"\n        # TODO: Implement deletion for BST\n        # This is the most complex operation - handle all cases:\n        # 1. Node with no children (leaf)\n        # 2. Node with one child\n        # 3. Node with two children\n        pass\n\n    def _find_min(self, node):\n        \"\"\"Find the minimum value node in a subtree\"\"\"\n        # TODO: Implement min finding\n        # Hint: Keep going left until you can't anymore\n        pass\n\n    def _find_max(self, node):\n        \"\"\"Find the maximum value node in a subtree\"\"\"\n        # TODO: Implement max finding\n        pass\n\n    def is_valid_bst(self):\n        \"\"\"Check if the tree is a valid BST\"\"\"\n        # TODO: Implement BST validation\n        # Verify that for each node, all left subtree &lt; node &lt; all right subtree\n        pass\n\n\ndef is_balanced(root):\n    \"\"\"Check if a binary tree is balanced (max difference in height between subtrees is 1)\"\"\"\n    # TODO: Implement balance checking\n    pass\n\n\ndef lowest_common_ancestor(root, p, q):\n    \"\"\"Find the lowest common ancestor of two nodes in a binary tree\"\"\"\n    # TODO: Implement LCA finding\n    pass\n\n\ndef serialize_tree(root):\n    \"\"\"Serialize a binary tree to a string\"\"\"\n    # TODO: Implement tree serialization\n    # Convert tree to a string format that can be sent over network or stored\n    pass\n\n\ndef deserialize_tree(data):\n    \"\"\"Deserialize a string back to a binary tree\"\"\"\n    # TODO: Implement tree deserialization\n    # Convert string representation back to a tree\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"ds_algo/data_structures/06_trees/#live-coding-collaboration-tasks","title":"Live Coding Collaboration Tasks","text":""},{"location":"ds_algo/data_structures/06_trees/#task-1-tree-traversal-implementations","title":"Task 1: Tree Traversal Implementations","text":"<p>Working in pairs, implement the tree traversal algorithms:</p> <ol> <li>First person implements <code>in_order_traversal</code> and <code>pre_order_traversal</code></li> <li>Second person implements <code>post_order_traversal</code> and <code>level_order_traversal</code></li> <li>Test all traversals on the sample tree and discuss the output differences</li> </ol>"},{"location":"ds_algo/data_structures/06_trees/#task-2-binary-search-tree-implementation","title":"Task 2: Binary Search Tree Implementation","text":"<p>In teams of 2-3:</p> <ol> <li>First coder implements <code>insert</code> method</li> <li>Second coder implements <code>search</code> method</li> <li>Third coder (or back to first) implements <code>delete</code> with all cases</li> <li>Test BST operations and verify correctness</li> </ol>"},{"location":"ds_algo/data_structures/06_trees/#task-3-tree-algorithms-and-applications","title":"Task 3: Tree Algorithms and Applications","text":"<p>Divide the utility functions among team members:</p> <ol> <li>One person implements <code>build_tree_from_list</code> and <code>is_balanced</code></li> <li>Another implements <code>lowest_common_ancestor</code></li> <li>A third implements <code>serialize_tree</code> and <code>deserialize_tree</code></li> <li>Test all algorithms together</li> </ol>"},{"location":"ds_algo/data_structures/06_trees/#peer-discussion-questions","title":"Peer Discussion Questions","text":"<ol> <li> <p>Tree Structure Design:</p> </li> <li> <p>How do trees compare to other data structures for different operations?</p> </li> <li>What factors affect the choice between different tree types?</li> <li> <p>What real-world hierarchical structures could be represented with trees?</p> </li> <li> <p>Algorithm Analysis:</p> </li> <li> <p>How do different traversal methods affect the processing order?</p> </li> <li>When would you use one traversal method over another?</li> <li> <p>What is the time and space complexity of each tree operation?</p> </li> <li> <p>Balancing Considerations:</p> </li> <li>Why is tree balancing important for performance?</li> <li>How do BST operations degrade with highly unbalanced trees?</li> <li>What strategies exist for maintaining balanced trees?</li> </ol>"},{"location":"ds_algo/data_structures/06_trees/#time-and-space-complexity","title":"Time and Space Complexity","text":"Operation Balanced BST Unbalanced BST (worst case) Search O(log n) O(n) Insert O(log n) O(n) Delete O(log n) O(n) Traversal O(n) O(n) Space O(n) O(n) Height O(log n) O(n)"},{"location":"ds_algo/data_structures/06_trees/#common-bugs-and-debugging-tips","title":"Common Bugs and Debugging Tips","text":"<ol> <li> <p>Parent-Child Connection Problems</p> </li> <li> <p>Issue: Incorrectly setting or updating parent-child relationships</p> </li> <li>Debug: Carefully manage connections when modifying the tree</li> </ol> <pre><code># Ensure both sides of the relationship are updated\ndef set_left_child(self, node, left_child):\n    node.left = left_child\n    # For trees that track parent pointers:\n    if left_child:\n        left_child.parent = node\n</code></pre> <ol> <li> <p>BST Property Violations</p> </li> <li> <p>Issue: Insertions or modifications that break the BST property</p> </li> <li>Debug: Implement a validation function to check BST property</li> </ol> <pre><code>def is_valid_bst(root, min_val=float('-inf'), max_val=float('inf')):\n    if not root:\n        return True\n    if root.data &lt;= min_val or root.data &gt;= max_val:\n        return False\n    return (is_valid_bst(root.left, min_val, root.data) and\n            is_valid_bst(root.right, root.data, max_val))\n</code></pre> <ol> <li> <p>Infinite Recursion</p> </li> <li> <p>Issue: Missing base case in recursive tree operations</p> </li> <li>Debug: Always check for null/empty tree condition</li> </ol> <pre><code>def traverse(node):\n    # Base case first to prevent infinite recursion\n    if node is None:\n        return\n    # Recursive cases\n    traverse(node.left)\n    print(node.data)\n    traverse(node.right)\n</code></pre> <ol> <li>Deletion Edge Cases</li> <li>Issue: Not handling all cases in BST deletion</li> <li>Debug: Carefully test all deletion scenarios    <pre><code># Test cases for deletion:\n# 1. Delete a leaf node\n# 2. Delete a node with one child\n# 3. Delete a node with two children\n# 4. Delete the root node\n# 5. Delete a node that doesn't exist\n</code></pre></li> </ol>"},{"location":"ds_algo/data_structures/06_trees/#group-challenge-expression-tree-builder-and-evaluator","title":"Group Challenge: Expression Tree Builder and Evaluator","text":""},{"location":"ds_algo/data_structures/06_trees/#challenge-task","title":"Challenge Task","text":"<p>Build a system that:</p> <ol> <li>Converts an infix mathematical expression to a binary expression tree</li> <li>Evaluates the expression tree to calculate the result</li> <li>Prints the expression in prefix (Polish) and postfix (Reverse Polish) notation</li> <li>Supports basic operators (+, -, *, /) and parentheses for precedence</li> </ol> <p>Example expression: \"3 + 4 * 2 - (6 / 3)\"</p>"},{"location":"ds_algo/data_structures/06_trees/#approach-instructions","title":"Approach Instructions","text":"<ol> <li> <p>Design Phase (10 minutes):</p> </li> <li> <p>Define the node structure for operators and operands</p> </li> <li>Plan the algorithm for converting infix to expression tree</li> <li> <p>Outline the expression evaluation approach</p> </li> <li> <p>Implementation Phase (15 minutes):</p> </li> <li> <p>Person 1: Implement tokenization and expression tree building</p> </li> <li>Person 2: Implement tree evaluation</li> <li> <p>Person 3: Implement notation conversions (infix to prefix/postfix)</p> </li> <li> <p>Testing Phase (5 minutes):</p> </li> <li>Create test expressions of varying complexity</li> <li>Verify the tree structure through visualization</li> <li>Compare evaluation results with expected answers</li> </ol>"},{"location":"ds_algo/data_structures/06_trees/#starter-code-for-expression-tree","title":"Starter Code for Expression Tree","text":"<pre><code>class ExpressionNode:\n    def __init__(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n    def is_operator(self):\n        \"\"\"Check if the node represents an operator\"\"\"\n        return self.value in \"+-*/\"\n\n\ndef build_expression_tree(expression):\n    \"\"\"Build an expression tree from an infix expression string\"\"\"\n    # TODO: Implement expression tree building\n    # Hint: Use a stack-based algorithm\n    pass\n\n\ndef evaluate_tree(root):\n    \"\"\"Evaluate an expression tree and return the result\"\"\"\n    # TODO: Implement recursive evaluation\n    # Base case: If leaf node (operand), return the value\n    # Recursive case: Evaluate left and right subtrees, apply the operator\n    pass\n\n\ndef infix_to_prefix(expression):\n    \"\"\"Convert infix expression to prefix notation using an expression tree\"\"\"\n    # TODO: Implement prefix conversion\n    pass\n\n\ndef infix_to_postfix(expression):\n    \"\"\"Convert infix expression to postfix notation using an expression tree\"\"\"\n    # TODO: Implement postfix conversion\n    pass\n\n\ndef test_expressions():\n    \"\"\"Test the expression tree with various expressions\"\"\"\n    expressions = [\n        \"3 + 4\",\n        \"3 + 4 * 2\",\n        \"3 * (4 + 2)\",\n        \"3 + 4 * 2 - (6 / 3)\",\n        \"(7 - 2) * (3 + 4) / 2\"\n    ]\n\n    for expr in expressions:\n        # TODO: Process each expression and show results\n        pass\n</code></pre>"},{"location":"ds_algo/data_structures/06_trees/#reflection-questions","title":"Reflection Questions","text":"<p>After completing the exercises:</p> <ol> <li>How does the structure of a binary search tree optimize search operations compared to linear data structures?</li> <li>What were the most challenging aspects of implementing tree traversal algorithms?</li> <li>How would you adapt a basic binary tree to handle more complex hierarchical data?</li> <li>In what real-world applications might you use different types of tree data structures?</li> </ol>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/","title":"Advanced Prompt Engineering Guide","text":""},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#project-type","title":"Project Type","text":"<ul> <li>Prompt Engineering Guide</li> </ul>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#goal","title":"Goal","text":"<ul> <li>Equip expert users with strategies for robust, context-aware, and reproducible AI workflows</li> </ul>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#audience","title":"Audience","text":"<ul> <li>Engineers, researchers, and prompt specialists with deep experience in model interaction</li> </ul>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#requirements","title":"Requirements","text":"<ul> <li>Environment: Browser or API-based model access</li> <li>Prerequisites: Solid understanding of prompt structure, context management, and LLM behaviors</li> <li>Dependencies: OpenAI API, Claude API, or equivalent; optional vector DB integration</li> </ul>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#overview","title":"Overview","text":"<p>This guide covers advanced prompt techniques for scalable systems, automated chains, and model orchestration.</p> <p>Topics include: - Reproducibility techniques - External memory (vector stores) - Tool-augmented prompts - Prompt tuning and embeddings - Dynamic prompt assembly via code</p>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#design-principles","title":"Design Principles","text":"<ul> <li>Systemic prompts: Define behavior at system-level to standardize across use cases</li> <li>Token shaping: Budget prompt/token ratio based on endpoint limits</li> <li>Semantic slotting: Inject runtime data via placeholders, templates, or middleware</li> <li>Instructional self-checks: Ask model to verify outputs before return</li> <li>Hierarchical control: Delegate stages to specialized sub-prompts</li> </ul>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#architecture-patterns","title":"Architecture Patterns","text":"Pattern Purpose Example Implementation Tool-use prompting Route output to API/tool \u201cCall Wolfram to solve the math\u201d Memory recall Pull related context on demand Query vector DB before prompt Dynamic injection Assemble prompts in real time Use Jinja or LangChain templates Chain decomposition Parallelize multi-step reasoning Map \u2192 Reduce via sub-model calls Self-verification Force output consistency \u201cDouble-check for logic errors\u201d"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#step-by-step-dynamic-prompt-generation","title":"Step-by-Step: Dynamic Prompt Generation","text":"<p>Context: You\u2019re generating emails for different product teams.</p> <p>Template Setup: <pre><code>Role: {role}\nProduct: {product}\nTone: {tone}\nInstruction: Write a {length}-word email summarizing the latest feature for {product}.\n</code></pre></p> <p>Runtime Injection: <pre><code>from jinja2 import Template\n\ntemplate = Template(open(\"email_template.txt\").read())\nfilled = template.render(role=\"PM\", product=\"ChatApp\", tone=\"neutral\", length=\"150\")\n</code></pre></p>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#embedding-driven-prompting","title":"Embedding-Driven Prompting","text":"<ul> <li>Vector context injection: Use embeddings to fetch top-k relevant facts before prompting</li> <li>Example: Retrieval-Augmented Generation (RAG) for legal/medical summarization</li> <li>Benefits: Reduces hallucination, increases factual accuracy</li> </ul> <pre><code># Pseudo-code\nquery_embedding = embed(\"Summarize this contract\")\ndocs = vectordb.query(query_embedding, top_k=3)\ncontext = \"\n\".join(docs)\nprompt = f\"{context}\n\nSummarize in 5 bullet points.\"\n</code></pre>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#testing-for-robustness","title":"Testing for Robustness","text":"Technique Goal Prompt regression tests Check versioned output stability Fuzzing inputs Expose weak prompt structures Multiple temperature runs Probe edge case behavior Output schema validation Enforce type-safe replies"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#example-advanced-prompts","title":"Example Advanced Prompts","text":"<pre><code># Tool-handling chain (LangChain style)\ntools = [SearchAPI(), CalculatorTool()]\nagent_prompt = \"Use available tools to answer: What is 23% of the population of France?\"\n\n# Code-as-context injection\ncode_snippet = open(\"example.py\").read()\nprompt = f\"Explain this Python code. Include edge cases.\n\n{code_snippet}\"\n\n# Evaluation meta-prompt\nmeta = \"Evaluate this output for logical consistency and factual accuracy. Respond in JSON with { is_valid: true/false, notes: '' }\"\n</code></pre>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#prompt-deployment-considerations","title":"Prompt Deployment Considerations","text":"<ul> <li>Use source-controlled prompt templates</li> <li>Add versioning metadata to every prompt</li> <li>Centralize prompt definitions in app logic</li> <li>Auto-log completions + token usage for analysis</li> <li>Monitor via observability stack (e.g., OpenTelemetry)</li> </ul>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#common-pitfalls-at-expert-level","title":"Common Pitfalls at Expert Level","text":"Mistake Fix Overstuffing context Trim and prioritize\u2014less is more after ~2K tokens Forgetting token budget Estimate output length from prompt shape Ignoring user safety in chain Check tool outputs before reuse in downstream prompts Hardcoding sensitive prompts Parameterize, encrypt, or access securely"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#-","title":"---","text":""},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#active-learning-activity","title":"Active Learning Activity","text":"<p>Time: 30 minutes Group Size: 3\u20135 people Goal Alignment: Build a reproducible and tool-integrated prompt workflow using prompt chaining or dynamic injection Instructions: 1. Choose a complex workflow (e.g., dynamic report generation, multi-role simulation, tool invocation). 2. Map stages and divide prompt engineering responsibilities among the group. 3. Use templates, code snippets, and if needed, an API or search tool simulator. 4. Each person runs and validates their stage. Ensure outputs are compatible across the chain. 5. Regroup to test the full pipeline and debug edge cases. Deliverables: - Workflow diagram with prompts - Each stage\u2019s output and test logs - Notes on integration, failure points, and improvement areas</p>"},{"location":"prompt_emgineering/advanced_prompt_engineering_guide/#advanced-prompt-templates-and-manipulation-techniques","title":"Advanced Prompt Templates and Manipulation Techniques","text":"<p>High-Control Techniques - Function-style prompts (force model to complete inputs) - Embedded memory lookup (via context or tools) - Prompt templates with slot injection - Embedded JSON and YAML templates - Self-evaluation, meta-instructions</p> <p>Examples</p> <pre><code># JSON validation\nprompt = \"Summarize this article in 3 bullet points. Return result as JSON: { summary: [], word_count: '' }\"\n\n# Role-switch + meta\nprompt = \"You are a code reviewer. Read this Python snippet and return two things: 1 improvement, 1 compliment. Then ask a follow-up question.\"\n\n# Dynamic input injection\ntemplate = \"\"\"Write a 150-word product description for {{product}}. Focus on its unique {{feature}} for the {{audience}}.\"\"\"\nrendered_prompt = template.format(product=\"SmartThermo\", feature=\"AI auto-scheduling\", audience=\"remote workers\")\n\n# Prompt-based fallback plan\nstep_1 = \"Summarize the dataset\"\nstep_2 = \"If the summary is too vague, re-ask the model using more specific context\"\n</code></pre> <p>Advanced Integration Challenges - API call output as prompt input - Multi-agent collaboration simulation - Prompt schema versioning across workflows</p>"},{"location":"prompt_emgineering/documentation_request/","title":"Documentation Request Template","text":""},{"location":"prompt_emgineering/documentation_request/#project-type","title":"Project Type","text":"<p>Prompt Engineering Guide for AI Models</p>"},{"location":"prompt_emgineering/documentation_request/#documentation-goal","title":"Documentation Goal","text":"<p>Setup guide and implementation tutorial for effective prompt engineering techniques</p>"},{"location":"prompt_emgineering/documentation_request/#target-audience","title":"Target Audience","text":"<p>Beginners with little to no experience in working with AI language models</p>"},{"location":"prompt_emgineering/documentation_request/#technical-requirements","title":"Technical Requirements","text":"<ul> <li>Environment: Any modern web browser</li> <li>Prerequisites: Basic understanding of natural language</li> <li>Dependencies: Access to AI language models (like ChatGPT, Claude, etc.)</li> </ul>"},{"location":"prompt_emgineering/documentation_request/#desired-sections","title":"Desired Sections","text":"<ul> <li>Introduction to prompt engineering</li> <li>Core principles and best practices</li> <li>Basic prompt patterns</li> <li>Step-by-step implementation examples</li> <li>Common use cases</li> <li>Evaluation methods</li> <li>Troubleshooting and refinement</li> </ul>"},{"location":"prompt_emgineering/documentation_request/#special-focus-areas","title":"Special Focus Areas","text":"<ul> <li>Clarity and specificity in prompts</li> <li>Context management techniques</li> <li>Handling model limitations</li> <li>Ethical considerations</li> <li>Avoiding common beginner mistakes</li> </ul>"},{"location":"prompt_emgineering/documentation_request/#code-examples-needed","title":"Code Examples Needed","text":"<ul> <li>Simple question-answering prompts</li> <li>Multi-step instruction prompts</li> <li>Format control examples</li> <li>Role-playing prompts</li> <li>System prompt examples</li> </ul>"},{"location":"prompt_emgineering/documentation_request/#additional-notes","title":"Additional Notes","text":"<p>Include visual examples of before/after prompts to demonstrate improvement techniques. Focus on practical applications rather than technical theory.</p>"},{"location":"prompt_emgineering/guide_prompt_chaining/","title":"Prompt Engineering Guide: Prompt Chaining","text":""},{"location":"prompt_emgineering/guide_prompt_chaining/#project-type","title":"Project Type","text":"<ul> <li>Prompt Engineering Guide</li> </ul>"},{"location":"prompt_emgineering/guide_prompt_chaining/#goal","title":"Goal","text":"<ul> <li>Teach users how to split tasks into prompt chains for staged outputs</li> </ul>"},{"location":"prompt_emgineering/guide_prompt_chaining/#audience","title":"Audience","text":"<ul> <li>Users experienced in structured and persona prompting</li> </ul>"},{"location":"prompt_emgineering/guide_prompt_chaining/#requirements","title":"Requirements","text":"<ul> <li>Environment: Any modern browser</li> <li>Prerequisites: Comfort with multi-step thinking and follow-up prompting</li> <li>Dependencies: ChatGPT, Claude, or similar</li> </ul>"},{"location":"prompt_emgineering/guide_prompt_chaining/#introduction","title":"Introduction","text":"<p>This guide introduces chained prompting: - Break a problem into sequenced prompts - Use intermediate outputs - Build staged reasoning</p>"},{"location":"prompt_emgineering/guide_prompt_chaining/#core-principles","title":"Core Principles","text":"<ul> <li>Stage control: Each prompt handles part of the job</li> <li>Data re-use: Feed output from step A into step B</li> <li>Workflow modeling: Mimic human thinking process</li> </ul>"},{"location":"prompt_emgineering/guide_prompt_chaining/#prompt-patterns","title":"Prompt Patterns","text":"Pattern Example Prompt Idea \u2192 Expand \u201cList 3 marketing ideas.\u201d \u2192 \u201cExpand the second into a 100-word pitch.\u201d Fact \u2192 Format \u201cGive a fact.\u201d \u2192 \u201cFormat that fact as a tweet under 280 characters.\u201d Extract \u2192 Summarize \u201cExtract key points.\u201d \u2192 \u201cSummarize those in a markdown list.\u201d"},{"location":"prompt_emgineering/guide_prompt_chaining/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Before:</p> <p>What\u2019s a good startup idea?</p> <p>After: 1. \u201cList 3 startup ideas for remote teams.\u201d 2. \u201cPick the first and write a 100-word product pitch.\u201d 3. \u201cSummarize that pitch as a one-line tweet.\u201d</p>"},{"location":"prompt_emgineering/guide_prompt_chaining/#use-cases","title":"Use Cases","text":"<ul> <li>Product ideation</li> <li>Multi-format generation</li> <li>Structured thinking assistance</li> </ul>"},{"location":"prompt_emgineering/guide_prompt_chaining/#evaluation","title":"Evaluation","text":"<ul> <li>Each output links logically</li> <li>Stages match intended task depth</li> <li>Output from one step feeds next step cleanly</li> </ul>"},{"location":"prompt_emgineering/guide_prompt_chaining/#troubleshooting","title":"Troubleshooting","text":"Problem Fix Gaps between steps Restate context in each prompt Lost focus Add reminders or stage labels Model hallucination Reduce temperature, clarify stage boundaries"},{"location":"prompt_emgineering/guide_prompt_chaining/#visual-beforeafter","title":"Visual Before/After","text":"<pre><code>\u274c \u201cWrite a startup idea.\u201d\n\n\u2705 \n1. \u201cList 3 startup ideas for seniors.\u201d\n2. \u201cPick one. Write a 3-sentence pitch.\u201d\n3. \u201cSummarize in a title and tagline.\u201d\n</code></pre>"},{"location":"prompt_emgineering/guide_prompt_chaining/#code-examples","title":"Code Examples","text":"<pre><code>prompt1 = \"List 3 AI use cases in education.\"\nprompt2 = \"Take the second and describe its impact in 100 words.\"\nprompt3 = \"Summarize that as a tweet.\"\n</code></pre>"},{"location":"prompt_emgineering/guide_role_persona_prompting/","title":"Prompt Engineering Guide: Role and Persona Control","text":""},{"location":"prompt_emgineering/guide_role_persona_prompting/#project-type","title":"Project Type","text":"<ul> <li>Prompt Engineering Guide</li> </ul>"},{"location":"prompt_emgineering/guide_role_persona_prompting/#goal","title":"Goal","text":"<ul> <li>Teach users how to control model tone and perspective using personas and roles</li> </ul>"},{"location":"prompt_emgineering/guide_role_persona_prompting/#audience","title":"Audience","text":"<ul> <li>Intermediate users ready to simulate character-based responses</li> </ul>"},{"location":"prompt_emgineering/guide_role_persona_prompting/#requirements","title":"Requirements","text":"<ul> <li>Environment: Any modern browser</li> <li>Prerequisites: Familiarity with format-specific prompting</li> <li>Dependencies: ChatGPT, Claude, or similar</li> </ul>"},{"location":"prompt_emgineering/guide_role_persona_prompting/#introduction","title":"Introduction","text":"<p>This guide introduces persona prompting: - Simulating professional or fictional roles - Changing tone and voice - Improving user-fit for outputs</p>"},{"location":"prompt_emgineering/guide_role_persona_prompting/#core-principles","title":"Core Principles","text":"<ul> <li>Persona control: Tell the model who it is</li> <li>Tone direction: Friendly, formal, technical</li> <li>Task alignment: Match persona to expected output</li> </ul>"},{"location":"prompt_emgineering/guide_role_persona_prompting/#prompt-patterns","title":"Prompt Patterns","text":"Pattern Example Prompt Professional \u201cAct as a nutritionist. Give a weekly meal plan for a diabetic patient.\u201d Teacher \u201cAs a math teacher, explain fractions to a 5th-grade student.\u201d Support Agent \u201cPretend you are a support rep. Write a response for a late delivery email.\u201d"},{"location":"prompt_emgineering/guide_role_persona_prompting/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Before:</p> <p>How should I learn SQL?</p> <p>After:</p> <p>Act as a data science mentor. Recommend a 4-step plan for learning SQL for analytics.</p>"},{"location":"prompt_emgineering/guide_role_persona_prompting/#use-cases","title":"Use Cases","text":"<ul> <li>Role-specific communication</li> <li>Expert simulation</li> <li>Guided coaching</li> </ul>"},{"location":"prompt_emgineering/guide_role_persona_prompting/#evaluation","title":"Evaluation","text":"<ul> <li>Tone match to role</li> <li>Persona relevance</li> <li>Clarity and structure</li> </ul>"},{"location":"prompt_emgineering/guide_role_persona_prompting/#troubleshooting","title":"Troubleshooting","text":"Problem Fix Output too generic Add explicit role and task description Wrong tone Specify \u201cprofessional,\u201d \u201cfriendly,\u201d etc. Role confusion Add \u201cact as\u2026\u201d or \u201cyou are\u2026\u201d at the start"},{"location":"prompt_emgineering/guide_role_persona_prompting/#visual-beforeafter","title":"Visual Before/After","text":"<pre><code>\u274c \u201cExplain budgeting.\u201d\n\n\u2705 \u201cAct as a personal finance coach. Explain budgeting to a recent college grad. Use friendly tone.\u201d\n</code></pre>"},{"location":"prompt_emgineering/guide_role_persona_prompting/#code-examples","title":"Code Examples","text":"<pre><code>prompt = \"Act as a hiring manager. Share 3 things you look for in resumes from junior devs.\"\nprompt = \"You are a startup founder. Explain how you chose your product-market fit strategy.\"\nprompt = \"As a UX researcher, write feedback for a first-time app designer.\"\n</code></pre>"},{"location":"prompt_emgineering/guide_structured_prompting/","title":"Prompt Engineering Guide: Structured Prompting","text":""},{"location":"prompt_emgineering/guide_structured_prompting/#project-type","title":"Project Type","text":"<ul> <li>Prompt Engineering Guide</li> </ul>"},{"location":"prompt_emgineering/guide_structured_prompting/#goal","title":"Goal","text":"<ul> <li>Teach users how to apply output formatting and structure to guide the model response</li> </ul>"},{"location":"prompt_emgineering/guide_structured_prompting/#audience","title":"Audience","text":"<ul> <li>Users with experience in simple prompts seeking control over structure and clarity</li> </ul>"},{"location":"prompt_emgineering/guide_structured_prompting/#requirements","title":"Requirements","text":"<ul> <li>Environment: Any modern browser</li> <li>Prerequisites: Understanding of basic prompts</li> <li>Dependencies: ChatGPT, Claude, or similar</li> </ul>"},{"location":"prompt_emgineering/guide_structured_prompting/#introduction","title":"Introduction","text":"<p>This guide introduces structured prompting techniques: - Markdown formatting - Ordered lists and steps - Word and format constraints</p>"},{"location":"prompt_emgineering/guide_structured_prompting/#core-principles","title":"Core Principles","text":"<ul> <li>Format directives: Markdown, numbered lists, tables</li> <li>Content scope: Add word count or topic focus</li> <li>Clarity: Avoid vague commands by narrowing topic and output type</li> </ul>"},{"location":"prompt_emgineering/guide_structured_prompting/#prompt-patterns","title":"Prompt Patterns","text":"Pattern Example Prompt Markdown bullets \u201cList 5 benefits of AI in markdown bullet format.\u201d Numbered steps \u201cExplain how to create a Git branch in 3 ordered steps.\u201d Word constraint \u201cWrite a 100-word summary on climate policy.\u201d"},{"location":"prompt_emgineering/guide_structured_prompting/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Before:</p> <p>Tell me about machine learning.</p> <p>After:</p> <p>Write a 3-point summary of machine learning applications in markdown bullet format. Keep it under 75 words.</p>"},{"location":"prompt_emgineering/guide_structured_prompting/#use-cases","title":"Use Cases","text":"<ul> <li>Structured summaries</li> <li>Quick-reference guides</li> <li>Short-form technical documentation</li> </ul>"},{"location":"prompt_emgineering/guide_structured_prompting/#evaluation","title":"Evaluation","text":"<ul> <li>Count output items (matches list?)</li> <li>Check formatting (matches request?)</li> <li>Confirm brevity and relevance</li> </ul>"},{"location":"prompt_emgineering/guide_structured_prompting/#troubleshooting","title":"Troubleshooting","text":"Problem Fix Too long Add word limits Wrong format Specify markdown, bullets, or numbered list Missing clarity Add topic focus and number of items requested"},{"location":"prompt_emgineering/guide_structured_prompting/#visual-beforeafter","title":"Visual Before/After","text":"<pre><code>\u274c \u201cDescribe deep learning.\u201d\n\n\u2705 \u201cList 3 real-world uses of deep learning in markdown bullets. Max 60 words.\u201d\n</code></pre>"},{"location":"prompt_emgineering/guide_structured_prompting/#code-examples","title":"Code Examples","text":"<pre><code>prompt = \"List 5 open-source LLMs in markdown bullet format.\"\nprompt = \"Explain HTTP in 3 numbered steps under 50 words.\"\nprompt = \"Give a 100-word intro to graph databases. Use simple language.\"\n</code></pre>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/","title":"Intermediate Prompt Engineering Guide","text":""},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#project-type","title":"Project Type","text":"<ul> <li>Prompt Engineering Guide</li> </ul>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#goal","title":"Goal","text":"<ul> <li>Help users refine control, structure, and reliability of AI model outputs</li> </ul>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#audience","title":"Audience","text":"<ul> <li>Intermediate users with some experience writing prompts</li> </ul>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#requirements","title":"Requirements","text":"<ul> <li>Environment: Any modern browser</li> <li>Prerequisites: Comfort with writing basic prompts</li> <li>Dependencies: Access to ChatGPT, Claude, or similar</li> </ul>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#introduction","title":"Introduction","text":"<p>This guide focuses on improving model performance through advanced prompting techniques: - Prompt chaining - Output templating - Multi-shot prompting - Token efficiency - Controlled hallucination mitigation</p>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#advanced-principles","title":"Advanced Principles","text":"<ul> <li>Chaining: Split tasks across multiple prompts (e.g., idea generation \u2192 expansion \u2192 summary).</li> <li>Memory Anchoring: Reinforce critical context with repeated key phrases or summaries.</li> <li>Type coercion: Force format using numbered steps, JSON schema, or pseudo-code structures.</li> <li>Bias mitigation: Explicitly request diverse answers or multiple perspectives.</li> <li>Output validation: Instruct model to self-check output before responding.</li> </ul>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#prompt-frameworks","title":"Prompt Frameworks","text":"Framework Use Case Example Prompt Chain-of-thought Step-by-step reasoning \u201cSolve this problem and explain each step: 23 + 47\u201d Zero vs Few-shot Show model structure via examples \u201cTranslate to French: Hello \u2192 Bonjour\u201d Reframing Reduce ambiguity or add perspective \u201cRephrase this from a manager's perspective\u201d JSON-forcing Get structured output \u201cReturn this summary in JSON: title, bullets, key_points\u201d Role layering Combine personas \u201cAct as a product manager and UX researcher writing feedback to a dev team.\u201d"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#step-by-step-refinement","title":"Step-by-Step Refinement","text":"<p>Before:</p> <p>Explain the importance of clean code.</p> <p>Intermediate:</p> <p>Act as a senior software engineer writing an internal memo to junior developers. Explain why clean code improves team efficiency and long-term maintenance. Give 3 clear examples using bullet points.</p>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#use-cases-and-techniques","title":"Use Cases and Techniques","text":"<ul> <li>Data preprocessing prompts</li> <li>Automated QA tasks</li> <li>Role play for stakeholder communication</li> <li>Real-time prompt chaining in workflows</li> <li>Content summarization with constraints</li> </ul>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#prompt-testing-and-optimization","title":"Prompt Testing and Optimization","text":"<ul> <li>Vary tone and role to test reliability</li> <li>Add instruction scaffolding (e.g., \u201cDo not guess if unsure\u201d)</li> <li>Observe token usage to avoid truncation</li> <li>Use temperature and top_p controls (in API) for output variance</li> </ul>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#example-code-snippets","title":"Example Code Snippets","text":"<pre><code># JSON structure enforcement\nprompt = \"Summarize this text in the following JSON format: { title: '', summary: '', tags: [] }\"\n\n# Few-shot learning\nexamples = '''\nInput: Convert to uppercase \u2192 hello\nOutput: HELLO\n\nInput: Convert to uppercase \u2192 apple\nOutput: APPLE\n'''\n\n# Prompt chaining (simulated)\nstep_1 = \"List 3 startup ideas in edtech.\"\nstep_2 = \"Expand the second idea into a 150-word pitch with market validation and features.\"\n</code></pre>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#visual-prompt-transformations","title":"Visual Prompt Transformations","text":"<pre><code>\u274c \u201cMake a lesson plan.\u201d\n\n\u2705 \u201cDesign a one-week lesson plan for a 10th grade biology class. Include daily topics, one key activity per day, and end-of-week assessment.\u201d\n</code></pre>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#ethical-guardrails","title":"Ethical Guardrails","text":"<ul> <li>Include disclaimers in sensitive or speculative prompts</li> <li>Avoid identity simulation unless explicitly needed</li> <li>Validate AI-generated data with reliable sources</li> <li>Clarify when roleplay should stop or switch</li> </ul>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#common-intermediate-errors","title":"Common Intermediate Errors","text":"Mistake Correction Tip Under-specifying formats Provide schema or structure upfront Assuming output reuse is stable Repeat constraints or use rephrased anchors Using high-temp without limiters Use \u201crespond in max 3 sentences\u201d + format directive"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#-","title":"---","text":""},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#active-learning-activity","title":"Active Learning Activity","text":"<p>Time: 30 minutes Group Size: 3\u20135 people Goal Alignment: Practice chaining, tone control, and response validation through iterative prompting Instructions: 1. Pick a multi-part content generation task (e.g., job post \u2192 summary \u2192 FAQ). 2. Assign each stage to a different team member. 3. Draft prompts collaboratively and run them in sequence. 4. At each stage, review the quality and consistency of the output. 5. Apply structure, tone, or persona adjustments to improve each step. Deliverables: - Prompt chain and outputs - Role notes and prompt rewrites - Final outputs and team reflections</p>"},{"location":"prompt_emgineering/intermediate_prompt_engineering_guide/#intermediate-prompt-frameworks-and-examples","title":"Intermediate Prompt Frameworks and Examples","text":"<p>Frameworks Used - Reframing (clarify ambiguous tasks) - Multimodal (output structure + role + tone) - Post-processing (model checks own work) - Chain-of-Thought + Schema</p> <p>Examples</p> <pre><code># Stepwise with post-check\nprompt = \"Explain Newton\u2019s laws briefly. Then check your explanation for accuracy and simplicity.\"\n\n# Role + Format + Length\nprompt = \"As a financial advisor, list 3 credit score tips in markdown bullets. Max 20 words each.\"\n\n# Reframed query\noriginal = \"Write about the internet.\"\nrevised = \"Write a 75-word paragraph explaining how the internet enables cloud computing.\"\n\n# JSON-coercion\nprompt = \"Return a list of 3 key plot points from the story in this JSON format: { title: '', event: '' }\"\n</code></pre> <p>Prompt Rewrite Exercise - Write \u2192 Reframe \u2192 Add structure \u2192 Add role \u2192 Output limiter \u2192 Validate</p>"},{"location":"prompt_emgineering/prompt_engineering_guide/","title":"Prompt Engineering Guide for AI Models","text":""},{"location":"prompt_emgineering/prompt_engineering_guide/#project-type","title":"Project Type","text":"<ul> <li>Prompt Engineering Guide</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide/#goal","title":"Goal","text":"<ul> <li>Teach beginners how to write clear, effective prompts for AI models</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide/#audience","title":"Audience","text":"<ul> <li>Beginners with no prior experience in prompt design</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide/#requirements","title":"Requirements","text":"<ul> <li>Environment: Any modern browser</li> <li>Prerequisites: Understand basic written instructions</li> <li>Dependencies: Access to ChatGPT, Claude, or similar</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide/#introduction-to-prompt-engineering","title":"Introduction to Prompt Engineering","text":"<p>Prompt engineering is the practice of designing inputs (prompts) that guide AI models to produce useful outputs.</p> <p>Good prompts: - Are clear and specific - Provide context - Match the AI\u2019s capabilities to the task</p>"},{"location":"prompt_emgineering/prompt_engineering_guide/#core-principles","title":"Core Principles","text":"<ul> <li>Be direct: Say what you want, avoid vague phrasing.</li> <li>Specify format: Ask for lists, tables, markdown, etc.</li> <li>Control tone: Use system instructions or persona cues.</li> <li>Provide context: Embed key information into the prompt.</li> <li>Test and refine: Prompt design is iterative.</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide/#prompt-patterns","title":"Prompt Patterns","text":"Type Example Prompt Simple Q&amp;A \u201cWhat is the capital of Japan?\u201d Multi-step Instructions \u201cList 3 ideas for a blog. Then expand each into a 100-word paragraph.\u201d Format Control \u201cSummarize this article in bullet points.\u201d Role-based \u201cAct as a nutritionist. Recommend meals for a vegan athlete.\u201d System Prompt \u201cYou are a friendly tutor helping a 10-year-old learn division.\u201d"},{"location":"prompt_emgineering/prompt_engineering_guide/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Before:</p> <p>Tell me about dogs.</p> <p>After:</p> <p>Give me a bulleted list of five dog breeds suitable for families with kids. Include size, temperament, and grooming needs.</p> <p>Why it\u2019s better: - Adds structure - Focuses the task - Guides formatting</p>"},{"location":"prompt_emgineering/prompt_engineering_guide/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Q&amp;A bots  </li> <li>Writing assistants  </li> <li>Simulated conversations  </li> <li>Text formatting tasks  </li> <li>Code generation helpers  </li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide/#evaluation-methods","title":"Evaluation Methods","text":"<ul> <li>Compare outputs across prompt variations  </li> <li>Check consistency over multiple runs  </li> <li>Measure clarity: Are outputs on-topic and structured?  </li> <li>Ask humans to rank responses  </li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide/#troubleshooting","title":"Troubleshooting","text":"Problem Fix Output too vague Add more constraints (e.g., format, word count) Model misunderstands Rephrase with simpler terms Too verbose Add: \u201cBe concise. Max 3 sentences.\u201d Hallucinated info Ask model to cite sources or say \u201cI don\u2019t know\u201d"},{"location":"prompt_emgineering/prompt_engineering_guide/#special-focus-areas","title":"Special Focus Areas","text":"<ul> <li>Clarity: Use short, structured prompts  </li> <li>Context Management: Reuse key info in long chats  </li> <li>Limitations: Models make things up; verify important data  </li> <li>Ethics: Avoid leading prompts, respect boundaries  </li> <li>Beginner Traps: Don\u2019t assume the model \u201cknows\u201d what you mean without spelling it out  </li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide/#visual-beforeafter-examples","title":"Visual Before/After Examples","text":"<pre><code>\u274c \u201cExplain climate change.\u201d\n\n\u2705 \u201cWrite a 5-sentence summary of climate change causes and effects for high school students. Use simple language.\u201d\n</code></pre>"},{"location":"prompt_emgineering/prompt_engineering_guide/#example-code-snippets","title":"Example Code Snippets","text":"<pre><code># Simple Q&amp;A\nprompt = \"What are three benefits of meditation?\"\n\n# Multi-step\nprompt = \"List 3 fitness goals. Then write one sentence for how to achieve each.\"\n\n# Format control\nprompt = \"Explain Kubernetes in a markdown table with columns: Concept, Description.\"\n\n# Role-play\nprompt = \"You are a personal finance coach. Give tips for saving money on groceries.\"\n\n# System prompt example (ChatGPT API-style)\nsystem_prompt = \"You are a concise expert in computer networking. Respond in two sentences max.\"\n</code></pre>"},{"location":"prompt_emgineering/prompt_engineering_guide/#-","title":"---","text":""},{"location":"prompt_emgineering/prompt_engineering_guide/#active-learning-activity","title":"Active Learning Activity","text":"<p>Time: 30 minutes Group Size: 2\u20134 people Goal Alignment: Learn how prompt clarity improves model performance in Q&amp;A, instructions, and format control Instructions: 1. Split into pairs. Each person writes 1 vague prompt and 1 clear, structured version. 2. Trade prompts with your partner and run them through an AI. 3. Compare results: Identify which prompt produced a better answer and why. 4. Revise both prompts together. Discuss improvements and re-run for validation. 5. Record final versions and outputs. Reflect on what prompt traits led to the best results. Deliverables: - Before/after prompt versions - Model output screenshots or copies - Notes on which traits improved clarity and accuracy</p>"},{"location":"prompt_emgineering/prompt_engineering_guide/#prompt-engineering-techniques-expanded-examples","title":"Prompt Engineering Techniques: Expanded Examples","text":"<p>Prompt Manipulation Methods - Specify output format (table, list, paragraph) - Set a maximum word or sentence limit - Frame as a role (teacher, coach, analyst) - Add structure (steps, questions, sections) - Combine multiple tasks into one prompt</p> <p>Examples</p> <pre><code>\u201cList 3 key benefits of recycling.\u201d \u2192 Adds clarity\n\u201cList 3 key benefits of recycling in bullet format. Max 20 words each.\u201d \u2192 Adds format and length control\n\n\u201cWrite about the French Revolution.\u201d \u2192 Vague\n\u201cWrite a 5-sentence overview of the French Revolution for 9th grade students. Use simple language.\u201d \u2192 Targeted and scoped\n\n\u201cExplain how computers work.\u201d \u2192 Too broad\n\u201cExplain how a CPU works using an analogy for a 12-year-old. Respond in under 4 sentences.\u201d \u2192 Focused, age-appropriate, concise\n\n\u201cSummarize this text.\u201d \u2192 Generic\n\u201cSummarize the following in markdown bullets with one bold keyword per line.\u201d \u2192 Output styling added\n</code></pre> <p>Group Exercise Variants - Reverse engineer a vague prompt - Add format, tone, and role constraints - Use numbered step prompts with error checks</p>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/","title":"Prompt Engineering Guide for AI Models","text":""},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#project-type","title":"Project Type","text":"<ul> <li>Prompt Engineering Guide</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#goal","title":"Goal","text":"<ul> <li>Teach beginners how to write clear, effective prompts for AI models</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#audience","title":"Audience","text":"<ul> <li>Beginners with no prior experience in prompt design</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#requirements","title":"Requirements","text":"<ul> <li>Environment: Any modern browser</li> <li>Prerequisites: Understand basic written instructions</li> <li>Dependencies: Access to ChatGPT, Claude, or similar</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#introduction-to-prompt-engineering","title":"Introduction to Prompt Engineering","text":"<p>Prompt engineering is the practice of designing inputs (prompts) that guide AI models to produce useful outputs.</p> <p>Good prompts: - Are clear and specific - Provide context - Match the AI\u2019s capabilities to the task</p>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#core-principles","title":"Core Principles","text":"<ul> <li>Be direct: Say what you want, avoid vague phrasing.</li> <li>Specify format: Ask for lists, tables, markdown, etc.</li> <li>Control tone: Use system instructions or persona cues.</li> <li>Provide context: Embed key information into the prompt.</li> <li>Test and refine: Prompt design is iterative.</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#prompt-patterns","title":"Prompt Patterns","text":"Type Example Prompt Simple Q&amp;A \u201cWhat is the capital of Japan?\u201d Multi-step Instructions \u201cList 3 ideas for a blog. Then expand each into a 100-word paragraph.\u201d Format Control \u201cSummarize this article in bullet points.\u201d Role-based \u201cAct as a nutritionist. Recommend meals for a vegan athlete.\u201d System Prompt \u201cYou are a friendly tutor helping a 10-year-old learn division.\u201d"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Before:</p> <p>Tell me about dogs.</p> <p>After:</p> <p>Give me a bulleted list of five dog breeds suitable for families with kids. Include size, temperament, and grooming needs.</p> <p>Why it\u2019s better: - Adds structure - Focuses the task - Guides formatting</p>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Q&amp;A bots  </li> <li>Writing assistants  </li> <li>Simulated conversations  </li> <li>Text formatting tasks  </li> <li>Code generation helpers  </li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#evaluation-methods","title":"Evaluation Methods","text":"<ul> <li>Compare outputs across prompt variations  </li> <li>Check consistency over multiple runs  </li> <li>Measure clarity: Are outputs on-topic and structured?  </li> <li>Ask humans to rank responses  </li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#troubleshooting","title":"Troubleshooting","text":"Problem Fix Output too vague Add more constraints (e.g., format, word count) Model misunderstands Rephrase with simpler terms Too verbose Add: \u201cBe concise. Max 3 sentences.\u201d Hallucinated info Ask model to cite sources or say \u201cI don\u2019t know\u201d"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#special-focus-areas","title":"Special Focus Areas","text":"<ul> <li>Clarity: Use short, structured prompts  </li> <li>Context Management: Reuse key info in long chats  </li> <li>Limitations: Models make things up; verify important data  </li> <li>Ethics: Avoid leading prompts, respect boundaries  </li> <li>Beginner Traps: Don\u2019t assume the model \u201cknows\u201d what you mean without spelling it out  </li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#visual-beforeafter-examples","title":"Visual Before/After Examples","text":"<pre><code>\u274c \u201cExplain climate change.\u201d\n\n\u2705 \u201cWrite a 5-sentence summary of climate change causes and effects for high school students. Use simple language.\u201d\n</code></pre>"},{"location":"prompt_emgineering/prompt_engineering_guide_easy/#example-code-snippets","title":"Example Code Snippets","text":"<pre><code># Simple Q&amp;A\nprompt = \"What are three benefits of meditation?\"\n\n# Multi-step\nprompt = \"List 3 fitness goals. Then write one sentence for how to achieve each.\"\n\n# Format control\nprompt = \"Explain Kubernetes in a markdown table with columns: Concept, Description.\"\n\n# Role-play\nprompt = \"You are a personal finance coach. Give tips for saving money on groceries.\"\n\n# System prompt example (ChatGPT API-style)\nsystem_prompt = \"You are a concise expert in computer networking. Respond in two sentences max.\"\n</code></pre>"},{"location":"prompt_emgineering/prompt_engineering_prompt_chaining/","title":"Prompt Engineering: Chained and Multi-Prompt Setup","text":""},{"location":"prompt_emgineering/prompt_engineering_prompt_chaining/#audience","title":"Audience","text":"<ul> <li>Users fluent in structured prompting and persona control, ready to split complex tasks</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_prompt_chaining/#goal","title":"Goal","text":"<ul> <li>Teach chained prompting and multi-turn workflows</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_prompt_chaining/#techniques-introduced","title":"Techniques Introduced","text":"<ul> <li>Split complex prompts into sub-steps</li> <li>Link outputs as inputs</li> <li>Use clarifying follow-ups</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_prompt_chaining/#prompt-evolution","title":"Prompt Evolution","text":"<p>Before:</p> <p>Write a blog post on healthy habits.</p> <p>Now (chained): 1. \u201cList 3 unique habits for health improvement.\u201d 2. \u201cTake the second habit and write a 100-word section explaining its science.\u201d 3. \u201cSummarize this section for Instagram.\u201d</p>"},{"location":"prompt_emgineering/prompt_engineering_prompt_chaining/#practice-prompts","title":"Practice Prompts","text":"<pre><code>prompt1 = \"List 3 tools used in data visualization.\"\nprompt2 = \"Choose one. Describe its pros/cons for beginners.\"\nprompt3 = \"Summarize your response in a tweet (under 280 chars).\"\n</code></pre>"},{"location":"prompt_emgineering/prompt_engineering_prompt_chaining/#-","title":"---","text":""},{"location":"prompt_emgineering/prompt_engineering_prompt_chaining/#active-learning-activity","title":"Active Learning Activity","text":"<p>Time: 30 minutes Group Size: 2\u20133 people Goal Alignment: Break down multi-part tasks into effective chained prompts Instructions: 1. As a group, pick a 3-part workflow (e.g., idea \u2192 pitch \u2192 tweet). 2. Divide steps so each person writes one prompt. 3. Pass each output to the next person as input. 4. Review the full chain at the end: Are transitions smooth? Is logic consistent? 5. Refine any step that breaks flow or returns unclear info. Rerun chain. Deliverables: - Prompt chain (3 steps) - Output after each step - Notes on what broke and what worked</p>"},{"location":"prompt_emgineering/prompt_engineering_prompt_chaining/#prompt-chaining-techniques-and-templates","title":"Prompt Chaining Techniques and Templates","text":"<p>Chain Types - Stepwise workflows - Output formatting at each stage - Data transformation chains - Role-switching per prompt - Creative expansion (idea \u2192 product \u2192 tagline)</p> <p>Prompt Chain Templates</p> <pre><code>Step 1: \u201cList 3 eco-friendly startup ideas.\u201d\nStep 2: \u201cPick the second one. Write a 150-word pitch with benefits and target audience.\u201d\nStep 3: \u201cSummarize that pitch as a tagline under 12 words.\u201d\n\n\u2014\n\nStep 1: \u201cExtract 5 key terms from this paragraph.\u201d\nStep 2: \u201cDefine each term in one sentence.\u201d\nStep 3: \u201cPut those definitions in a markdown list.\u201d\n\n\u2014\n\nStep 1: \u201cYou are a CEO. Explain your company\u2019s mission in 3 sentences.\u201d\nStep 2: \u201cYou are a PR manager. Rewrite that mission in a tone suitable for social media.\u201d\n</code></pre> <p>Variants - Output validation after step 2 - Role handoffs between steps - Insert user feedback mid-chain</p>"},{"location":"prompt_emgineering/prompt_engineering_role_persona/","title":"Prompt Engineering: Role and Persona Control","text":""},{"location":"prompt_emgineering/prompt_engineering_role_persona/#audience","title":"Audience","text":"<ul> <li>Users able to specify output structure and now learning how to apply personas for tailored responses</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_role_persona/#goal","title":"Goal","text":"<ul> <li>Teach tone management and role simulation to fit user needs</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_role_persona/#techniques-introduced","title":"Techniques Introduced","text":"<ul> <li>Impersonate a defined role (teacher, lawyer, support agent)</li> <li>Adjust tone: casual, professional, concise</li> <li>Combine with structured responses</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_role_persona/#prompt-evolution","title":"Prompt Evolution","text":"<p>Before:</p> <p>Give me tips on writing resumes.</p> <p>Now:</p> <p>Act as a senior recruiter. Give 3 resume tips to recent college grads in a friendly tone.</p>"},{"location":"prompt_emgineering/prompt_engineering_role_persona/#practice-prompts","title":"Practice Prompts","text":"<pre><code>prompt = \"Act as a UX researcher. List 3 feedback techniques for user interviews.\"\nprompt = \"You are a customer service trainer. Write an email template for handling delayed orders.\"\nprompt = \"As a career coach, write 3 tips for introverts preparing for interviews.\"\n</code></pre>"},{"location":"prompt_emgineering/prompt_engineering_role_persona/#-","title":"---","text":""},{"location":"prompt_emgineering/prompt_engineering_role_persona/#active-learning-activity","title":"Active Learning Activity","text":"<p>Time: 30 minutes Group Size: 2\u20134 people Goal Alignment: Explore how role prompts change tone, relevance, and information delivery Instructions: 1. Pick a use case (e.g., job advice, health tips, customer support). 2. Each person writes a prompt using a different persona (e.g., recruiter, doctor, support rep). 3. Run each prompt and compare tone, depth, and output relevance. 4. Pick the best output, then rewrite one weaker prompt to better fit the role. 5. Reflect as a group on how persona framing affects communication. Deliverables: - Use case and persona list - Output comparison table - Revised prompt and reflections</p>"},{"location":"prompt_emgineering/prompt_engineering_role_persona/#persona-based-prompting-techniques","title":"Persona-Based Prompting Techniques","text":"<p>Persona Types - Professionals: doctor, lawyer, software engineer - Educators: teacher, coach, mentor - Agents: support rep, chatbot, recruiter - Fictional: historical figures, aliens, AI with constraints</p> <p>Framing Methods - \u201cAct as a\u2026\u201d + role - Set tone: friendly, concise, formal - Combine role + output format</p> <p>Examples</p> <pre><code>\u201cAct as a resume coach. Give 3 bullet-point tips for someone changing careers.\u201d\n\u201cPretend you are a nutritionist. Write a short grocery list for a high-protein diet.\u201d\n\u201cYou are a high school teacher. Explain why studying history matters. Use simple terms.\u201d\n\u201cAct as a product manager writing a one-sentence summary of a failed feature rollout.\u201d\n</code></pre> <p>Prompt Expansion Ideas - Compare same prompt across roles - Rewrite using a humorous or sarcastic tone - Use two characters responding to each other (dialogue)</p>"},{"location":"prompt_emgineering/prompt_engineering_structured_prompting/","title":"Prompt Engineering: Structured Prompting","text":""},{"location":"prompt_emgineering/prompt_engineering_structured_prompting/#audience","title":"Audience","text":"<ul> <li>Users comfortable with simple prompts, ready to introduce format control and task clarity</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_structured_prompting/#goal","title":"Goal","text":"<ul> <li>Teach prompt structure through formatting, lists, markdown, and input scaffolding</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_structured_prompting/#techniques-introduced","title":"Techniques Introduced","text":"<ul> <li>Add word count or structure requirements</li> <li>Force markdown or HTML in response</li> <li>Use numbered steps to get ordered results</li> <li>Combine clarity and constraints</li> </ul>"},{"location":"prompt_emgineering/prompt_engineering_structured_prompting/#prompt-evolution","title":"Prompt Evolution","text":"<p>Before:</p> <p>Write a summary of AI in education.</p> <p>Now:</p> <p>Summarize AI in education in 5 bullet points using markdown. Focus on classroom applications.</p>"},{"location":"prompt_emgineering/prompt_engineering_structured_prompting/#practice-prompts","title":"Practice Prompts","text":"<pre><code>prompt = \"List 5 use cases of blockchain in logistics. Output in markdown bullets.\"\nprompt = \"Explain Git branching in 3 ordered steps.\"\nprompt = \"Write a 75-word paragraph on renewable energy. Make it suitable for high school students.\"\n</code></pre>"},{"location":"prompt_emgineering/prompt_engineering_structured_prompting/#-","title":"---","text":""},{"location":"prompt_emgineering/prompt_engineering_structured_prompting/#active-learning-activity","title":"Active Learning Activity","text":"<p>Time: 30 minutes Group Size: 3\u20134 people Goal Alignment: Practice formatting AI outputs using markdown, bullets, numbered steps, and concise summaries Instructions: 1. Form teams and assign each a topic (e.g., climate change, blockchain, healthy eating). 2. Each member drafts a prompt for the topic using a different structure (e.g., list, step-by-step, 100-word summary). 3. Run the prompts and review each output for format accuracy and clarity. 4. As a team, improve 1 prompt together and test the revision. 5. Discuss how format instructions affected the model output. Deliverables: - Topic and prompt variations - Original vs. revised outputs - Notes on which formats were best followed</p>"},{"location":"prompt_emgineering/prompt_engineering_structured_prompting/#structured-prompt-techniques-expanded-examples","title":"Structured Prompt Techniques: Expanded Examples","text":"<p>Techniques - Markdown formatting (lists, headers, bold text) - Tables with explicit structure - Word-limited sections - Numbered steps for how-to guides - Multi-block responses (summary + recommendations)</p> <p>Examples</p> <pre><code>\u201cList five AI tools.\u201d  \n\u2192 \u201cList five AI tools in markdown bullets. Include one use case per tool.\u201d\n\n\u201cExplain version control.\u201d  \n\u2192 \u201cExplain version control in three steps. Use numbered list format. Under 100 words total.\u201d\n\n\u201cGive a guide to installing Python.\u201d  \n\u2192 \u201cProvide a 3-step setup guide for installing Python 3.10 on macOS. Use markdown and include terminal commands.\u201d\n\n\u201cDescribe healthy snacks.\u201d  \n\u2192 \u201cList 5 healthy snacks in a markdown table with columns: Snack, Calories, Health Benefit.\u201d\n</code></pre> <p>Practice Prompts</p> <pre><code>prompt = \"List 3 common sorting algorithms in markdown bullet format with one-line descriptions.\"\nprompt = \"Create a table of 4 cloud services and their main features.\"\nprompt = \"Write a two-paragraph overview of cybersecurity best practices. Max 150 words total.\"\n</code></pre>"},{"location":"quantum/part0_orientation/","title":"Part 0: Orientation \u2014 Road-map &amp; Dev Setup","text":""},{"location":"quantum/part0_orientation/#objective","title":"Objective","text":"<p>Set up a collaborative quantum computing development environment and understand the roadmap for your quantum computing journey. By the end of this session, your team will have a functioning quantum computing environment and a clear understanding of the curriculum ahead.</p>"},{"location":"quantum/part0_orientation/#environment-setup","title":"Environment Setup","text":""},{"location":"quantum/part0_orientation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>Git for version control</li> <li>VS Code with Python extension or Jupyter Notebook</li> <li>Command line access</li> </ul>"},{"location":"quantum/part0_orientation/#installation-instructions","title":"Installation Instructions","text":"Command to run <code>pip install qiskit matplotlib numpy jupyter</code> <p>For Cirq users: | Command to run | | :---- | | <code>pip install cirq matplotlib numpy jupyter</code> |</p>"},{"location":"quantum/part0_orientation/#environment-verification","title":"Environment Verification","text":"<p>Test your quantum environment with this simple script:</p> <pre><code># For Qiskit\ntry:\n    import qiskit\n    from qiskit import QuantumCircuit\n    print(f\"Qiskit version: {qiskit.__version__}\")\n\n    # Create a simple quantum circuit\n    qc = QuantumCircuit(1, 1)\n    qc.h(0)  # Apply Hadamard gate\n    qc.measure(0, 0)  # Measure qubit 0 into classical bit 0\n\n    print(\"\u2705 Qiskit is correctly installed!\")\n    print(\"Circuit created:\")\n    print(qc.draw())\nexcept ImportError:\n    print(\"\u274c Qiskit not found. Please install with: pip install qiskit\")\n</code></pre> <p>Alternative verification for Cirq:</p> <pre><code># For Cirq\ntry:\n    import cirq\n    print(f\"Cirq version: {cirq.__version__}\")\n\n    # Create a simple quantum circuit\n    q0 = cirq.LineQubit(0)\n    circuit = cirq.Circuit(\n        cirq.H(q0),  # Apply Hadamard gate\n        cirq.measure(q0, key='m')  # Measure qubit\n    )\n\n    print(\"\u2705 Cirq is correctly installed!\")\n    print(\"Circuit created:\")\n    print(circuit)\nexcept ImportError:\n    print(\"\u274c Cirq not found. Please install with: pip install cirq\")\n</code></pre>"},{"location":"quantum/part0_orientation/#the-quantum-playground-roadmap","title":"The Quantum Playground Roadmap","text":""},{"location":"quantum/part0_orientation/#course-structure-overview","title":"Course Structure Overview","text":"<ol> <li> <p>Orientation &amp; Setup (You are here)</p> </li> <li> <p>Environment setup</p> </li> <li>Team formation</li> <li> <p>Understanding the learning journey</p> </li> <li> <p>Bits vs Qubits</p> </li> <li> <p>Classical computing review</p> </li> <li>Introduction to quantum states</li> <li> <p>Superposition concept</p> </li> <li> <p>Linear Algebra Foundations</p> </li> <li> <p>Vectors and matrices in quantum computing</p> </li> <li>Hands-on matrix manipulation</li> <li> <p>Quantum state representation</p> </li> <li> <p>Single-Qubit Operations</p> </li> <li> <p>Quantum gates visualization</p> </li> <li>Bloch sphere representation</li> <li> <p>Circuit building basics</p> </li> <li> <p>Measurement &amp; Probability</p> </li> <li> <p>Quantum measurement theory</p> </li> <li>Randomness and probability</li> <li> <p>Repeated execution analysis</p> </li> <li> <p>Multi-Qubit Systems</p> </li> <li> <p>Entanglement phenomena</p> </li> <li>Bell states creation</li> <li> <p>Multi-qubit circuits</p> </li> <li> <p>Quantum Algorithms</p> </li> <li> <p>Deutschh-Jozsa algorithm</p> </li> <li>Grover's search algorithm</li> <li> <p>Algorithm analysis techniques</p> </li> <li> <p>Error Correction &amp; Noise</p> </li> <li> <p>Real-world quantum limitations</p> </li> <li>Noise models</li> <li> <p>Error mitigation strategies</p> </li> <li> <p>Quantum Toolchain Exploration</p> </li> <li> <p>Vendor platforms comparison</p> </li> <li>Cloud quantum access</li> <li> <p>Framework differences</p> </li> <li> <p>Capstone Project</p> <ul> <li>End-to-end quantum application</li> <li>Team collaboration</li> <li>Results presentation</li> </ul> </li> </ol>"},{"location":"quantum/part0_orientation/#team-roles-collaboration","title":"Team Roles &amp; Collaboration","text":"<p>For each lab session, team members should rotate through the following roles:</p> <ol> <li>Quantum Developer: Writes the quantum circuit code</li> <li>Classical Developer: Implements supporting classical code</li> <li>Debugger: Tests code and identifies issues</li> <li>Analyst: Interprets results and leads discussions</li> <li>Presenter: Documents findings and prepares explanations</li> </ol>"},{"location":"quantum/part0_orientation/#collaboration-process","title":"Collaboration Process","text":"<ol> <li>Start: Read the lab document together, clarify team roles</li> <li>Implement: Work together on the coding challenges</li> <li>Checkpoint: Discuss insights at designated checkpoints</li> <li>Debug: Troubleshoot issues as a team</li> <li>Extend: Work on extension challenges if time permits</li> <li>Review: Present results and reflect on learnings</li> </ol>"},{"location":"quantum/part0_orientation/#discussion-topics","title":"Discussion Topics","text":"<p>Take time to discuss these questions with your team:</p> <ol> <li>What previous programming experience do team members have?</li> <li>What are your expectations for this quantum computing curriculum?</li> <li>What real-world applications of quantum computing interest your team most?</li> <li>How will you organize your collaborative coding sessions?</li> </ol>"},{"location":"quantum/part0_orientation/#collaboration-challenge","title":"Collaboration Challenge","text":""},{"location":"quantum/part0_orientation/#team-setup-exercise","title":"Team Setup Exercise","text":"<ol> <li>Create a shared repository for your team's quantum projects</li> <li>Each team member should clone the repository locally</li> <li>Create a hello-quantum.py file with the following structure:</li> </ol> <pre><code># TEAM MEMBERS: [List names here]\n# DATE: [Today's date]\n\n# Import quantum library (Qiskit or Cirq)\n# YOUR CODE HERE\n\ndef create_bell_pair():\n    \"\"\"\n    Creates a simple Bell pair (entangled qubits)\n    Returns the quantum circuit\n    \"\"\"\n    # YOUR CODE HERE\n    pass\n\ndef main():\n    # Initialize team member roles\n    team_roles = {\n        \"Quantum Developer\": \"[Name]\",\n        \"Classical Developer\": \"[Name]\",\n        \"Debugger\": \"[Name]\",\n        \"Analyst\": \"[Name]\",\n        \"Presenter\": \"[Name]\"\n    }\n\n    print(\"Quantum Playground - Team Setup\")\n    print(\"================================\")\n    print(\"Team Roles:\")\n    for role, name in team_roles.items():\n        print(f\"- {role}: {name}\")\n\n    # Create and display a simple quantum circuit\n    # YOUR CODE HERE\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <ol> <li>Complete the code as a team</li> <li>Commit and push your changes</li> <li>Verify everyone can run the same code</li> </ol>"},{"location":"quantum/part0_orientation/#whats-next","title":"What's Next?","text":"<p>In the next session, we'll dive into comparing classical bits and quantum qubits, exploring the fundamental differences that make quantum computing unique.</p>"},{"location":"quantum/part0_orientation/#extension-ibm-quantum-experience","title":"Extension: IBM Quantum Experience","text":"<p>For those interested in accessing real quantum hardware:</p> <ol> <li>Create an IBM Quantum account at quantum-computing.ibm.com</li> <li>Setup your IBM Quantum API token with Qiskit</li> <li>Explore the available quantum processors</li> </ol>"},{"location":"quantum/part0_orientation/#troubleshooting-tips","title":"Troubleshooting Tips","text":"<ul> <li>ImportError: Ensure your Python environment has the correct packages installed</li> <li>Version conflicts: Create a dedicated virtual environment for quantum computing</li> <li>Visualization issues: Make sure matplotlib is correctly installed</li> <li>Jupyter notebook problems: Try running with <code>jupyter notebook --NotebookApp.iopub_data_rate_limit=1e10</code></li> </ul>"},{"location":"quantum/part1_bits_vs_qubits/","title":"Part 1: Bits vs Qubits \u2014 Booleans on Steroids","text":""},{"location":"quantum/part1_bits_vs_qubits/#objective","title":"Objective","text":"<p>Compare classical bits with quantum qubits to understand the fundamental differences that enable quantum computing's unique capabilities. By the end of this session, your team will understand superposition, visualize quantum states, and implement basic quantum operations that have no classical equivalent.</p>"},{"location":"quantum/part1_bits_vs_qubits/#core-concepts","title":"Core Concepts","text":""},{"location":"quantum/part1_bits_vs_qubits/#classical-bits-vs-quantum-qubits","title":"Classical Bits vs Quantum Qubits","text":"Classical Bit Quantum Qubit Can be 0 OR 1 Can be in superposition of 0 AND 1 Deterministic Probabilistic Directly observable Collapses upon measurement Binary logic Complex amplitude logic Can represent 1 state at a time Can represent 2^n states with n qubits"},{"location":"quantum/part1_bits_vs_qubits/#quantum-state-notation","title":"Quantum State Notation","text":"<p>Dirac (Bra-Ket) Notation:</p> <ul> <li>State \"0\" is represented as: |0\u27e9</li> <li>State \"1\" is represented as: |1\u27e9</li> <li>Superposition: \u03b1|0\u27e9 + \u03b2|1\u27e9, where |\u03b1|\u00b2 + |\u03b2|\u00b2 = 1</li> </ul>"},{"location":"quantum/part1_bits_vs_qubits/#visual-explanation","title":"Visual Explanation","text":"<p>A classical bit is like a coin showing either heads or tails, while a qubit is like a spinning coin\u2014it has some probability of being heads and some probability of being tails until observed.</p>"},{"location":"quantum/part1_bits_vs_qubits/#environment-setup","title":"Environment Setup","text":"<p>Ensure you have the necessary packages installed:</p> Command to run <code>pip install qiskit matplotlib numpy</code> <p>For Cirq users: | Command to run | | :---- | | <code>pip install cirq matplotlib numpy</code> |</p>"},{"location":"quantum/part1_bits_vs_qubits/#starter-code","title":"Starter Code","text":"<p>Create a new Python file called <code>bit_vs_qubit.py</code> with the following structure:</p> <pre><code># TEAM MEMBERS: [List names here]\n# DATE: [Today's date]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\n\n# Choose your framework\nUSE_QISKIT = True  # Set to False if using Cirq\n\nif USE_QISKIT:\n    from qiskit import QuantumCircuit, transpile, Aer, assemble\n    from qiskit.visualization import plot_bloch_vector, plot_histogram\nelse:\n    import cirq\n    from cirq.sim.density_matrix_simulator import DensityMatrixSimulator\n\ndef classical_bit_operations():\n    \"\"\"Demonstrate classical bit operations\"\"\"\n    # Initialize a classical bit (0 or 1)\n    bit = 0\n    print(f\"Initial bit value: {bit}\")\n\n    # Classical NOT operation\n    bit = 1 - bit\n    print(f\"After NOT: {bit}\")\n\n    # Classical AND operation\n    bit2 = 1\n    bit_and = bit &amp; bit2\n    print(f\"AND operation: {bit} &amp; {bit2} = {bit_and}\")\n\n    # Classical OR operation\n    bit_or = bit | bit2\n    print(f\"OR operation: {bit} | {bit2} = {bit_or}\")\n\n    # Classical XOR operation\n    bit_xor = bit ^ bit2\n    print(f\"XOR operation: {bit} ^ {bit2} = {bit_xor}\")\n\n    # CHALLENGE: Can we represent multiple states simultaneously with classical bits?\n    # YOUR ANSWER HERE\n\ndef qubit_operations_qiskit():\n    \"\"\"Demonstrate qubit operations using Qiskit\"\"\"\n    # CREATE QUANTUM CIRCUIT\n    # YOUR CODE HERE: Create a circuit with 1 qubit and 1 classical bit\n\n    # Initial state is |0\u27e9\n    # YOUR CODE HERE: Print or visualize the initial state\n\n    # Apply Hadamard gate to create superposition\n    # YOUR CODE HERE: Add H gate and visualize superposition\n\n    # Measure the qubit\n    # YOUR CODE HERE: Add measurement operation\n\n    # Simulate the circuit\n    # YOUR CODE HERE: Run the circuit on a simulator and get counts\n\n    # CHALLENGE: What happens if you measure multiple times?\n    # YOUR CODE HERE\n\ndef qubit_operations_cirq():\n    \"\"\"Demonstrate qubit operations using Cirq\"\"\"\n    # Initial qubit\n    q = cirq.LineQubit(0)\n\n    # Initial state |0\u27e9\n    circuit = cirq.Circuit()\n\n    # YOUR CODE HERE: Create superposition with Hadamard\n\n    # YOUR CODE HERE: Measure the qubit\n\n    # YOUR CODE HERE: Simulate and print results\n\n    # CHALLENGE: What happens if you measure multiple times?\n    # YOUR CODE HERE\n\ndef visualize_qubit_vs_bit():\n    \"\"\"Visualize the difference between a bit and qubit\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n    # Classical bit visualization (simple binary state)\n    ax1.set_title('Classical Bit')\n    ax1.set_xlim(-1.5, 1.5)\n    ax1.set_ylim(-1.5, 1.5)\n\n    # Draw a simple binary representation\n    circle0 = Circle((-0.5, 0), 0.4, color='blue', alpha=0.7)\n    circle1 = Circle((0.5, 0), 0.4, color='red', alpha=0.2)  # Dimmed to show it's not active\n    ax1.add_patch(circle0)\n    ax1.add_patch(circle1)\n    ax1.text(-0.5, 0, \"0\", ha='center', va='center', color='white')\n    ax1.text(0.5, 0, \"1\", ha='center', va='center')\n    ax1.text(0, -1, \"The bit is definitely in state |0\u27e9\", ha='center')\n\n    # Qubit visualization (probability distribution)\n    ax2.set_title('Quantum Qubit (after Hadamard)')\n    ax2.set_xlim(-1.5, 1.5)\n    ax2.set_ylim(-1.5, 1.5)\n\n    # Draw a superposition representation\n    circle0 = Circle((-0.5, 0), 0.4, color='blue', alpha=0.5)\n    circle1 = Circle((0.5, 0), 0.4, color='red', alpha=0.5)\n    ax2.add_patch(circle0)\n    ax2.add_patch(circle1)\n    ax2.text(-0.5, 0, \"0\", ha='center', va='center')\n    ax2.text(0.5, 0, \"1\", ha='center', va='center')\n    ax2.text(0, -1, \"The qubit has 50% probability of being |0\u27e9 or |1\u27e9\", ha='center')\n\n    plt.tight_layout()\n    plt.savefig(\"bit_vs_qubit.png\")\n    plt.show()\n\n    # CHALLENGE: Modify this function to show different probability distributions\n    # YOUR CODE HERE\n\ndef main():\n    print(\"CLASSICAL BIT OPERATIONS\")\n    print(\"========================\")\n    classical_bit_operations()\n\n    print(\"\\nQUANTUM QUBIT OPERATIONS\")\n    print(\"========================\")\n    if USE_QISKIT:\n        qubit_operations_qiskit()\n    else:\n        qubit_operations_cirq()\n\n    print(\"\\nVISUALIZING DIFFERENCES\")\n    print(\"========================\")\n    visualize_qubit_vs_bit()\n\n    # TEAM DISCUSSION POINT:\n    # What operations can you perform with qubits that are impossible with classical bits?\n    # YOUR DISCUSSION NOTES HERE\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"quantum/part1_bits_vs_qubits/#collaborative-challenge-implement-the-missing-sections","title":"Collaborative Challenge: Implement the Missing Sections","text":"<p>Working together as a team, your challenge is to:</p> <ol> <li>Complete all sections marked with <code># YOUR CODE HERE</code></li> <li>Implement the <code>qubit_operations_qiskit()</code> or <code>qubit_operations_cirq()</code> function (based on your chosen framework)</li> <li>Enhance the <code>visualize_qubit_vs_bit()</code> function to show different quantum states</li> <li>Discuss and document your answers to the challenge questions</li> </ol>"},{"location":"quantum/part1_bits_vs_qubits/#discussion-checkpoints","title":"Discussion Checkpoints","text":""},{"location":"quantum/part1_bits_vs_qubits/#checkpoint-1-after-classical-bit-operations","title":"Checkpoint 1: After Classical Bit Operations","text":"<ul> <li>Can we represent multiple states simultaneously with classical bits?</li> <li>What makes quantum superposition fundamentally different from classical probability?</li> </ul>"},{"location":"quantum/part1_bits_vs_qubits/#checkpoint-2-after-implementing-quantum-operations","title":"Checkpoint 2: After Implementing Quantum Operations","text":"<ul> <li>What happens when you measure a qubit in superposition?</li> <li>Why does quantum require complex numbers for amplitudes, not just probabilities?</li> </ul>"},{"location":"quantum/part1_bits_vs_qubits/#checkpoint-3-after-visualization","title":"Checkpoint 3: After Visualization","text":"<ul> <li>How does visualizing qubits help understand quantum behavior?</li> <li>What information is lost when a quantum state is measured?</li> </ul>"},{"location":"quantum/part1_bits_vs_qubits/#debugging-walkthrough","title":"Debugging Walkthrough","text":"<p>Common issues and their solutions:</p> <ol> <li> <p>Problem: Quantum circuit doesn't show expected probabilities    Solution: Check if the Hadamard gate was properly applied and that you're using the correct simulator backend</p> </li> <li> <p>Problem: Visualization doesn't display properly    Solution: Ensure matplotlib is working by testing with a simple plot first; check for correct array dimensions</p> </li> <li> <p>Problem: Getting errors with complex numbers    Solution: Remember that quantum amplitudes are complex numbers; use numpy's complex number support</p> </li> </ol>"},{"location":"quantum/part1_bits_vs_qubits/#extension-challenge-bloch-sphere-representation","title":"Extension Challenge: Bloch Sphere Representation","text":"<p>For a deeper understanding, implement a function to visualize qubits on the Bloch sphere:</p> <pre><code>def bloch_sphere_visualization():\n    \"\"\"Visualize qubit states on the Bloch sphere\"\"\"\n    # YOUR CODE HERE\n\n    # For Qiskit:\n    # from qiskit.visualization import plot_bloch_vector\n    # plot_bloch_vector([x, y, z])\n\n    # For Cirq:\n    # You'll need to use matplotlib to create a 3D plot\n</code></pre>"},{"location":"quantum/part1_bits_vs_qubits/#what-would-break-this","title":"What Would Break This?","text":"<p>Discuss with your team:</p> <ol> <li>What happens if we try to create a superposition state with \u03b1|0\u27e9 + \u03b2|1\u27e9 where |\u03b1|\u00b2 + |\u03b2|\u00b2 \u2260 1?</li> <li>Can we clone an unknown quantum state? Why or why not?</li> <li>What happens if we try to directly print the state of a qubit without measuring?</li> </ol>"},{"location":"quantum/part1_bits_vs_qubits/#capstone-group-task","title":"Capstone Group Task","text":"<p>Design and implement a quantum coin flip simulator that:</p> <ol> <li>Creates a \"fair\" quantum coin (50/50 chance)</li> <li>Creates a \"biased\" quantum coin (with configurable bias)</li> <li>Compares results of multiple flips between classical and quantum coins</li> <li>Visualizes the results with histograms</li> </ol>"},{"location":"quantum/part1_bits_vs_qubits/#how-would-this-work-on-real-hardware","title":"How Would This Work on Real Hardware?","text":"<p>If you were to run these experiments on real quantum hardware:</p> <ol> <li>How would noise affect your superposition states?</li> <li>How many shots (repetitions) would you need for accurate statistics?</li> <li>Would gate errors make it difficult to distinguish between intended superposition and errors?</li> </ol>"},{"location":"quantum/part1_bits_vs_qubits/#next-steps","title":"Next Steps","text":"<p>In the next session, we'll explore the essential linear algebra concepts needed to work effectively with quantum states and operations.</p>"},{"location":"quantum/part2_linear_algebra/","title":"Part 2: Linear Algebra Survival Kit","text":""},{"location":"quantum/part2_linear_algebra/#objective","title":"Objective","text":"<p>Master the essential linear algebra operations that form the mathematical foundation of quantum computing. By the end of this session, your team will be able to represent quantum states as vectors, understand how quantum operations work as matrices, and apply these concepts to predict the behavior of simple quantum systems.</p>"},{"location":"quantum/part2_linear_algebra/#core-concepts","title":"Core Concepts","text":""},{"location":"quantum/part2_linear_algebra/#vectors-matrices-and-quantum-states","title":"Vectors, Matrices and Quantum States","text":"<p>In quantum computing, we represent:</p> <ul> <li>Quantum states as vectors (using ket notation |\u03c8\u27e9)</li> <li>Quantum operations as matrices (unitary matrices)</li> <li>Measurements as projections of vectors</li> </ul>"},{"location":"quantum/part2_linear_algebra/#key-linear-algebra-operations","title":"Key Linear Algebra Operations","text":"Operation Mathematical Form Quantum Computing Application Vector addition v\u20d7 + w\u20d7 Superposition of states Matrix-vector multiplication Mv\u20d7 Applying quantum gates Matrix multiplication MN Combining quantum operations Inner product \u27e8v\u20d7,w\u20d7\u27e9 Calculating measurement probabilities Tensor product v\u20d7 \u2297 w\u20d7 Combining quantum systems"},{"location":"quantum/part2_linear_algebra/#visual-explanation","title":"Visual Explanation","text":"<p>Quantum states live in a complex vector space called Hilbert space. A qubit's state can be visualized as a point on the Bloch sphere, with quantum operations rotating this point.</p>"},{"location":"quantum/part2_linear_algebra/#environment-setup","title":"Environment Setup","text":"<p>Ensure you have the necessary packages installed:</p> Command to run <code>pip install numpy scipy matplotlib</code>"},{"location":"quantum/part2_linear_algebra/#starter-code","title":"Starter Code","text":"<p>Create a new Python file called <code>quantum_linear_algebra.py</code> with the following structure:</p> <pre><code># TEAM MEMBERS: [List names here]\n# DATE: [Today's date]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Quantum states in computational basis\n# |0\u27e9 and |1\u27e9 as column vectors\nket_0 = np.array([[1], [0]], dtype=complex)\nket_1 = np.array([[0], [1]], dtype=complex)\n\n# Common quantum gates as matrices\nI = np.array([[1, 0], [0, 1]], dtype=complex)  # Identity\nX = np.array([[0, 1], [1, 0]], dtype=complex)  # Pauli-X (NOT)\nZ = np.array([[1, 0], [0, -1]], dtype=complex)  # Pauli-Z\nH = (1/np.sqrt(2)) * np.array([[1, 1], [1, -1]], dtype=complex)  # Hadamard\n\ndef print_state(state, label=\"State\"):\n    \"\"\"Pretty print a quantum state vector\"\"\"\n    print(f\"{label}:\")\n    # Complex numbers are displayed as (real, imag)\n    # YOUR CODE HERE: Format and display the state vector\n\ndef apply_gate(gate, state):\n    \"\"\"Apply a quantum gate to a state vector\"\"\"\n    # YOUR CODE HERE: Implement matrix-vector multiplication\n    pass\n\ndef vector_inner_product(state1, state2):\n    \"\"\"Calculate the inner product between two state vectors\"\"\"\n    # YOUR CODE HERE: Implement inner product\n    # Hint: For complex vectors, we need the complex conjugate\n    pass\n\ndef state_to_bloch(state):\n    \"\"\"Convert a qubit state vector to Bloch sphere coordinates\"\"\"\n    # A pure state |\u03c8\u27e9 = a|0\u27e9 + b|1\u27e9 maps to:\n    # x = 2*Re(a*b*)\n    # y = 2*Im(a*b*)\n    # z = |a|^2 - |b|^2\n\n    # YOUR CODE HERE: Calculate Bloch coordinates\n    pass\n\ndef plot_bloch_vector(bloch_coords, title=\"Bloch Sphere Representation\"):\n    \"\"\"Plot a state on the Bloch sphere\"\"\"\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Draw the Bloch sphere\n    u, v = np.mgrid[0:2*np.pi:20j, 0:np.pi:10j]\n    x = np.sin(v) * np.cos(u)\n    y = np.sin(v) * np.sin(u)\n    z = np.cos(v)\n    ax.plot_wireframe(x, y, z, color=\"gray\", alpha=0.2)\n\n    # Draw the axes\n    ax.plot([-1, 1], [0, 0], [0, 0], 'k-', alpha=0.5, lw=1)  # x-axis\n    ax.plot([0, 0], [-1, 1], [0, 0], 'k-', alpha=0.5, lw=1)  # y-axis\n    ax.plot([0, 0], [0, 0], [-1, 1], 'k-', alpha=0.5, lw=1)  # z-axis\n\n    # Add basis states\n    ax.text(0, 0, 1.1, r'$|0\\rangle$')\n    ax.text(0, 0, -1.1, r'$|1\\rangle$')\n\n    # Plot the input Bloch vector\n    x, y, z = bloch_coords\n    ax.plot([0, x], [0, y], [0, z], 'r-', lw=2)\n    ax.plot([x], [y], [z], 'ro', markersize=10)\n\n    # Set labels and title\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_title(title)\n\n    # Set the limits\n    ax.set_xlim([-1.2, 1.2])\n    ax.set_ylim([-1.2, 1.2])\n    ax.set_zlim([-1.2, 1.2])\n\n    plt.tight_layout()\n    return fig\n\ndef measure_probability(state):\n    \"\"\"Calculate the measurement probabilities for a quantum state\"\"\"\n    # YOUR CODE HERE: Calculate probability of measuring |0\u27e9 and |1\u27e9\n    pass\n\ndef tensor_product_demo():\n    \"\"\"Demonstrate tensor product to combine quantum systems\"\"\"\n    # YOUR CODE HERE: Implement and explain tensor products\n    pass\n\ndef main():\n    print(\"QUANTUM STATE VECTORS\")\n    print(\"====================\")\n    print_state(ket_0, \"Basis state |0\u27e9\")\n    print_state(ket_1, \"Basis state |1\u27e9\")\n\n    # Create a superposition state\n    print(\"\\nCREATING SUPERPOSITION\")\n    print(\"=====================\")\n\n    # YOUR CODE HERE: Create and print a superposition state\n    # Hint: apply_gate(H, ket_0) creates |+\u27e9 state\n\n    # Team Exercise: Create and visualize different states\n    # 1. Create the |+\u27e9 state\n    # YOUR CODE HERE\n\n    # 2. Create the |-\u27e9 state\n    # YOUR CODE HERE\n\n    # 3. Create the |i\u27e9 state (hint: involves complex numbers)\n    # YOUR CODE HERE\n\n    print(\"\\nQUANTUM GATE OPERATIONS\")\n    print(\"======================\")\n\n    # YOUR CODE HERE: Apply different gates and observe results\n\n    print(\"\\nMEASUREMENT PROBABILITIES\")\n    print(\"========================\")\n\n    # YOUR CODE HERE: Calculate measurement probabilities\n\n    print(\"\\nCOMBINING QUANTUM SYSTEMS\")\n    print(\"========================\")\n    tensor_product_demo()\n\n    # TEAM DISCUSSION POINT:\n    # How does quantum state representation differ from classical state?\n    # YOUR DISCUSSION NOTES HERE\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"quantum/part2_linear_algebra/#collaborative-challenge-implement-the-linear-algebra-functions","title":"Collaborative Challenge: Implement the Linear Algebra Functions","text":"<p>Working together as a team, your challenge is to:</p> <ol> <li>Implement all functions marked with <code># YOUR CODE HERE</code></li> <li>Create and visualize various quantum states on the Bloch sphere</li> <li>Verify your understanding by calculating expected probabilities</li> </ol>"},{"location":"quantum/part2_linear_algebra/#team-roles-for-this-exercise","title":"Team Roles for this Exercise","text":"<p>For this lab, consider the following role assignments:</p> <ol> <li>Matrix Operations Developer: Implements core matrix functions</li> <li>Visualization Specialist: Focuses on Bloch sphere visualization</li> <li>Verification Analyst: Tests functions with known examples</li> <li>Documentation Lead: Explains the mathematics in comments</li> <li>Extension Developer: Works on additional features or optimizations</li> </ol>"},{"location":"quantum/part2_linear_algebra/#discussion-checkpoints","title":"Discussion Checkpoints","text":""},{"location":"quantum/part2_linear_algebra/#checkpoint-1-after-state-vectors","title":"Checkpoint 1: After State Vectors","text":"<ul> <li>How do quantum state vectors differ from classical states?</li> <li>What is the significance of complex numbers in quantum computing?</li> </ul>"},{"location":"quantum/part2_linear_algebra/#checkpoint-2-after-gate-operations","title":"Checkpoint 2: After Gate Operations","text":"<ul> <li>Why must quantum gates be unitary matrices?</li> <li>How does the Hadamard gate create superposition?</li> </ul>"},{"location":"quantum/part2_linear_algebra/#checkpoint-3-after-measurement","title":"Checkpoint 3: After Measurement","text":"<ul> <li>How do we calculate the probability of measuring a specific outcome?</li> <li>Why is quantum measurement probabilistic rather than deterministic?</li> </ul>"},{"location":"quantum/part2_linear_algebra/#debugging-walkthrough","title":"Debugging Walkthrough","text":"<p>Common issues and their solutions:</p> <ol> <li> <p>Problem: Matrices or vectors have incorrect dimensions    Solution: Double-check that you're using column vectors (shape n\u00d71) not row vectors</p> </li> <li> <p>Problem: Getting complex numbers when expecting real results    Solution: For some values like probabilities, you may need to take the absolute square: <code>np.abs(value)**2</code></p> </li> <li> <p>Problem: Bloch sphere visualization appears distorted    Solution: Ensure your state vector is normalized, and Bloch coordinates are calculated correctly</p> </li> </ol>"},{"location":"quantum/part2_linear_algebra/#extension-challenge-quantum-circuits-with-linear-algebra","title":"Extension Challenge: Quantum Circuits with Linear Algebra","text":"<p>Implement a simple quantum circuit simulator using only linear algebra operations:</p> <pre><code>def simulate_circuit(gates, initial_state=None):\n    \"\"\"\n    Simulate a quantum circuit using linear algebra\n\n    Args:\n        gates: List of tuples (gate, qubit_indices) for multi-qubit systems\n        initial_state: Starting state vector, defaults to |0...0\u27e9\n\n    Returns:\n        Final state vector after applying all gates\n    \"\"\"\n    # YOUR CODE HERE\n    pass\n</code></pre>"},{"location":"quantum/part2_linear_algebra/#what-would-break-this","title":"What Would Break This?","text":"<p>Discuss with your team:</p> <ol> <li>What happens if your quantum state vector is not normalized?</li> <li>Is every 2\u00d72 matrix a valid quantum operation? Why or why not?</li> <li>How would errors in matrix multiplication affect quantum simulations?</li> </ol>"},{"location":"quantum/part2_linear_algebra/#capstone-group-task","title":"Capstone Group Task","text":"<p>Develop a quantum \"memory\" game that challenges players to:</p> <ol> <li>Observe a quantum state's Bloch sphere representation</li> <li>Identify which quantum gates (H, X, Z) were applied to the |0\u27e9 state</li> <li>Calculate the correct measurement probabilities</li> </ol> <p>Each team member should contribute a different puzzle for others to solve.</p>"},{"location":"quantum/part2_linear_algebra/#how-would-this-work-on-real-hardware","title":"How Would This Work on Real Hardware?","text":"<p>In real quantum computers:</p> <ol> <li>How does the mathematical formalism of state vectors map to physical qubits?</li> <li>How do sources of noise and decoherence affect the linear algebra description?</li> <li>Why can't we directly measure the quantum state vector of a physical qubit?</li> </ol>"},{"location":"quantum/part2_linear_algebra/#next-steps","title":"Next Steps","text":"<p>In the next session, we'll explore single-qubit operations in more detail, focusing on implementing and visualizing various quantum gates.</p>"},{"location":"quantum/part3_single_qubit_operations/","title":"Part 3: Single-Qubit Operations","text":""},{"location":"quantum/part3_single_qubit_operations/#objective","title":"Objective","text":"<p>Master the fundamental quantum gates that operate on single qubits. By the end of this session, your team will understand the geometric and algebraic interpretation of common quantum gates, implement gate sequences in code, and visualize their effects on the Bloch sphere.</p>"},{"location":"quantum/part3_single_qubit_operations/#core-concepts","title":"Core Concepts","text":""},{"location":"quantum/part3_single_qubit_operations/#single-qubit-gates","title":"Single-Qubit Gates","text":"Gate Matrix Representation Effect on Qubit Classical Analog Pauli-X \\begin{pmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{pmatrix} Bit flip (0\u21941) NOT gate Pauli-Y \\begin{pmatrix} 0 &amp; -i \\\\ i &amp; 0 \\end{pmatrix} Bit + phase flip None Pauli-Z \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix} Phase flip None Hadamard \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; -1 \\end{pmatrix} Creates superposition None S (Phase) \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; i \\end{pmatrix} \u03c0/2 phase rotation None T \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; e^{i\\pi/4} \\end{pmatrix} \u03c0/4 phase rotation None"},{"location":"quantum/part3_single_qubit_operations/#gate-sequences-and-composition","title":"Gate Sequences and Composition","text":"<p>Quantum gates can be applied in sequence to create more complex operations:</p> <ul> <li>Gates are applied from right to left in matrix notation</li> <li>Matrix multiplication represents sequential application</li> <li>Identity property: IX = XI = X</li> </ul>"},{"location":"quantum/part3_single_qubit_operations/#visual-explanation","title":"Visual Explanation","text":"<p>Single-qubit gates can be understood as rotations on the Bloch sphere:</p> <ul> <li>X gate: 180\u00b0 rotation around the x-axis</li> <li>Y gate: 180\u00b0 rotation around the y-axis</li> <li>Z gate: 180\u00b0 rotation around the z-axis</li> <li>Hadamard: 180\u00b0 rotation around the x+z axis</li> </ul>"},{"location":"quantum/part3_single_qubit_operations/#environment-setup","title":"Environment Setup","text":"<p>Ensure you have the necessary packages installed:</p> Command to run <code>pip install qiskit matplotlib numpy jupyter seaborn</code> <p>For Cirq users: | Command to run | | :---- | | <code>pip install cirq matplotlib numpy jupyter seaborn</code> |</p>"},{"location":"quantum/part3_single_qubit_operations/#starter-code","title":"Starter Code","text":"<p>Create a new Python file called <code>single_qubit_gates.py</code> with the following structure:</p> <pre><code># TEAM MEMBERS: [List names here]\n# DATE: [Today's date]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import FancyArrowPatch\nfrom mpl_toolkits.mplot3d import proj3d\nimport matplotlib.animation as animation\n\n# Choose your framework\nUSE_QISKIT = True  # Set to False if using Cirq\n\nif USE_QISKIT:\n    from qiskit import QuantumCircuit, execute, Aer\n    from qiskit.visualization import plot_bloch_vector, plot_histogram, plot_bloch_multivector\n    from qiskit.quantum_info import Statevector\nelse:\n    import cirq\n    from cirq.sim.density_matrix_simulator import DensityMatrixSimulator\n\nclass Arrow3D(FancyArrowPatch):\n    \"\"\"Custom 3D arrow for visualization\"\"\"\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        super().__init__((0,0), (0,0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def do_3d_projection(self, renderer=None):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M)\n        self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n        return np.min(zs)\n\ndef setup_bloch_sphere(ax, title=\"Bloch Sphere\"):\n    \"\"\"Set up Bloch sphere visualization\"\"\"\n    # YOUR CODE HERE: Draw Bloch sphere wireframe\n    # Hint: Use sphere equations x^2 + y^2 + z^2 = 1\n\n    # Add basis state labels\n    ax.text(0, 0, 1.1, r'$|0\\rangle$')\n    ax.text(0, 0, -1.1, r'$|1\\rangle$')\n    ax.text(1.1, 0, 0, r'$|+\\rangle$')\n    ax.text(-1.1, 0, 0, r'$|-\\rangle$')\n    ax.text(0, 1.1, 0, r'$|i\\rangle$')\n    ax.text(0, -1.1, 0, r'$|-i\\rangle$')\n\n    # Set labels and title\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_title(title)\n\n    # Set the limits\n    ax.set_xlim([-1.2, 1.2])\n    ax.set_ylim([-1.2, 1.2])\n    ax.set_zlim([-1.2, 1.2])\n\n    return ax\n\ndef state_to_bloch(state_vector):\n    \"\"\"Convert state vector to Bloch sphere coordinates\"\"\"\n    # For a state |\u03c8\u27e9 = a|0\u27e9 + b|1\u27e9:\n    # x = 2*Re(a*b*)\n    # y = 2*Im(a*b*)\n    # z = |a|^2 - |b|^2\n\n    # YOUR CODE HERE: Calculate Bloch coordinates\n    # Return [x, y, z]\n    pass\n\ndef apply_x_gate_qiskit():\n    \"\"\"Demonstrate X gate with Qiskit\"\"\"\n    # YOUR CODE HERE: Create circuit with X gate\n    # Visualize before and after states\n    pass\n\ndef apply_z_gate_qiskit():\n    \"\"\"Demonstrate Z gate with Qiskit\"\"\"\n    # YOUR CODE HERE: Create circuit with Z gate\n    # Visualize before and after states\n    pass\n\ndef apply_h_gate_qiskit():\n    \"\"\"Demonstrate Hadamard gate with Qiskit\"\"\"\n    # YOUR CODE HERE: Create circuit with H gate\n    # Visualize before and after states\n    pass\n\ndef apply_x_gate_cirq():\n    \"\"\"Demonstrate X gate with Cirq\"\"\"\n    # YOUR CODE HERE: Create circuit with X gate\n    # Visualize before and after states\n    pass\n\ndef apply_z_gate_cirq():\n    \"\"\"Demonstrate Z gate with Cirq\"\"\"\n    # YOUR CODE HERE: Create circuit with Z gate\n    # Visualize before and after states\n    pass\n\ndef apply_h_gate_cirq():\n    \"\"\"Demonstrate Hadamard gate with Cirq\"\"\"\n    # YOUR CODE HERE: Create circuit with H gate\n    # Visualize before and after states\n    pass\n\ndef gate_sequence_visualization():\n    \"\"\"Visualize a sequence of gates applied to a qubit\"\"\"\n    # Animation of gate sequence: H \u2192 X \u2192 Z\n    # YOUR CODE HERE: Create an animation of gate sequence\n    pass\n\ndef implement_arbitrary_rotation():\n    \"\"\"Implement an arbitrary rotation on Bloch sphere\"\"\"\n    # YOUR CODE HERE: Implement Rx, Ry, Rz rotations\n    # Show how to achieve any rotation\n    pass\n\ndef main():\n    print(\"SINGLE-QUBIT QUANTUM GATES\")\n    print(\"=========================\")\n\n    print(\"\\nX GATE (QUANTUM NOT)\")\n    print(\"===================\")\n    if USE_QISKIT:\n        apply_x_gate_qiskit()\n    else:\n        apply_x_gate_cirq()\n\n    print(\"\\nZ GATE (PHASE FLIP)\")\n    print(\"===================\")\n    if USE_QISKIT:\n        apply_z_gate_qiskit()\n    else:\n        apply_z_gate_cirq()\n\n    print(\"\\nHADAMARD GATE (SUPERPOSITION)\")\n    print(\"=============================\")\n    if USE_QISKIT:\n        apply_h_gate_qiskit()\n    else:\n        apply_h_gate_cirq()\n\n    print(\"\\nGATE SEQUENCE VISUALIZATION\")\n    print(\"===========================\")\n    gate_sequence_visualization()\n\n    print(\"\\nARBITRARY ROTATIONS\")\n    print(\"===================\")\n    implement_arbitrary_rotation()\n\n    # TEAM DISCUSSION POINT:\n    # How do quantum gates differ from classical gates?\n    # YOUR DISCUSSION NOTES HERE\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"quantum/part3_single_qubit_operations/#collaborative-challenge-implement-gate-visualizations","title":"Collaborative Challenge: Implement Gate Visualizations","text":"<p>Working together as a team, your challenge is to:</p> <ol> <li>Complete the functions marked with <code># YOUR CODE HERE</code></li> <li>Implement and visualize the effect of X, Z, and H gates</li> <li>Create a visualization of a gate sequence (H \u2192 X \u2192 Z)</li> <li>Implement arbitrary rotations (Rx, Ry, Rz gates)</li> </ol>"},{"location":"quantum/part3_single_qubit_operations/#team-roles-for-this-exercise","title":"Team Roles for This Exercise","text":"<p>For this lab, consider the following role assignments:</p> <ol> <li>Quantum Gate Implementer: Focuses on implementing gate operations</li> <li>Visualization Developer: Works on Bloch sphere visualizations</li> <li>Animation Specialist: Creates gate sequence animation</li> <li>Verification Tester: Tests gate implementations against expected outcomes</li> <li>Documentation Lead: Explains the gate effects and their implications</li> </ol>"},{"location":"quantum/part3_single_qubit_operations/#discussion-checkpoints","title":"Discussion Checkpoints","text":""},{"location":"quantum/part3_single_qubit_operations/#checkpoint-1-after-implementing-basic-gates","title":"Checkpoint 1: After Implementing Basic Gates","text":"<ul> <li>How does the X gate compare to a classical NOT gate?</li> <li>What special property does the Hadamard gate have?</li> <li>Why are there no classical equivalents to Z, S and T gates?</li> </ul>"},{"location":"quantum/part3_single_qubit_operations/#checkpoint-2-after-gate-sequence-visualization","title":"Checkpoint 2: After Gate Sequence Visualization","text":"<ul> <li>Is the order of gate application important? Why?</li> <li>Can all single-qubit gates be composed from a smaller set?</li> <li>What does it mean that gates are unitary?</li> </ul>"},{"location":"quantum/part3_single_qubit_operations/#checkpoint-3-after-arbitrary-rotations","title":"Checkpoint 3: After Arbitrary Rotations","text":"<ul> <li>How can we reach any point on the Bloch sphere?</li> <li>How many parameters are needed to specify an arbitrary single-qubit state?</li> <li>Why are rotational gates important for quantum algorithms?</li> </ul>"},{"location":"quantum/part3_single_qubit_operations/#debugging-walkthrough","title":"Debugging Walkthrough","text":"<p>Common issues and their solutions:</p> <ol> <li> <p>Problem: State vector doesn't change after applying gate    Solution: Check if you're creating a new circuit for each gate or correctly updating the state vector</p> </li> <li> <p>Problem: Bloch sphere visualization is incorrect    Solution: Ensure that the state vector is normalized and the Bloch coordinate conversion is correct</p> </li> <li> <p>Problem: Animation not displaying properly    Solution: Test with a simple animation first, and verify matplotlib's animation capabilities in your environment</p> </li> </ol>"},{"location":"quantum/part3_single_qubit_operations/#extension-challenge-quantum-tomography","title":"Extension Challenge: Quantum Tomography","text":"<p>Implement a simplified version of quantum state tomography:</p> <pre><code>def quantum_tomography(state_vector):\n    \"\"\"\n    Perform simplified quantum state tomography\n\n    Args:\n        state_vector: The quantum state to analyze\n\n    Returns:\n        Reconstructed Bloch sphere coordinates based on \"measurements\"\n    \"\"\"\n    # YOUR CODE HERE\n    pass\n</code></pre> <p>This should simulate:</p> <ol> <li>Measuring the state in X, Y, and Z bases</li> <li>Using the measurement statistics to reconstruct the Bloch vector</li> <li>Comparing the reconstructed state with the original</li> </ol>"},{"location":"quantum/part3_single_qubit_operations/#what-would-break-this","title":"What Would Break This?","text":"<p>Discuss with your team:</p> <ol> <li>What happens if you apply a non-unitary matrix to a qubit?</li> <li>Can you create an operation that extracts both \u03b1 and \u03b2 from a qubit state \u03b1|0\u27e9 + \u03b2|1\u27e9 in a single measurement?</li> <li>What limitations would noise introduce to single-qubit operations?</li> </ol>"},{"location":"quantum/part3_single_qubit_operations/#capstone-group-task","title":"Capstone Group Task","text":"<p>Create an interactive \"Quantum Gate Explorer\" tool that:</p> <ol> <li>Allows users to select from common gates (X, Y, Z, H, S, T)</li> <li>Shows the gate's matrix representation</li> <li>Visualizes the gate's effect on different input states</li> <li>Displays the resulting measurement probabilities</li> <li>Demonstrates the gate's effect as a rotation on the Bloch sphere</li> </ol>"},{"location":"quantum/part3_single_qubit_operations/#how-would-this-work-on-real-hardware","title":"How Would This Work on Real Hardware?","text":"<p>On real quantum hardware:</p> <ol> <li>How accurately can single-qubit gates be implemented?</li> <li>What causes gate errors, and how are they measured?</li> <li>How could you determine the fidelity of your gate operations?</li> <li>What's the difference between coherent and incoherent errors?</li> </ol>"},{"location":"quantum/part3_single_qubit_operations/#next-steps","title":"Next Steps","text":"<p>In the next session, we'll explore measurement and probability in quantum computing, focusing on how to extract classical information from quantum states.</p>"},{"location":"quantum/part4_measurement_probability/","title":"Part 4: Measurement &amp; Probability","text":""},{"location":"quantum/part4_measurement_probability/#objective","title":"Objective","text":"<p>Explore the probabilistic nature of quantum measurements and understand how quantum states collapse upon observation. By the end of this session, your team will be able to predict measurement outcomes, analyze repeated experiment results, and understand the fundamental differences between quantum probability and classical uncertainty.</p>"},{"location":"quantum/part4_measurement_probability/#core-concepts","title":"Core Concepts","text":""},{"location":"quantum/part4_measurement_probability/#quantum-measurement","title":"Quantum Measurement","text":"<p>Quantum measurement is the process of extracting classical information from a quantum system. Key principles:</p> <ul> <li>Measurement collapses superposition states into basis states</li> <li>Measurement outcomes are probabilistic</li> <li>Probabilities are determined by the squared magnitudes of amplitudes</li> <li>Post-measurement, the quantum state \"resets\" to the measured state</li> </ul>"},{"location":"quantum/part4_measurement_probability/#measurement-probability-calculation","title":"Measurement Probability Calculation","text":"<p>For a qubit state |\u03c8\u27e9 = \u03b1|0\u27e9 + \u03b2|1\u27e9:</p> <ul> <li>Probability of measuring |0\u27e9: P(0) = |\u03b1|\u00b2</li> <li>Probability of measuring |1\u27e9: P(1) = |\u03b2|\u00b2</li> <li>The sum of all probabilities must equal 1: |\u03b1|\u00b2 + |\u03b2|\u00b2 = 1</li> </ul>"},{"location":"quantum/part4_measurement_probability/#projective-measurements","title":"Projective Measurements","text":"<p>A measurement in quantum mechanics is described by a set of projection operators {P\u2080, P\u2081, ...}:</p> <ul> <li>For standard basis measurement: P\u2080 = |0\u27e9\u27e80| and P\u2081 = |1\u27e9\u27e81|</li> <li>The probability of outcome 'k' is p(k) = \u27e8\u03c8|P\u2096|\u03c8\u27e9</li> </ul>"},{"location":"quantum/part4_measurement_probability/#visual-explanation","title":"Visual Explanation","text":"<p>Quantum measurement can be visualized as projecting a state vector onto the measurement basis. The probability of a particular outcome is related to the \"length\" of this projection.</p>"},{"location":"quantum/part4_measurement_probability/#environment-setup","title":"Environment Setup","text":"<p>Ensure you have the necessary packages installed:</p> Command to run <code>pip install qiskit matplotlib numpy pandas seaborn</code> <p>For Cirq users: | Command to run | | :---- | | <code>pip install cirq matplotlib numpy pandas seaborn</code> |</p>"},{"location":"quantum/part4_measurement_probability/#starter-code","title":"Starter Code","text":"<p>Create a new Python file called <code>quantum_measurement.py</code> with the following structure:</p> <pre><code># TEAM MEMBERS: [List names here]\n# DATE: [Today's date]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Choose your framework\nUSE_QISKIT = True  # Set to False if using Cirq\n\nif USE_QISKIT:\n    from qiskit import QuantumCircuit, execute, Aer\n    from qiskit.visualization import plot_histogram\n    from qiskit.quantum_info import Statevector\nelse:\n    import cirq\n    import sympy\n\ndef theoretical_probabilities(state_vector):\n    \"\"\"\n    Calculate theoretical measurement probabilities from a state vector\n\n    Args:\n        state_vector: Quantum state as a NumPy array\n\n    Returns:\n        Dictionary of basis states and their probabilities\n    \"\"\"\n    # YOUR CODE HERE\n    # For a state |\u03c8\u27e9 = \u03b1|0\u27e9 + \u03b2|1\u27e9:\n    # P(0) = |\u03b1|\u00b2, P(1) = |\u03b2|\u00b2\n    pass\n\ndef circuit_with_measurement_qiskit():\n    \"\"\"Create and measure a quantum circuit with Qiskit\"\"\"\n    # Create a simple circuit with superposition\n    circuit = QuantumCircuit(1, 1)\n\n    # YOUR CODE HERE\n    # Apply gates to create an interesting state\n    # Add measurement\n    # Draw the circuit\n    # Run on simulator\n    # Plot histogram of results\n    pass\n\ndef circuit_with_measurement_cirq():\n    \"\"\"Create and measure a quantum circuit with Cirq\"\"\"\n    # YOUR CODE HERE\n    # Create a qubit and circuit\n    # Apply gates to create an interesting state\n    # Add measurement\n    # Print the circuit\n    # Run on simulator\n    # Plot results\n    pass\n\ndef measure_multiple_times(n_shots=1024):\n    \"\"\"\n    Perform the same measurement multiple times and analyze statistics\n\n    Args:\n        n_shots: Number of repetitions\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a circuit with a known state\n    # Run multiple shots on simulator\n    # Compare results with theoretical predictions\n    pass\n\ndef state_tomography_demo():\n    \"\"\"Demonstrate simple state tomography\"\"\"\n    # YOUR CODE HERE\n    # Prepare a state\n    # Measure in different bases (X, Y, Z)\n    # Reconstruct the state from measurements\n    pass\n\ndef analyze_measurement_results(counts, shots, theoretical_probs=None):\n    \"\"\"\n    Analyze measurement results and compare with theory\n\n    Args:\n        counts: Dictionary of results\n        shots: Number of experiment repetitions\n        theoretical_probs: Expected probabilities (optional)\n    \"\"\"\n    # YOUR CODE HERE\n    # Calculate empirical probabilities\n    # Calculate statistical metrics (variance, std error)\n    # Compare with theoretical expectations if provided\n    pass\n\ndef visualize_measurement_process():\n    \"\"\"Visualize the measurement process\"\"\"\n    # YOUR CODE HERE\n    # Create a sequence of images showing:\n    # 1. Initial superposition state\n    # 2. Measurement operation\n    # 3. Collapsed state after measurement\n    # 4. Multiple measurements showing distribution\n    pass\n\ndef measure_in_different_bases():\n    \"\"\"Measure the same state in different bases\"\"\"\n    # YOUR CODE HERE\n    # Create a state\n    # Measure in computational (Z) basis\n    # Measure in X basis (apply H before measurement)\n    # Measure in Y basis (apply S\u2020H before measurement)\n    # Compare results\n    pass\n\ndef main():\n    print(\"QUANTUM MEASUREMENT &amp; PROBABILITY\")\n    print(\"================================\")\n\n    print(\"\\nTHEORETICAL PROBABILITIES\")\n    print(\"========================\")\n    # Example: |\u03c8\u27e9 = (\u221a0.3)|0\u27e9 + (\u221a0.7)|1\u27e9\n    example_state = np.array([[np.sqrt(0.3)], [np.sqrt(0.7)]], dtype=complex)\n    probs = theoretical_probabilities(example_state)\n    print(f\"State probabilities: {probs}\")\n\n    print(\"\\nQUANTUM CIRCUIT WITH MEASUREMENT\")\n    print(\"===============================\")\n    if USE_QISKIT:\n        circuit_with_measurement_qiskit()\n    else:\n        circuit_with_measurement_cirq()\n\n    print(\"\\nMULTIPLE MEASUREMENTS\")\n    print(\"====================\")\n    # Try different numbers of shots\n    for shots in [10, 100, 1000, 10000]:\n        print(f\"\\nRunning with {shots} shots:\")\n        measure_multiple_times(shots)\n\n    print(\"\\nMEASUREMENT IN DIFFERENT BASES\")\n    print(\"=============================\")\n    measure_in_different_bases()\n\n    print(\"\\nMEASUREMENT VISUALIZATION\")\n    print(\"========================\")\n    visualize_measurement_process()\n\n    print(\"\\nSTATE TOMOGRAPHY DEMO\")\n    print(\"====================\")\n    state_tomography_demo()\n\n    # TEAM DISCUSSION POINT:\n    # How does quantum probability differ from classical probability?\n    # YOUR DISCUSSION NOTES HERE\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"quantum/part4_measurement_probability/#collaborative-challenge-implement-measurement-analysis","title":"Collaborative Challenge: Implement Measurement Analysis","text":"<p>Working together as a team, your challenge is to:</p> <ol> <li>Complete the functions marked with <code># YOUR CODE HERE</code></li> <li>Implement measurement in computational and non-computational bases</li> <li>Analyze how the number of shots affects measurement accuracy</li> <li>Visualize the measurement process and results effectively</li> </ol>"},{"location":"quantum/part4_measurement_probability/#team-roles-for-this-exercise","title":"Team Roles for This Exercise","text":"<p>For this lab, consider the following role assignments:</p> <ol> <li>Circuit Designer: Creates quantum circuits with interesting states</li> <li>Measurement Analyst: Implements the probability calculations and analysis</li> <li>Statistics Expert: Analyzes how shot count affects result accuracy</li> <li>Visualization Specialist: Creates clear visualizations of the measurement process</li> <li>Documentation Lead: Explains the conceptual meaning of results</li> </ol>"},{"location":"quantum/part4_measurement_probability/#discussion-checkpoints","title":"Discussion Checkpoints","text":""},{"location":"quantum/part4_measurement_probability/#checkpoint-1-after-implementing-basic-measurements","title":"Checkpoint 1: After Implementing Basic Measurements","text":"<ul> <li>Why is quantum measurement probabilistic rather than deterministic?</li> <li>What is the role of the Born rule in quantum measurement?</li> <li>How does superposition \"collapse\" during measurement?</li> </ul>"},{"location":"quantum/part4_measurement_probability/#checkpoint-2-after-multiple-measurement-analysis","title":"Checkpoint 2: After Multiple Measurement Analysis","text":"<ul> <li>How many measurements (shots) are needed for reliable statistics?</li> <li>What statistical tools help us analyze measurement uncertainty?</li> <li>How can we distinguish quantum randomness from classical noise?</li> </ul>"},{"location":"quantum/part4_measurement_probability/#checkpoint-3-after-different-basis-measurements","title":"Checkpoint 3: After Different Basis Measurements","text":"<ul> <li>Why might we want to measure in different bases?</li> <li>How is measuring in the X-basis different from the Z-basis?</li> <li>What information can we extract from each type of measurement?</li> </ul>"},{"location":"quantum/part4_measurement_probability/#debugging-walkthrough","title":"Debugging Walkthrough","text":"<p>Common issues and their solutions:</p> <ol> <li> <p>Problem: Getting unexpected measurement probabilities    Solution: Check state normalization and verify correct squaring of amplitudes (not just the amplitude values)</p> </li> <li> <p>Problem: Statistics don't match theoretical expectations    Solution: Increase the number of shots for more accurate results; statistical fluctuations are normal</p> </li> <li> <p>Problem: Basis measurements are confusing    Solution: Remember that measuring in a different basis means transforming the state first, then measuring in the computational basis</p> </li> </ol>"},{"location":"quantum/part4_measurement_probability/#extension-challenge-partial-measurements","title":"Extension Challenge: Partial Measurements","text":"<p>Implement a function to demonstrate partial measurement of a two-qubit system:</p> <pre><code>def partial_measurement_demo():\n    \"\"\"\n    Demonstrate partial measurement of a multi-qubit system\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a two-qubit entangled state\n    # Measure only one qubit\n    # Analyze the effect on the unmeasured qubit\n    # Visualize the conditional probabilities\n</code></pre>"},{"location":"quantum/part4_measurement_probability/#what-would-break-this","title":"What Would Break This?","text":"<p>Discuss with your team:</p> <ol> <li>What would happen if measurements didn't collapse quantum states?</li> <li>Can we design a measurement that doesn't disturb the quantum state?</li> <li>What would it mean if measurement results weren't truly random?</li> <li>How would imperfect detectors affect measurement results?</li> </ol>"},{"location":"quantum/part4_measurement_probability/#capstone-group-task","title":"Capstone Group Task","text":"<p>Design and implement a \"Quantum State Guessing Game\" that:</p> <ol> <li>Prepares a secret quantum state</li> <li>Allows players a limited number of measurements in different bases</li> <li>Challenges players to guess the prepared state</li> <li>Scores based on how close the guessed state is to the actual state</li> <li>Includes different difficulty levels</li> </ol>"},{"location":"quantum/part4_measurement_probability/#how-would-this-work-on-real-hardware","title":"How Would This Work on Real Hardware?","text":"<p>On real quantum hardware:</p> <ol> <li>How do readout errors affect measurement results?</li> <li>What techniques are used to mitigate measurement errors?</li> <li>How are measurement results physically obtained from qubits?</li> <li>What's the difference between destructive and non-destructive measurements?</li> </ol>"},{"location":"quantum/part4_measurement_probability/#next-steps","title":"Next Steps","text":"<p>In the next session, we'll explore multi-qubit systems and entanglement, building on our understanding of single qubit operations and measurements.</p>"},{"location":"quantum/part5_multi_qubit_magic/","title":"Part 5: Multi-Qubit Magic","text":""},{"location":"quantum/part5_multi_qubit_magic/#objective","title":"Objective","text":"<p>Explore the fascinating world of multi-qubit systems and quantum entanglement. By the end of this session, your team will be able to create and manipulate entangled states, understand Bell states and their properties, and appreciate how entanglement enables quantum computing's exponential advantage over classical computers.</p>"},{"location":"quantum/part5_multi_qubit_magic/#core-concepts","title":"Core Concepts","text":""},{"location":"quantum/part5_multi_qubit_magic/#multi-qubit-systems","title":"Multi-Qubit Systems","text":"<ul> <li>Tensor Product: The mathematical operation that combines quantum systems</li> <li>State Space Growth: With n qubits, state space grows as 2^n</li> <li>Separable States: States that can be written as tensor products of individual qubit states</li> <li>Entangled States: States that cannot be written as tensor products of individual qubit states</li> </ul>"},{"location":"quantum/part5_multi_qubit_magic/#common-two-qubit-gates","title":"Common Two-Qubit Gates","text":"<p>| Gate | Matrix/Circuit                                         | Description                            | | :--- | :----------------------------------------------------- | :------------------------------------- | --- | | CNOT |  | Flips target qubit if control qubit is | 1\u27e9  | | CZ   |      | Applies Z gate to target if control is | 1\u27e9  | | SWAP |  | Exchanges the states of two qubits     |</p>"},{"location":"quantum/part5_multi_qubit_magic/#bell-states","title":"Bell States","text":"<p>The four maximally entangled two-qubit states:</p> <ul> <li>|\u03a6\u207a\u27e9 = (|00\u27e9 + |11\u27e9)/\u221a2</li> <li>|\u03a6\u207b\u27e9 = (|00\u27e9 - |11\u27e9)/\u221a2</li> <li>|\u03a8\u207a\u27e9 = (|01\u27e9 + |10\u27e9)/\u221a2</li> <li>|\u03a8\u207b\u27e9 = (|01\u27e9 - |10\u27e9)/\u221a2</li> </ul>"},{"location":"quantum/part5_multi_qubit_magic/#visual-explanation","title":"Visual Explanation","text":"<p>In entangled states, measuring one qubit instantly determines the state of the other, regardless of distance. This \"spooky action at a distance\" (as Einstein called it) is a fundamental feature of quantum mechanics with no classical analog.</p>"},{"location":"quantum/part5_multi_qubit_magic/#environment-setup","title":"Environment Setup","text":"<p>Ensure you have the necessary packages installed:</p> Command to run <code>pip install qiskit matplotlib numpy pandas</code> <p>For Cirq users: | Command to run | | :---- | | <code>pip install cirq matplotlib numpy pandas</code> |</p>"},{"location":"quantum/part5_multi_qubit_magic/#starter-code","title":"Starter Code","text":"<p>Create a new Python file called <code>multi_qubit_systems.py</code> with the following structure:</p> <pre><code># TEAM MEMBERS: [List names here]\n# DATE: [Today's date]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Choose your framework\nUSE_QISKIT = True  # Set to False if using Cirq\n\nif USE_QISKIT:\n    from qiskit import QuantumCircuit, execute, Aer\n    from qiskit.visualization import plot_histogram, plot_bloch_multivector\n    from qiskit.quantum_info import Statevector\nelse:\n    import cirq\n    from cirq.sim.density_matrix_simulator import DensityMatrixSimulator\n\ndef tensor_product_demo():\n    \"\"\"Demonstrate tensor product of quantum states\"\"\"\n    # Define single-qubit states\n    q0_state = np.array([1, 0])  # |0\u27e9\n    q1_state = np.array([0, 1])  # |1\u27e9\n\n    # YOUR CODE HERE\n    # Compute tensor product of states\n    # Show resulting state vector\n    # Interpret the results\n    pass\n\ndef create_bell_state_qiskit(bell_type='phi_plus'):\n    \"\"\"\n    Create the specified Bell state using Qiskit\n\n    Args:\n        bell_type: One of 'phi_plus', 'phi_minus', 'psi_plus', 'psi_minus'\n\n    Returns:\n        Quantum circuit with the Bell state\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a circuit with 2 qubits\n    # Apply appropriate gates to create the Bell state\n    # Return the circuit\n    pass\n\ndef create_bell_state_cirq(bell_type='phi_plus'):\n    \"\"\"\n    Create the specified Bell state using Cirq\n\n    Args:\n        bell_type: One of 'phi_plus', 'phi_minus', 'psi_plus', 'psi_minus'\n\n    Returns:\n        Cirq circuit with the Bell state\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a circuit with 2 qubits\n    # Apply appropriate gates to create the Bell state\n    # Return the circuit\n    pass\n\ndef visualize_entangled_state(state_vector):\n    \"\"\"\n    Visualize an entangled state\n\n    Args:\n        state_vector: 4-element state vector for a 2-qubit system\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a visual representation of the entangled state\n    # Show correlations between qubits\n    pass\n\ndef measure_entangled_state(n_shots=1024):\n    \"\"\"\n    Create and measure an entangled state\n\n    Args:\n        n_shots: Number of measurements to perform\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a Bell state\n    # Measure both qubits\n    # Analyze the correlations between measurements\n    pass\n\ndef demonstrate_ghz_state():\n    \"\"\"Create and analyze a 3-qubit GHZ state |000\u27e9 + |111\u27e9\"\"\"\n    # YOUR CODE HERE\n    # Create a 3-qubit circuit\n    # Apply gates to create the GHZ state\n    # Visualize and measure the state\n    pass\n\ndef demonstrate_w_state():\n    \"\"\"Create and analyze a 3-qubit W state |001\u27e9 + |010\u27e9 + |100\u27e9\"\"\"\n    # YOUR CODE HERE\n    # Create a 3-qubit circuit\n    # Apply gates to create the W state\n    # Visualize and measure the state\n    pass\n\ndef test_bell_inequality():\n    \"\"\"Demonstrate violation of Bell's inequality\"\"\"\n    # YOUR CODE HERE\n    # Create an entangled state\n    # Perform measurements at different angles\n    # Calculate correlation values\n    # Check if Bell's inequality is violated\n    pass\n\ndef quantum_teleportation_demo():\n    \"\"\"Implement the quantum teleportation protocol\"\"\"\n    # YOUR CODE HERE\n    # Create a 3-qubit circuit\n    # Prepare arbitrary state to teleport\n    # Create entangled pair between qubits 1 and 2\n    # Perform Bell measurement on qubits 0 and 1\n    # Apply corrections to qubit 2 based on measurement results\n    # Verify teleportation success\n    pass\n\ndef compare_independent_vs_entangled():\n    \"\"\"Compare independent qubit behavior with entangled qubits\"\"\"\n    # YOUR CODE HERE\n    # Create two circuits: one with independent qubits, one with entangled qubits\n    # Apply the same operations to both\n    # Measure and compare results\n    # Highlight the differences\n    pass\n\ndef main():\n    print(\"MULTI-QUBIT SYSTEMS AND ENTANGLEMENT\")\n    print(\"===================================\")\n\n    print(\"\\nTENSOR PRODUCT DEMONSTRATION\")\n    print(\"===========================\")\n    tensor_product_demo()\n\n    print(\"\\nCREATING BELL STATES\")\n    print(\"===================\")\n    if USE_QISKIT:\n        for bell_type in ['phi_plus', 'phi_minus', 'psi_plus', 'psi_minus']:\n            circuit = create_bell_state_qiskit(bell_type)\n            print(f\"\\nBell state: {bell_type}\")\n            print(circuit.draw())\n    else:\n        for bell_type in ['phi_plus', 'phi_minus', 'psi_plus', 'psi_minus']:\n            circuit = create_bell_state_cirq(bell_type)\n            print(f\"\\nBell state: {bell_type}\")\n            print(circuit)\n\n    print(\"\\nMEASURING ENTANGLED STATES\")\n    print(\"=========================\")\n    for shots in [10, 100, 1000]:\n        print(f\"\\nMeasuring with {shots} shots:\")\n        measure_entangled_state(shots)\n\n    print(\"\\nMULTI-QUBIT ENTANGLED STATES\")\n    print(\"===========================\")\n    print(\"\\nGHZ State:\")\n    demonstrate_ghz_state()\n    print(\"\\nW State:\")\n    demonstrate_w_state()\n\n    print(\"\\nBELL'S INEQUALITY TEST\")\n    print(\"====================\")\n    test_bell_inequality()\n\n    print(\"\\nQUANTUM TELEPORTATION\")\n    print(\"====================\")\n    quantum_teleportation_demo()\n\n    print(\"\\nINDEPENDENT VS ENTANGLED QUBITS\")\n    print(\"=============================\")\n    compare_independent_vs_entangled()\n\n    # TEAM DISCUSSION POINT:\n    # What makes entanglement different from classical correlation?\n    # YOUR DISCUSSION NOTES HERE\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"quantum/part5_multi_qubit_magic/#collaborative-challenge-implement-multi-qubit-operations","title":"Collaborative Challenge: Implement Multi-Qubit Operations","text":"<p>Working together as a team, your challenge is to:</p> <ol> <li>Complete the functions marked with <code># YOUR CODE HERE</code></li> <li>Create and visualize all four Bell states</li> <li>Demonstrate the correlations in measurement outcomes for entangled qubits</li> <li>Implement and test the quantum teleportation protocol</li> </ol>"},{"location":"quantum/part5_multi_qubit_magic/#team-roles-for-this-exercise","title":"Team Roles for This Exercise","text":"<p>For this lab, consider the following role assignments:</p> <ol> <li>Entanglement Engineer: Focuses on creating entangled states</li> <li>Measurement Specialist: Analyzes the statistics of entangled measurements</li> <li>Teleportation Developer: Implements the quantum teleportation protocol</li> <li>Visualization Expert: Creates visuals of multi-qubit states</li> <li>Bell Test Analyzer: Implements and analyzes Bell inequality tests</li> </ol>"},{"location":"quantum/part5_multi_qubit_magic/#discussion-checkpoints","title":"Discussion Checkpoints","text":""},{"location":"quantum/part5_multi_qubit_magic/#checkpoint-1-after-creating-bell-states","title":"Checkpoint 1: After Creating Bell States","text":"<ul> <li>How does the creation of Bell states demonstrate quantum entanglement?</li> <li>Why can't we describe Bell states as separate qubit states?</li> <li>What's the significance of having four different Bell states?</li> </ul>"},{"location":"quantum/part5_multi_qubit_magic/#checkpoint-2-after-measuring-entangled-states","title":"Checkpoint 2: After Measuring Entangled States","text":"<ul> <li>What patterns do you observe in the measurement results?</li> <li>How do the correlations between entangled qubits differ from classical correlations?</li> <li>What happens when you measure just one qubit of an entangled pair?</li> </ul>"},{"location":"quantum/part5_multi_qubit_magic/#checkpoint-3-after-quantum-teleportation","title":"Checkpoint 3: After Quantum Teleportation","text":"<ul> <li>How does quantum teleportation work without violating the no-cloning theorem?</li> <li>Why are classical communication channels necessary for teleportation?</li> <li>How is the information transferred in teleportation?</li> </ul>"},{"location":"quantum/part5_multi_qubit_magic/#debugging-walkthrough","title":"Debugging Walkthrough","text":"<p>Common issues and their solutions:</p> <ol> <li> <p>Problem: Bell states aren't showing expected correlations    Solution: Ensure the Hadamard and CNOT gates are applied in the correct order</p> </li> <li> <p>Problem: Teleportation protocol isn't working    Solution: Verify all three steps: entanglement creation, Bell measurement, and conditional corrections</p> </li> <li> <p>Problem: Tensor product calculations are incorrect    Solution: Remember that tensor product increases dimensionality; check matrix dimensions</p> </li> </ol>"},{"location":"quantum/part5_multi_qubit_magic/#extension-challenge-quantum-superdense-coding","title":"Extension Challenge: Quantum Superdense Coding","text":"<p>Implement the quantum superdense coding protocol:</p> <pre><code>def superdense_coding_demo():\n    \"\"\"\n    Implement the quantum superdense coding protocol\n\n    Shows how to transmit 2 classical bits using 1 qubit transfer\n    \"\"\"\n    # YOUR CODE HERE\n    # Create an entangled Bell pair\n    # Encode 2 classical bits by applying operations to 1 qubit\n    # Send the qubit\n    # Decode the 2 bits with a Bell measurement\n    # Verify all 4 possible messages can be transmitted\n</code></pre>"},{"location":"quantum/part5_multi_qubit_magic/#what-would-break-this","title":"What Would Break This?","text":"<p>Discuss with your team:</p> <ol> <li>What happens to entanglement when one qubit interacts with the environment (decoherence)?</li> <li>Can entanglement be used to transmit information faster than light? Why or why not?</li> <li>What would happen if quantum mechanics allowed cloning of quantum states?</li> <li>How would errors in two-qubit gates affect entanglement quality?</li> </ol>"},{"location":"quantum/part5_multi_qubit_magic/#capstone-group-task","title":"Capstone Group Task","text":"<p>Design and implement an \"Entanglement-Based Quantum Game\" that:</p> <ol> <li>Creates different types of entangled states</li> <li>Allows players to make strategic measurement choices</li> <li>Demonstrates how entanglement can provide an advantage</li> <li>Visualizes the quantum correlations</li> </ol> <p>For example, create a simplified version of the \"CHSH game\" where entanglement helps two players coordinate without communication.</p>"},{"location":"quantum/part5_multi_qubit_magic/#how-would-this-work-on-real-hardware","title":"How Would This Work on Real Hardware?","text":"<p>On real quantum hardware:</p> <ol> <li>How is entanglement created physically?</li> <li>What limits the fidelity of two-qubit gates?</li> <li>How can we verify entanglement was actually created?</li> <li>What's the current record for the number of qubits entangled together?</li> </ol>"},{"location":"quantum/part5_multi_qubit_magic/#next-steps","title":"Next Steps","text":"<p>In the next session, we'll explore quantum algorithms that leverage the properties of superposition and entanglement to solve problems more efficiently than classical computers.</p>"},{"location":"quantum/part6_algorithms_primer/","title":"Part 6: Algorithms Primer","text":""},{"location":"quantum/part6_algorithms_primer/#objective","title":"Objective","text":"<p>Understand and implement foundational quantum algorithms that demonstrate quantum advantage. By the end of this session, your team will be able to implement the Deutsch-Jozsa algorithm and Grover's search algorithm, analyze their performance compared to classical alternatives, and understand the principles that give quantum algorithms their power.</p>"},{"location":"quantum/part6_algorithms_primer/#core-concepts","title":"Core Concepts","text":""},{"location":"quantum/part6_algorithms_primer/#quantum-algorithm-paradigms","title":"Quantum Algorithm Paradigms","text":"<ul> <li>Quantum Parallelism: Exploiting superposition to evaluate a function for multiple inputs simultaneously</li> <li>Quantum Interference: Using interference to amplify desired results and cancel unwanted ones</li> <li>Quantum Measurement: Extracting classical information from quantum states</li> </ul>"},{"location":"quantum/part6_algorithms_primer/#key-quantum-algorithms","title":"Key Quantum Algorithms","text":"Algorithm Problem Classical Complexity Quantum Complexity Speedup Deutsch-Jozsa Determine if f(x) is constant or balanced O(2^(n-1) + 1) O(1) Exponential Grover's Search Find marked item in unsorted database O(N) O(\u221aN) Quadratic Shor's Factoring Find prime factors of integer N O(e^(log N)^(1/3)) O((log N)^3) Exponential Quantum Fourier Transform Fourier transform O(N log N) O(log^2 N) Exponential"},{"location":"quantum/part6_algorithms_primer/#visual-explanation","title":"Visual Explanation","text":"<p>Quantum algorithms gain their advantage by exploring multiple solution paths simultaneously through superposition, then using interference to increase the probability of measuring the correct answer.</p>"},{"location":"quantum/part6_algorithms_primer/#environment-setup","title":"Environment Setup","text":"<p>Ensure you have the necessary packages installed:</p> Command to run <code>pip install qiskit matplotlib numpy pandas</code> <p>For Cirq users: | Command to run | | :---- | | <code>pip install cirq matplotlib numpy pandas</code> |</p>"},{"location":"quantum/part6_algorithms_primer/#starter-code","title":"Starter Code","text":"<p>Create a new Python file called <code>quantum_algorithms.py</code> with the following structure:</p> <pre><code># TEAM MEMBERS: [List names here]\n# DATE: [Today's date]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Choose your framework\nUSE_QISKIT = True  # Set to False if using Cirq\n\nif USE_QISKIT:\n    from qiskit import QuantumCircuit, execute, Aer\n    from qiskit.visualization import plot_histogram\n    from qiskit.quantum_info import Statevector\nelse:\n    import cirq\n    from cirq.sim.density_matrix_simulator import DensityMatrixSimulator\n\n# ===================================\n# Deutsch-Jozsa Algorithm\n# ===================================\n\ndef deutsch_jozsa_oracle(circuit, n_qubits, oracle_type, ancilla_idx):\n    \"\"\"\n    Implements the oracle for the Deutsch-Jozsa algorithm\n\n    Args:\n        circuit: Quantum circuit to add the oracle to\n        n_qubits: Number of qubits in the circuit (excluding ancilla)\n        oracle_type: 'constant_0', 'constant_1', or 'balanced'\n        ancilla_idx: Index of the ancilla qubit\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement different types of oracles:\n    # 1. Constant-0 oracle: f(x) = 0 for all x\n    # 2. Constant-1 oracle: f(x) = 1 for all x\n    # 3. Balanced oracle: f(x) = 0 for half of inputs, 1 for other half\n    pass\n\ndef deutsch_jozsa_algorithm_qiskit(n_qubits, oracle_type):\n    \"\"\"\n    Implements the Deutsch-Jozsa algorithm using Qiskit\n\n    Args:\n        n_qubits: Number of input qubits\n        oracle_type: Type of oracle to use\n\n    Returns:\n        QuantumCircuit with the algorithm\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a circuit with n_qubits + 1 qubits (ancilla)\n    # Apply Hadamard gates to all qubits\n    # Apply the oracle\n    # Apply Hadamard gates to input qubits\n    # Measure input qubits\n    # Return the circuit\n    pass\n\ndef deutsch_jozsa_algorithm_cirq(n_qubits, oracle_type):\n    \"\"\"\n    Implements the Deutsch-Jozsa algorithm using Cirq\n\n    Args:\n        n_qubits: Number of input qubits\n        oracle_type: Type of oracle to use\n\n    Returns:\n        Cirq circuit with the algorithm\n    \"\"\"\n    # YOUR CODE HERE\n    # Similar to the Qiskit implementation but using Cirq\n    pass\n\ndef analyze_deutsch_jozsa_results(result_counts):\n    \"\"\"\n    Analyzes the results from the Deutsch-Jozsa algorithm\n\n    Args:\n        result_counts: Measurement counts from circuit execution\n\n    Returns:\n        String indicating if function is constant or balanced\n    \"\"\"\n    # YOUR CODE HERE\n    # Determine if the function is constant or balanced from results\n    # If all qubits are 0, the function is constant\n    # Otherwise, the function is balanced\n    pass\n\n# ===================================\n# Grover's Search Algorithm\n# ===================================\n\ndef grover_oracle_qiskit(circuit, n_qubits, marked_state):\n    \"\"\"\n    Implements Grover's oracle for a specific marked state\n\n    Args:\n        circuit: Quantum circuit to add the oracle to\n        n_qubits: Number of qubits\n        marked_state: Binary string representing the marked state\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement the phase oracle that flips the phase of the marked state\n    pass\n\ndef diffusion_operator_qiskit(circuit, n_qubits):\n    \"\"\"\n    Implements the diffusion operator for Grover's algorithm\n\n    Args:\n        circuit: Quantum circuit to add the operator to\n        n_qubits: Number of qubits\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement the diffusion operator: H^\u2297n (2|0\u27e9\u27e80| - I) H^\u2297n\n    pass\n\ndef grover_algorithm_qiskit(n_qubits, marked_state, num_iterations=None):\n    \"\"\"\n    Implements Grover's search algorithm using Qiskit\n\n    Args:\n        n_qubits: Number of qubits (log2 of search space size)\n        marked_state: Binary string of the state to find\n        num_iterations: Number of Grover iterations (optional)\n\n    Returns:\n        QuantumCircuit with Grover's algorithm\n    \"\"\"\n    # YOUR CODE HERE\n    # If num_iterations is None, calculate optimal number: \u03c0/4 * sqrt(N)\n    # Create circuit with n_qubits\n    # Apply H gates to create superposition\n    # For each iteration:\n    #   Apply oracle\n    #   Apply diffusion operator\n    # Measure all qubits\n    # Return the circuit\n    pass\n\ndef grover_oracle_cirq(qubits, marked_state):\n    \"\"\"\n    Implements Grover's oracle for Cirq\n\n    Args:\n        qubits: List of qubits\n        marked_state: Binary string representing the marked state\n\n    Returns:\n        Cirq operations for the oracle\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement the phase oracle for Cirq\n    pass\n\ndef diffusion_operator_cirq(qubits):\n    \"\"\"\n    Implements the diffusion operator for Cirq\n\n    Args:\n        qubits: List of qubits\n\n    Returns:\n        Cirq operations for the diffusion operator\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement the diffusion operator for Cirq\n    pass\n\ndef grover_algorithm_cirq(n_qubits, marked_state, num_iterations=None):\n    \"\"\"\n    Implements Grover's search algorithm using Cirq\n\n    Args:\n        n_qubits: Number of qubits\n        marked_state: Binary string of the state to find\n        num_iterations: Number of Grover iterations (optional)\n\n    Returns:\n        Cirq circuit with Grover's algorithm\n    \"\"\"\n    # YOUR CODE HERE\n    # Similar to Qiskit implementation but using Cirq\n    pass\n\ndef analyze_grover_results(result_counts, marked_state):\n    \"\"\"\n    Analyzes the results from Grover's algorithm\n\n    Args:\n        result_counts: Measurement counts from circuit execution\n        marked_state: The marked state we're searching for\n\n    Returns:\n        Success probability and analysis\n    \"\"\"\n    # YOUR CODE HERE\n    # Calculate the probability of measuring the marked state\n    # Compare with random guessing (1/N)\n    # Calculate speedup\n    pass\n\ndef classical_search_simulation(n_items, rng_seed=None):\n    \"\"\"\n    Simulates a classical search for comparison\n\n    Args:\n        n_items: Number of items in the search space\n        rng_seed: Random number generator seed (optional)\n\n    Returns:\n        Number of tries needed to find the marked item\n    \"\"\"\n    # YOUR CODE HERE\n    # Simulate a classical search by generating random guesses\n    # Count how many tries are needed to find the marked item\n    # Return statistics\n    pass\n\ndef compare_quantum_vs_classical(n_qubits_list):\n    \"\"\"\n    Compares quantum vs classical search performance\n\n    Args:\n        n_qubits_list: List of qubit numbers to test\n\n    Returns:\n        DataFrame with comparison results\n    \"\"\"\n    # YOUR CODE HERE\n    # For each n_qubits:\n    #   Run Grover's algorithm\n    #   Simulate classical search\n    #   Compare performance\n    # Return and visualize results\n    pass\n\ndef main():\n    print(\"QUANTUM ALGORITHM DEMONSTRATIONS\")\n    print(\"==============================\")\n\n    print(\"\\nDEUTSCH-JOZSA ALGORITHM\")\n    print(\"======================\")\n    n_qubits = 3  # Number of input qubits\n\n    for oracle_type in ['constant_0', 'constant_1', 'balanced']:\n        print(f\"\\nTesting oracle type: {oracle_type}\")\n\n        if USE_QISKIT:\n            circuit = deutsch_jozsa_algorithm_qiskit(n_qubits, oracle_type)\n            print(circuit.draw())\n\n            # Execute the circuit\n            simulator = Aer.get_backend('qasm_simulator')\n            result = execute(circuit, simulator, shots=1024).result()\n            counts = result.get_counts()\n\n            print(\"Results:\", counts)\n            conclusion = analyze_deutsch_jozsa_results(counts)\n            print(f\"Conclusion: Function is {conclusion}\")\n        else:\n            circuit = deutsch_jozsa_algorithm_cirq(n_qubits, oracle_type)\n            print(circuit)\n\n            # Execute the circuit with Cirq\n            # YOUR CODE HERE\n\n    print(\"\\nGROVER'S SEARCH ALGORITHM\")\n    print(\"========================\")\n    n_qubits = 3  # 2^3 = 8 items in the search space\n    marked_state = '101'  # The item we're searching for\n\n    if USE_QISKIT:\n        circuit = grover_algorithm_qiskit(n_qubits, marked_state)\n        print(circuit.draw())\n\n        # Execute the circuit\n        simulator = Aer.get_backend('qasm_simulator')\n        result = execute(circuit, simulator, shots=1024).result()\n        counts = result.get_counts()\n\n        print(\"Results:\", counts)\n        analysis = analyze_grover_results(counts, marked_state)\n        print(analysis)\n    else:\n        circuit = grover_algorithm_cirq(n_qubits, marked_state)\n        print(circuit)\n\n        # Execute the circuit with Cirq\n        # YOUR CODE HERE\n\n    print(\"\\nQUANTUM VS CLASSICAL COMPARISON\")\n    print(\"==============================\")\n    comparison = compare_quantum_vs_classical([2, 3, 4, 5, 6])\n    print(comparison)\n\n    # TEAM DISCUSSION POINT:\n    # What gives quantum algorithms their advantage over classical algorithms?\n    # YOUR DISCUSSION NOTES HERE\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"quantum/part6_algorithms_primer/#collaborative-challenge-implement-quantum-algorithms","title":"Collaborative Challenge: Implement Quantum Algorithms","text":"<p>Working together as a team, your challenge is to:</p> <ol> <li>Complete all functions marked with <code># YOUR CODE HERE</code></li> <li>Implement and test the Deutsch-Jozsa algorithm</li> <li>Implement and test Grover's search algorithm</li> <li>Compare the performance of quantum vs. classical approaches</li> </ol>"},{"location":"quantum/part6_algorithms_primer/#team-roles-for-this-exercise","title":"Team Roles for This Exercise","text":"<p>For this lab, consider the following role assignments:</p> <ol> <li>Oracle Designer: Focuses on implementing the quantum oracles for both algorithms</li> <li>Algorithm Implementer: Works on the main algorithm structure</li> <li>Performance Analyst: Compares quantum vs. classical performance</li> <li>Visualization Expert: Creates clear visualizations of results and speedups</li> <li>Documentation Lead: Explains the algorithms and their applications</li> </ol>"},{"location":"quantum/part6_algorithms_primer/#discussion-checkpoints","title":"Discussion Checkpoints","text":""},{"location":"quantum/part6_algorithms_primer/#checkpoint-1-after-implementing-deutsch-jozsa","title":"Checkpoint 1: After Implementing Deutsch-Jozsa","text":"<ul> <li>What is the key insight that allows the Deutsch-Jozsa algorithm to work?</li> <li>Why can quantum computing determine if a function is constant or balanced in one query?</li> <li>What are the limitations of this algorithm in practical applications?</li> </ul>"},{"location":"quantum/part6_algorithms_primer/#checkpoint-2-after-implementing-grovers-search","title":"Checkpoint 2: After Implementing Grover's Search","text":"<ul> <li>How does the diffusion operator amplify the amplitude of the marked state?</li> <li>Why does Grover's algorithm achieve quadratic speedup but not exponential?</li> <li>What happens if you run too many iterations of Grover's algorithm?</li> </ul>"},{"location":"quantum/part6_algorithms_primer/#checkpoint-3-after-performance-comparison","title":"Checkpoint 3: After Performance Comparison","text":"<ul> <li>Which types of problems are well-suited for quantum algorithms?</li> <li>What patterns do you see in problems where quantum computing excels?</li> <li>How do the resource requirements scale for quantum vs. classical approaches?</li> </ul>"},{"location":"quantum/part6_algorithms_primer/#debugging-walkthrough","title":"Debugging Walkthrough","text":"<p>Common issues and their solutions:</p> <ol> <li> <p>Problem: Deutsch-Jozsa algorithm always returns \"constant\"    Solution: Check that your balanced oracle is correctly implemented; it should mark exactly half of the inputs</p> </li> <li> <p>Problem: Grover's search doesn't find the marked item    Solution: Verify the number of iterations; too many or too few can reduce success probability</p> </li> <li> <p>Problem: Quantum circuit is too complex to visualize    Solution: For larger qubit numbers, focus on visualizing results rather than the full circuit</p> </li> </ol>"},{"location":"quantum/part6_algorithms_primer/#extension-challenge-implement-quantum-counting","title":"Extension Challenge: Implement Quantum Counting","text":"<p>Implement a quantum counting algorithm that combines Grover's algorithm with the Quantum Fourier Transform:</p> <pre><code>def quantum_counting_algorithm(n_qubits, marked_state, counting_qubits=4):\n    \"\"\"\n    Implements the quantum counting algorithm\n\n    Args:\n        n_qubits: Number of qubits for the search space\n        marked_state: The marked state to count\n        counting_qubits: Number of qubits for counting\n\n    Returns:\n        Circuit that estimates the number of solutions\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement quantum counting algorithm\n    # Use phase estimation on Grover's operator\n    # Return circuit that estimates number of solutions\n</code></pre>"},{"location":"quantum/part6_algorithms_primer/#what-would-break-this","title":"What Would Break This?","text":"<p>Discuss with your team:</p> <ol> <li>What assumptions do these algorithms make about quantum computers?</li> <li>How would noise and decoherence affect algorithm performance?</li> <li>What are the current limitations in implementing these algorithms on real hardware?</li> <li>Why can't we just read out all the information in a superposition?</li> </ol>"},{"location":"quantum/part6_algorithms_primer/#capstone-group-task","title":"Capstone Group Task","text":"<p>Design and implement a \"Quantum Function Identifier\" that:</p> <ol> <li>Takes a mystery oracle function</li> <li>Uses the Deutsch-Jozsa and related algorithms to determine its properties</li> <li>Classifies the function as constant, balanced, OR-type, AND-type, etc.</li> <li>Compares the quantum approach with classical simulation</li> </ol>"},{"location":"quantum/part6_algorithms_primer/#how-would-this-work-on-real-hardware","title":"How Would This Work on Real Hardware?","text":"<p>On real quantum hardware:</p> <ol> <li>How do we implement oracles for real problems?</li> <li>What are the current record sizes for implementing these algorithms?</li> <li>How do we measure algorithm success on noisy hardware?</li> <li>What is the quantum volume needed to demonstrate quantum advantage?</li> </ol>"},{"location":"quantum/part6_algorithms_primer/#next-steps","title":"Next Steps","text":"<p>In the next session, we'll explore quantum noise and error correction, focusing on the real-world challenges of implementing quantum algorithms on imperfect hardware.</p>"},{"location":"quantum/part7_noise_error_reality/","title":"Part 7: Noise, Error &amp; Reality Checks","text":""},{"location":"quantum/part7_noise_error_reality/#objective","title":"Objective","text":"<p>Explore the challenges of real-world quantum computing by understanding quantum noise, error models, and mitigation techniques. By the end of this session, your team will be able to simulate realistic quantum systems with noise, analyze how errors affect algorithm performance, and appreciate the importance of error correction in quantum computing.</p>"},{"location":"quantum/part7_noise_error_reality/#core-concepts","title":"Core Concepts","text":""},{"location":"quantum/part7_noise_error_reality/#sources-of-quantum-errors","title":"Sources of Quantum Errors","text":"<ul> <li>Coherent Errors: Systematic imperfections in quantum operations (miscalibration)</li> <li>Incoherent Errors: Random noise disrupting quantum states (decoherence)</li> <li>Readout Errors: Mistakes in measuring qubit states</li> <li>Cross-talk: Unwanted interactions between neighboring qubits</li> <li>Thermal Relaxation: Loss of quantum information to the environment</li> </ul>"},{"location":"quantum/part7_noise_error_reality/#error-models-and-metrics","title":"Error Models and Metrics","text":"<p>| Error Type         | Description               | Relevant Metrics                   | | :----------------- | :------------------------ | :--------------------------------- | --- | -------------------- | | Bit Flip           | X errors:                 | 0\u27e9 \u2194                               | 1\u27e9  | Bit flip probability | | Phase Flip         | Z errors: phase reversal  | Phase flip probability             | | Depolarizing       | Random Pauli errors       | Depolarizing rate                  | | Amplitude Damping  | Energy dissipation        | T\u2081 time (relaxation)               | | Phase Damping      | Loss of phase coherence   | T\u2082 time (dephasing)                | | Gate Errors        | Imperfect gate operations | Gate fidelity, process fidelity    | | Measurement Errors | Incorrect readout         | Assignment error, readout fidelity |</p>"},{"location":"quantum/part7_noise_error_reality/#error-mitigation-strategies","title":"Error Mitigation Strategies","text":"<ul> <li>Error detection: Identifying when errors have occurred</li> <li>Quantum error correction: Using redundancy to protect quantum information</li> <li>Dynamical decoupling: Applying pulses to reduce noise effects</li> <li>Error extrapolation: Estimating error-free results from noisy ones</li> <li>Noise-robust algorithm design: Creating algorithms less sensitive to noise</li> </ul>"},{"location":"quantum/part7_noise_error_reality/#visual-explanation","title":"Visual Explanation","text":"<p>Quantum information is extremely fragile. While classical bits are discrete (0 or 1), quantum states exist in a continuum, making them susceptible to small perturbations. Error correction techniques aim to preserve quantum information despite these challenges.</p>"},{"location":"quantum/part7_noise_error_reality/#environment-setup","title":"Environment Setup","text":"<p>Ensure you have the necessary packages installed:</p> Command to run <code>pip install qiskit qiskit-aer matplotlib numpy pandas</code> <p>For Cirq users: | Command to run | | :---- | | <code>pip install cirq matplotlib numpy pandas</code> |</p>"},{"location":"quantum/part7_noise_error_reality/#starter-code","title":"Starter Code","text":"<p>Create a new Python file called <code>quantum_noise_simulation.py</code> with the following structure:</p> <pre><code># TEAM MEMBERS: [List names here]\n# DATE: [Today's date]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\n\n# Choose your framework\nUSE_QISKIT = True  # Set to False if using Cirq\n\nif USE_QISKIT:\n    from qiskit import QuantumCircuit, execute, Aer, IBMQ\n    from qiskit.providers.aer import QasmSimulator\n    from qiskit.providers.aer.noise import NoiseModel\n    from qiskit.providers.aer.noise import depolarizing_error, pauli_error, amplitude_damping_error\n    from qiskit.visualization import plot_histogram\n    from qiskit.quantum_info import Statevector, state_fidelity\nelse:\n    import cirq\n    from cirq import depolarize, amplitude_damp\n    from cirq.circuits import Circuit\n    from cirq.sim.density_matrix_simulator import DensityMatrixSimulator\n\ndef create_ideal_bell_pair():\n    \"\"\"Create an ideal Bell pair circuit\"\"\"\n    # YOUR CODE HERE\n    # Create a circuit with 2 qubits\n    # Apply Hadamard to first qubit\n    # Apply CNOT gate\n    # Return the circuit\n    pass\n\ndef add_bit_flip_noise(circuit, error_probability=0.05):\n    \"\"\"\n    Simulates a circuit with bit flip (X) errors\n\n    Args:\n        circuit: The quantum circuit to simulate\n        error_probability: Probability of a bit flip error\n\n    Returns:\n        Noisy circuit simulator and noise model\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a noise model with bit flip errors\n    # Return the noisy simulator\n    pass\n\ndef add_phase_flip_noise(circuit, error_probability=0.05):\n    \"\"\"\n    Simulates a circuit with phase flip (Z) errors\n\n    Args:\n        circuit: The quantum circuit to simulate\n        error_probability: Probability of a phase flip error\n\n    Returns:\n        Noisy circuit simulator and noise model\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a noise model with phase flip errors\n    # Return the noisy simulator\n    pass\n\ndef add_depolarizing_noise(circuit, error_probability=0.05):\n    \"\"\"\n    Simulates a circuit with depolarizing noise\n\n    Args:\n        circuit: The quantum circuit to simulate\n        error_probability: Depolarizing probability\n\n    Returns:\n        Noisy circuit simulator and noise model\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a noise model with depolarizing noise\n    # Return the noisy simulator\n    pass\n\ndef add_thermal_relaxation(circuit, t1=50, t2=30, gate_time=10):\n    \"\"\"\n    Simulates a circuit with thermal relaxation\n\n    Args:\n        circuit: The quantum circuit to simulate\n        t1: T1 relaxation time (microseconds)\n        t2: T2 dephasing time (microseconds)\n        gate_time: Gate operation time (nanoseconds)\n\n    Returns:\n        Noisy circuit simulator and noise model\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a noise model with thermal relaxation\n    # Return the noisy simulator\n    pass\n\ndef add_measurement_error(circuit, error_probability=0.05):\n    \"\"\"\n    Simulates a circuit with measurement errors\n\n    Args:\n        circuit: The quantum circuit to simulate\n        error_probability: Error probability\n\n    Returns:\n        Noisy circuit simulator and noise model\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a noise model with measurement errors\n    # Return the noisy simulator\n    pass\n\ndef compare_noise_models(shots=1024):\n    \"\"\"\n    Compares different noise models on a Bell pair\n\n    Args:\n        shots: Number of circuit repetitions\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a Bell pair circuit\n    # Simulate with different noise models\n    # Compare results\n    # Visualize differences\n    pass\n\ndef fidelity_vs_noise_strength():\n    \"\"\"\n    Analyzes how state fidelity decreases with increasing noise\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a circuit\n    # Simulate with varying noise strength\n    # Calculate state fidelity compared to ideal case\n    # Plot fidelity vs noise strength\n    pass\n\ndef quantum_circuit_with_error_detection():\n    \"\"\"\n    Implements a simple error detection code\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a circuit with error detection\n    # Add noise\n    # Detect errors\n    # Compare to circuit without error detection\n    pass\n\ndef bit_flip_code_demonstration():\n    \"\"\"\n    Demonstrates the 3-qubit bit flip code\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement the 3-qubit bit flip code\n    # Apply bit flip errors\n    # Detect and correct errors\n    # Compare with an unprotected qubit\n    pass\n\ndef phase_flip_code_demonstration():\n    \"\"\"\n    Demonstrates the 3-qubit phase flip code\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement the 3-qubit phase flip code\n    # Apply phase flip errors\n    # Detect and correct errors\n    # Compare with an unprotected qubit\n    pass\n\ndef analyze_algorithm_under_noise(algorithm_circuit, noise_model, shots=1024):\n    \"\"\"\n    Analyzes how noise affects algorithm performance\n\n    Args:\n        algorithm_circuit: Quantum circuit implementing an algorithm\n        noise_model: Noise model to apply\n        shots: Number of circuit repetitions\n    \"\"\"\n    # YOUR CODE HERE\n    # Run the algorithm with and without noise\n    # Compare results\n    # Analyze how noise affected the outcome\n    # Calculate success probability degradation\n    pass\n\ndef error_mitigation_demonstration():\n    \"\"\"\n    Demonstrates simple error mitigation techniques\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement a circuit with error mitigation\n    # Compare to unmitigated circuit\n    # Analyze effectiveness\n    pass\n\ndef real_device_noise_model():\n    \"\"\"\n    Creates a noise model based on real quantum device characteristics\n\n    Returns:\n        Noise model calibrated to a real quantum processor\n    \"\"\"\n    # YOUR CODE HERE\n    # Create a noise model based on real device parameters\n    # Either by loading IBMQ backend properties or manually setting parameters\n    pass\n\ndef main():\n    print(\"QUANTUM NOISE AND ERROR CORRECTION\")\n    print(\"=================================\")\n\n    print(\"\\nCOMPARING NOISE MODELS\")\n    print(\"=====================\")\n    compare_noise_models()\n\n    print(\"\\nFIDELITY VS NOISE STRENGTH\")\n    print(\"=========================\")\n    fidelity_vs_noise_strength()\n\n    print(\"\\nERROR DETECTION DEMONSTRATION\")\n    print(\"===========================\")\n    quantum_circuit_with_error_detection()\n\n    print(\"\\nBIT FLIP CODE DEMONSTRATION\")\n    print(\"==========================\")\n    bit_flip_code_demonstration()\n\n    print(\"\\nPHASE FLIP CODE DEMONSTRATION\")\n    print(\"============================\")\n    phase_flip_code_demonstration()\n\n    print(\"\\nALGORITHM UNDER NOISE\")\n    print(\"====================\")\n    # Create a simple algorithm circuit (e.g., Deutsch-Jozsa)\n    algorithm_circuit = QuantumCircuit(3, 3) if USE_QISKIT else Circuit()\n    # YOUR CODE HERE: Create a test algorithm\n\n    # Analyze algorithm under different noise models\n    for noise_type, noise_prob in [\n        (\"Bit Flip\", 0.01),\n        (\"Phase Flip\", 0.01),\n        (\"Depolarizing\", 0.01),\n        (\"Measurement\", 0.05),\n    ]:\n        print(f\"\\nTesting algorithm with {noise_type} noise (p={noise_prob}):\")\n        # YOUR CODE HERE: Create appropriate noise model and analyze\n\n    print(\"\\nERROR MITIGATION DEMONSTRATION\")\n    print(\"=============================\")\n    error_mitigation_demonstration()\n\n    # TEAM DISCUSSION POINT:\n    # What are the biggest challenges in implementing quantum algorithms on real hardware?\n    # YOUR DISCUSSION NOTES HERE\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"quantum/part7_noise_error_reality/#collaborative-challenge-implement-noise-simulations","title":"Collaborative Challenge: Implement Noise Simulations","text":"<p>Working together as a team, your challenge is to:</p> <ol> <li>Complete all functions marked with <code># YOUR CODE HERE</code></li> <li>Implement and analyze different quantum noise models</li> <li>Demonstrate simple error detection and correction techniques</li> <li>Compare algorithm performance with and without noise</li> </ol>"},{"location":"quantum/part7_noise_error_reality/#team-roles-for-this-exercise","title":"Team Roles for This Exercise","text":"<p>For this lab, consider the following role assignments:</p> <ol> <li>Noise Modeler: Focuses on implementing various noise models</li> <li>Error Correction Specialist: Develops error detection and correction circuits</li> <li>Analysis Expert: Analyzes the impact of noise on quantum states and algorithms</li> <li>Visualization Lead: Creates visualizations of noise effects</li> <li>Mitigation Developer: Implements error mitigation techniques</li> </ol>"},{"location":"quantum/part7_noise_error_reality/#discussion-checkpoints","title":"Discussion Checkpoints","text":""},{"location":"quantum/part7_noise_error_reality/#checkpoint-1-after-implementing-noise-models","title":"Checkpoint 1: After Implementing Noise Models","text":"<ul> <li>What types of noise are most damaging to quantum information?</li> <li>How do different noise models affect quantum states differently?</li> <li>How close are our noise models to real quantum hardware?</li> </ul>"},{"location":"quantum/part7_noise_error_reality/#checkpoint-2-after-error-detection-demonstrations","title":"Checkpoint 2: After Error Detection Demonstrations","text":"<ul> <li>What is the trade-off between error protection and qubit overhead?</li> <li>How can we detect errors without directly measuring quantum states?</li> <li>What are the limitations of simple error detection codes?</li> </ul>"},{"location":"quantum/part7_noise_error_reality/#checkpoint-3-after-algorithm-analysis","title":"Checkpoint 3: After Algorithm Analysis","text":"<ul> <li>Which quantum algorithms are most robust against noise?</li> <li>How does noise affect the quantum advantage of algorithms?</li> <li>What level of noise can be tolerated before an algorithm fails?</li> </ul>"},{"location":"quantum/part7_noise_error_reality/#debugging-walkthrough","title":"Debugging Walkthrough","text":"<p>Common issues and their solutions:</p> <ol> <li> <p>Problem: Noise models not showing expected behavior    Solution: Verify noise parameter values; too high values can completely destroy quantum states</p> </li> <li> <p>Problem: Error correction not improving results    Solution: Make sure the error correction encoding and decoding are correctly implemented</p> </li> <li> <p>Problem: Simulation runs very slowly    Solution: For larger circuits, reduce the circuit size or number of shots; density matrix simulations are more expensive than statevector</p> </li> </ol>"},{"location":"quantum/part7_noise_error_reality/#extension-challenge-implement-surface-code-elements","title":"Extension Challenge: Implement Surface Code Elements","text":"<p>Implement elements of a surface code, a more advanced error correction technique:</p> <pre><code>def surface_code_basic_elements():\n    \"\"\"\n    Implements basic elements of a surface code\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement stabilizer measurements\n    # Demonstrate error detection with stabilizers\n    # Show how logical qubits are encoded\n</code></pre>"},{"location":"quantum/part7_noise_error_reality/#what-would-break-this","title":"What Would Break This?","text":"<p>Discuss with your team:</p> <ol> <li>What happens when error rates exceed error correction thresholds?</li> <li>How do correlated errors affect error correction strategies?</li> <li>What impact would non-Markovian noise (noise with memory) have?</li> <li>How many physical qubits would be needed for a fault-tolerant quantum computation?</li> </ol>"},{"location":"quantum/part7_noise_error_reality/#capstone-group-task","title":"Capstone Group Task","text":"<p>Design and implement a \"Noise Resistance Benchmark\" that:</p> <ol> <li>Tests a quantum algorithm under various noise conditions</li> <li>Determines the noise threshold at which the algorithm fails</li> <li>Compares different error mitigation strategies</li> <li>Creates a visualization showing how algorithm performance degrades with noise</li> <li>Recommends the best error mitigation approach for specific noise types</li> </ol>"},{"location":"quantum/part7_noise_error_reality/#how-would-this-work-on-real-hardware","title":"How Would This Work on Real Hardware?","text":"<p>On real quantum hardware:</p> <ol> <li>What are the dominant noise sources in current superconducting qubits?</li> <li>How are error rates measured and reported on real devices?</li> <li>What is the state of the art in quantum error correction implementations?</li> <li>What are the prospects for fault-tolerant quantum computing?</li> </ol>"},{"location":"quantum/part7_noise_error_reality/#next-steps","title":"Next Steps","text":"<p>In the next session, we'll explore the quantum computing toolchain and ecosystem, focusing on the different frameworks, cloud services, and quantum hardware platforms available today.</p>"},{"location":"quantum/part8_toolchain_ecosystem/","title":"Part 8: Toolchain &amp; Ecosystem","text":""},{"location":"quantum/part8_toolchain_ecosystem/#objective","title":"Objective","text":"<p>Explore the diverse quantum computing ecosystem and compare popular frameworks, cloud platforms, and hardware approaches. By the end of this session, your team will understand the trade-offs between different quantum computing tools, be able to access both simulators and real quantum hardware, and make informed decisions about which platforms to use for different quantum computing tasks.</p>"},{"location":"quantum/part8_toolchain_ecosystem/#core-concepts","title":"Core Concepts","text":""},{"location":"quantum/part8_toolchain_ecosystem/#quantum-computing-frameworks","title":"Quantum Computing Frameworks","text":"Framework Organization Strengths Ideal Use Cases Qiskit IBM Comprehensive, well-documented, access to IBM hardware Education, research, algorithm development Cirq Google Low-level control, access to Google hardware Custom gate development, hardware-specific optimization PennyLane Xanadu Quantum machine learning focus, hybrid models QML, variational algorithms, gradient-based optimization Q# Microsoft High-level language, strong classical integration Algorithm design, theoretical exploration PyQuil Rigetti Quil assembly language, access to Rigetti hardware Low-level control, custom gate design Ocean D-Wave Quantum annealing, optimization problems Combinatorial optimization, sampling problems Braket SDK Amazon Multi-hardware access, Amazon integration Cloud-based exploration of different hardware types"},{"location":"quantum/part8_toolchain_ecosystem/#quantum-hardware-approaches","title":"Quantum Hardware Approaches","text":"Approach Companies Qubits Strengths Weaknesses Superconducting IBM, Google, Rigetti 50-433 Fast gates, scalable fabrication Short coherence times, crosstalk Trapped Ions IonQ, Honeywell 11-32 Long coherence times, high fidelity Slower gates, scaling challenges Photonic Xanadu, PsiQuantum Variable Room temperature, natural connectivity Probabilistic gates, photon loss Neutral Atoms QuEra, Pasqal 100-256 Scalability, long coherence Limited gate sets, young technology Silicon Spin Intel, Silicon Quantum Computing 1-4 Manufacturing compatibility, long coherence Early stage, limited qubit count Topological Microsoft Research Potentially fault-tolerant Not yet demonstrated Quantum Annealing D-Wave 5000+ Large qubit count, optimization focused Limited problem types, not universal"},{"location":"quantum/part8_toolchain_ecosystem/#cloud-access-models","title":"Cloud Access Models","text":"<ul> <li>Queue-based systems: Submit jobs to a queue (IBM Quantum, Amazon Braket)</li> <li>Interactive access: Direct connection to quantum processors</li> <li>Hybrid classical-quantum: Combine classical and quantum resources</li> <li>Simulator options: Local simulators vs. cloud-based high-performance simulators</li> </ul>"},{"location":"quantum/part8_toolchain_ecosystem/#visual-explanation","title":"Visual Explanation","text":"<p>The quantum computing ecosystem consists of software layers (programming languages, compilers, simulators) and hardware layers (quantum processors of different types). Cloud services provide the bridge between developers and physical quantum computers.</p>"},{"location":"quantum/part8_toolchain_ecosystem/#environment-setup","title":"Environment Setup","text":"<p>Ensure you have the necessary packages installed:</p> Command to run <code>pip install qiskit cirq pennylane amazon-braket-sdk matplotlib numpy pandas</code>"},{"location":"quantum/part8_toolchain_ecosystem/#starter-code","title":"Starter Code","text":"<p>Create a new Python file called <code>quantum_ecosystem_comparison.py</code> with the following structure:</p> <pre><code># TEAM MEMBERS: [List names here]\n# DATE: [Today's date]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\nimport os\nimport json\nfrom tabulate import tabulate\n\n# Import various frameworks (comment out any that are not installed)\n# Qiskit\ntry:\n    import qiskit\n    from qiskit import QuantumCircuit, execute, Aer\n    from qiskit.visualization import plot_histogram\n    QISKIT_AVAILABLE = True\nexcept ImportError:\n    QISKIT_AVAILABLE = False\n\n# Cirq\ntry:\n    import cirq\n    CIRQ_AVAILABLE = True\nexcept ImportError:\n    CIRQ_AVAILABLE = False\n\n# PennyLane\ntry:\n    import pennylane as qml\n    PENNYLANE_AVAILABLE = True\nexcept ImportError:\n    PENNYLANE_AVAILABLE = False\n\n# Amazon Braket\ntry:\n    import braket\n    from braket.circuits import Circuit as BraketCircuit\n    BRAKET_AVAILABLE = True\nexcept ImportError:\n    BRAKET_AVAILABLE = False\n\ndef print_available_frameworks():\n    \"\"\"Print which frameworks are available in the current environment\"\"\"\n    frameworks = {\n        \"Qiskit (IBM)\": QISKIT_AVAILABLE,\n        \"Cirq (Google)\": CIRQ_AVAILABLE,\n        \"PennyLane (Xanadu)\": PENNYLANE_AVAILABLE,\n        \"Braket SDK (Amazon)\": BRAKET_AVAILABLE,\n    }\n\n    print(\"Available Quantum Computing Frameworks:\")\n    for framework, available in frameworks.items():\n        status = \"\u2705 Installed\" if available else \"\u274c Not installed\"\n        print(f\"- {framework}: {status}\")\n\ndef create_bell_pair_qiskit():\n    \"\"\"Create a Bell pair using Qiskit\"\"\"\n    if not QISKIT_AVAILABLE:\n        print(\"Qiskit not available\")\n        return None\n\n    # YOUR CODE HERE\n    # Create a Bell pair circuit with Qiskit\n    pass\n\ndef create_bell_pair_cirq():\n    \"\"\"Create a Bell pair using Cirq\"\"\"\n    if not CIRQ_AVAILABLE:\n        print(\"Cirq not available\")\n        return None\n\n    # YOUR CODE HERE\n    # Create a Bell pair circuit with Cirq\n    pass\n\ndef create_bell_pair_pennylane():\n    \"\"\"Create a Bell pair using PennyLane\"\"\"\n    if not PENNYLANE_AVAILABLE:\n        print(\"PennyLane not available\")\n        return None\n\n    # YOUR CODE HERE\n    # Create a Bell pair circuit with PennyLane\n    pass\n\ndef create_bell_pair_braket():\n    \"\"\"Create a Bell pair using Amazon Braket\"\"\"\n    if not BRAKET_AVAILABLE:\n        print(\"Amazon Braket not available\")\n        return None\n\n    # YOUR CODE HERE\n    # Create a Bell pair circuit with Amazon Braket\n    pass\n\ndef compare_syntax():\n    \"\"\"Compare the syntax of different frameworks\"\"\"\n    examples = {}\n\n    if QISKIT_AVAILABLE:\n        examples[\"Qiskit\"] = \"\"\"\n# Qiskit Bell Pair\nfrom qiskit import QuantumCircuit\n\nqc = QuantumCircuit(2, 2)\nqc.h(0)\nqc.cx(0, 1)\nqc.measure([0, 1], [0, 1])\n\"\"\"\n\n    if CIRQ_AVAILABLE:\n        examples[\"Cirq\"] = \"\"\"\n# Cirq Bell Pair\nimport cirq\n\nq0, q1 = cirq.LineQubit.range(2)\ncircuit = cirq.Circuit(\n    cirq.H(q0),\n    cirq.CNOT(q0, q1),\n    cirq.measure(q0, q1, key='result')\n)\n\"\"\"\n\n    if PENNYLANE_AVAILABLE:\n        examples[\"PennyLane\"] = \"\"\"\n# PennyLane Bell Pair\nimport pennylane as qml\n\ndev = qml.device('default.qubit', wires=2)\n\n@qml.qnode(dev)\ndef bell_pair():\n    qml.Hadamard(wires=0)\n    qml.CNOT(wires=[0, 1])\n    return qml.probs(wires=[0, 1])\n\"\"\"\n\n    if BRAKET_AVAILABLE:\n        examples[\"Braket\"] = \"\"\"\n# Amazon Braket Bell Pair\nfrom braket.circuits import Circuit\n\ncircuit = Circuit()\ncircuit.h(0)\ncircuit.cnot(0, 1)\ncircuit.probability()\n\"\"\"\n\n    print(\"Syntax Comparison for Bell Pair Circuit:\")\n    for framework, code in examples.items():\n        print(f\"\\n{framework}:\")\n        print(code)\n\ndef benchmark_simulators(n_qubits=5, depth=5, shots=1024):\n    \"\"\"\n    Benchmark simulator performance across frameworks\n\n    Args:\n        n_qubits: Number of qubits in test circuit\n        depth: Circuit depth (number of layers)\n        shots: Number of simulations\n    \"\"\"\n    results = []\n\n    # YOUR CODE HERE\n    # For each available framework:\n    # 1. Create a test circuit with n_qubits and depth\n    # 2. Time how long it takes to simulate\n    # 3. Record the results\n\n    # Display the benchmark results\n    pass\n\ndef list_available_backends():\n    \"\"\"List available backends for each framework\"\"\"\n    backend_info = {}\n\n    # Qiskit backends\n    if QISKIT_AVAILABLE:\n        # YOUR CODE HERE\n        # Get a list of available Qiskit backends\n        # Include both simulators and real hardware (if configured)\n        pass\n\n    # Cirq backends\n    if CIRQ_AVAILABLE:\n        # YOUR CODE HERE\n        # List Cirq simulator options\n        pass\n\n    # PennyLane backends\n    if PENNYLANE_AVAILABLE:\n        # YOUR CODE HERE\n        # List available PennyLane devices\n        pass\n\n    # Braket backends\n    if BRAKET_AVAILABLE:\n        # YOUR CODE HERE\n        # List available Amazon Braket backends\n        # Include both simulators and hardware options\n        pass\n\n    # Display backend information\n    for framework, backends in backend_info.items():\n        print(f\"\\n{framework} Backends:\")\n        for backend in backends:\n            print(f\"- {backend}\")\n\ndef framework_feature_comparison():\n    \"\"\"Compare features of different quantum frameworks\"\"\"\n    features = {\n        \"Feature\": [\n            \"Open Source\",\n            \"Hardware Access\",\n            \"Built-in Simulators\",\n            \"Circuit Visualization\",\n            \"Noise Modeling\",\n            \"Pulse-level Control\",\n            \"Optimizer Integration\",\n            \"Error Mitigation\",\n            \"Community Size\",\n            \"Documentation Quality\"\n        ]\n    }\n\n    if QISKIT_AVAILABLE:\n        features[\"Qiskit\"] = [\n            \"Yes\",\n            \"IBM Quantum\",\n            \"Statevector, QASM, Density Matrix, MPS\",\n            \"Excellent\",\n            \"Advanced\",\n            \"Yes\",\n            \"Good\",\n            \"Yes\",\n            \"Very Large\",\n            \"Excellent\"\n        ]\n\n    if CIRQ_AVAILABLE:\n        features[\"Cirq\"] = [\n            \"Yes\",\n            \"Google Quantum AI\",\n            \"Statevector, Density Matrix\",\n            \"Good\",\n            \"Advanced\",\n            \"Limited\",\n            \"Basic\",\n            \"Basic\",\n            \"Medium\",\n            \"Good\"\n        ]\n\n    if PENNYLANE_AVAILABLE:\n        features[\"PennyLane\"] = [\n            \"Yes\",\n            \"Multiple via plugins\",\n            \"Default, Lightning\",\n            \"Basic\",\n            \"Basic\",\n            \"No\",\n            \"Excellent\",\n            \"Basic\",\n            \"Medium\",\n            \"Good\"\n        ]\n\n    if BRAKET_AVAILABLE:\n        features[\"Braket\"] = [\n            \"Partial\",\n            \"IonQ, Rigetti, OQC\",\n            \"SV1, DM1, TN1\",\n            \"Basic\",\n            \"Basic\",\n            \"No\",\n            \"Basic\",\n            \"No\",\n            \"Small\",\n            \"Good\"\n        ]\n\n    # Display feature comparison table\n    df = pd.DataFrame(features)\n    print(\"\\nQuantum Framework Feature Comparison:\")\n    print(tabulate(df, headers='keys', tablefmt='pretty'))\n\ndef quantum_hardware_comparison():\n    \"\"\"Compare different quantum hardware approaches\"\"\"\n    hardware = {\n        \"Property\": [\n            \"Qubit Count Range\",\n            \"Gate Fidelity\",\n            \"Coherence Time\",\n            \"Gate Speed\",\n            \"Operating Temperature\",\n            \"Primary Error Sources\",\n            \"Connectivity\",\n            \"Readout Fidelity\"\n        ],\n        \"Superconducting\": [\n            \"50-433\",\n            \"99-99.9%\",\n            \"100-300 \u03bcs\",\n            \"10-50 ns\",\n            \"~15 mK\",\n            \"Thermal noise, crosstalk\",\n            \"Limited, nearest-neighbor\",\n            \"95-99%\"\n        ],\n        \"Trapped Ions\": [\n            \"11-32\",\n            \"99.5-99.99%\",\n            \"1-100 s\",\n            \"1-10 \u03bcs\",\n            \"Room temp (vacuum)\",\n            \"Motional heating, laser fluctuations\",\n            \"All-to-all\",\n            \"99-99.9%\"\n        ],\n        \"Photonic\": [\n            \"Variable\",\n            \"99-99.9%\",\n            \"Long\",\n            \"1-10 ns\",\n            \"Room/cryo\",\n            \"Photon loss, detector efficiency\",\n            \"Programmable\",\n            \"Variable\"\n        ],\n        \"Neutral Atoms\": [\n            \"100-256\",\n            \"95-99%\",\n            \"1-10 s\",\n            \"100 ns-10 \u03bcs\",\n            \"\u00b5K range\",\n            \"Control precision, atom loss\",\n            \"Programmable\",\n            \"95-99%\"\n        ]\n    }\n\n    # Display hardware comparison table\n    df = pd.DataFrame(hardware)\n    print(\"\\nQuantum Hardware Comparison:\")\n    print(tabulate(df, headers='keys', tablefmt='pretty'))\n\ndef run_on_simulator(framework=\"qiskit\"):\n    \"\"\"\n    Run a simple algorithm on a simulator\n\n    Args:\n        framework: Which framework to use (\"qiskit\", \"cirq\", \"pennylane\", \"braket\")\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement a simple algorithm (e.g. Bell pair or GHZ state)\n    # Run it on a simulator\n    # Display the results\n    pass\n\ndef configure_real_hardware_access():\n    \"\"\"Provide instructions for configuring access to real quantum hardware\"\"\"\n    print(\"\\nConfiguring Access to Real Quantum Hardware:\")\n\n    print(\"\\nIBM Quantum Experience:\")\n    print(\"1. Create an account at https://quantum-computing.ibm.com/\")\n    print(\"2. Get your API token from the user profile page\")\n    print(\"3. Save your token with:\")\n    print(\"   from qiskit import IBMQ\")\n    print(\"   IBMQ.save_account('YOUR_TOKEN')\")\n\n    print(\"\\nAmazon Braket:\")\n    print(\"1. Create an AWS account\")\n    print(\"2. Set up AWS CLI and configure credentials\")\n    print(\"3. Set up a quantum task role in AWS IAM\")\n    print(\"4. Configure AWS credentials locally\")\n    print(\"5. Use the Braket SDK with appropriate region\")\n\n    print(\"\\nGoogle Quantum AI:\")\n    print(\"1. Access may be limited to research partners\")\n    print(\"2. See https://quantumai.google/cirq/tutorials/google/start\")\n\n    # For team discussion: What are the pros and cons of each hardware access model?\n\ndef framework_decision_guide():\n    \"\"\"Guide for choosing the right framework for different tasks\"\"\"\n    use_cases = {\n        \"Use Case\": [\n            \"Education &amp; Learning\",\n            \"Research\",\n            \"Algorithm Development\",\n            \"Quantum Chemistry\",\n            \"Quantum Machine Learning\",\n            \"Optimization Problems\",\n            \"Industry Deployment\",\n            \"Maximum Hardware Control\",\n            \"Hybrid Classical-Quantum\"\n        ],\n        \"Recommended Framework\": [\n            \"Qiskit\",\n            \"Qiskit, Cirq, PennyLane\",\n            \"Qiskit, PennyLane\",\n            \"PennyLane, Qiskit\",\n            \"PennyLane, TensorFlow Quantum\",\n            \"D-Wave Ocean, Qiskit\",\n            \"Braket, Qiskit\",\n            \"Cirq, Qiskit Pulse\",\n            \"PennyLane, Qiskit\"\n        ],\n        \"Rationale\": [\n            \"Best documentation, community, learning resources\",\n            \"Different strengths for different research areas\",\n            \"Comprehensive libraries and tools\",\n            \"Specialized modules available\",\n            \"Native gradient-based optimization\",\n            \"Specialized for different optimization approaches\",\n            \"Enterprise support and reliability\",\n            \"Low-level hardware access\",\n            \"Strong integration with classical ML frameworks\"\n        ]\n    }\n\n    # Display decision guide\n    df = pd.DataFrame(use_cases)\n    print(\"\\nFramework Decision Guide:\")\n    print(tabulate(df, headers='keys', tablefmt='pretty'))\n\ndef main():\n    print(\"QUANTUM COMPUTING TOOLCHAIN &amp; ECOSYSTEM\")\n    print(\"======================================\")\n\n    print(\"\\nAVAILABLE FRAMEWORKS\")\n    print(\"===================\")\n    print_available_frameworks()\n\n    print(\"\\nSYNTAX COMPARISON\")\n    print(\"================\")\n    compare_syntax()\n\n    print(\"\\nBENCHMARK SIMULATORS\")\n    print(\"===================\")\n    benchmark_simulators()\n\n    print(\"\\nAVAILABLE BACKENDS\")\n    print(\"=================\")\n    list_available_backends()\n\n    print(\"\\nFRAMEWORK FEATURE COMPARISON\")\n    print(\"===========================\")\n    framework_feature_comparison()\n\n    print(\"\\nQUANTUM HARDWARE COMPARISON\")\n    print(\"==========================\")\n    quantum_hardware_comparison()\n\n    print(\"\\nRUNNING ON SIMULATORS\")\n    print(\"====================\")\n    for framework in [\"qiskit\", \"cirq\", \"pennylane\", \"braket\"]:\n        if (framework == \"qiskit\" and QISKIT_AVAILABLE or\n            framework == \"cirq\" and CIRQ_AVAILABLE or\n            framework == \"pennylane\" and PENNYLANE_AVAILABLE or\n            framework == \"braket\" and BRAKET_AVAILABLE):\n            print(f\"\\nRunning on {framework.capitalize()} simulator:\")\n            run_on_simulator(framework)\n\n    print(\"\\nCONFIGURING REAL HARDWARE ACCESS\")\n    print(\"===============================\")\n    configure_real_hardware_access()\n\n    print(\"\\nFRAMEWORK DECISION GUIDE\")\n    print(\"=======================\")\n    framework_decision_guide()\n\n    # TEAM DISCUSSION POINT:\n    # What are the key factors to consider when choosing a quantum computing framework for a project?\n    # YOUR DISCUSSION NOTES HERE\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"quantum/part8_toolchain_ecosystem/#collaborative-challenge-explore-the-quantum-ecosystem","title":"Collaborative Challenge: Explore the Quantum Ecosystem","text":"<p>Working together as a team, your challenge is to:</p> <ol> <li>Complete all functions marked with <code># YOUR CODE HERE</code></li> <li>Compare different quantum computing frameworks</li> <li>Benchmark simulator performance</li> <li>Research and document the state of quantum hardware</li> <li>Create a guide for choosing the right tools for different quantum computing tasks</li> </ol>"},{"location":"quantum/part8_toolchain_ecosystem/#team-roles-for-this-exercise","title":"Team Roles for This Exercise","text":"<p>For this lab, consider the following role assignments:</p> <ol> <li>Framework Researcher: Explores and compares different frameworks</li> <li>Benchmark Developer: Creates and runs benchmarks across frameworks</li> <li>Hardware Analyst: Researches and compares quantum hardware approaches</li> <li>Access Specialist: Investigates how to access real quantum hardware</li> <li>Decision Guide Developer: Creates guidelines for framework selection</li> </ol>"},{"location":"quantum/part8_toolchain_ecosystem/#discussion-checkpoints","title":"Discussion Checkpoints","text":""},{"location":"quantum/part8_toolchain_ecosystem/#checkpoint-1-after-framework-exploration","title":"Checkpoint 1: After Framework Exploration","text":"<ul> <li>What are the key differences between the major quantum frameworks?</li> <li>Which frameworks have the best documentation and learning resources?</li> <li>How do the programming models differ between frameworks?</li> </ul>"},{"location":"quantum/part8_toolchain_ecosystem/#checkpoint-2-after-simulator-benchmarking","title":"Checkpoint 2: After Simulator Benchmarking","text":"<ul> <li>How do the simulators compare in terms of performance?</li> <li>What are the scaling limitations of different simulators?</li> <li>Which simulators are best for different types of circuits?</li> </ul>"},{"location":"quantum/part8_toolchain_ecosystem/#checkpoint-3-after-hardware-comparison","title":"Checkpoint 3: After Hardware Comparison","text":"<ul> <li>What are the trade-offs between different quantum hardware approaches?</li> <li>Which hardware approach seems most promising for near-term advantage?</li> <li>How do hardware differences impact algorithm development?</li> </ul>"},{"location":"quantum/part8_toolchain_ecosystem/#debugging-walkthrough","title":"Debugging Walkthrough","text":"<p>Common issues and their solutions:</p> <ol> <li> <p>Problem: API access to quantum hardware fails    Solution: Check your authentication tokens and network connection; consider using a VPN if needed</p> </li> <li> <p>Problem: Different frameworks produce different results    Solution: Check normalization and measurement approaches; frameworks may have different conventions</p> </li> <li> <p>Problem: Simulator crashes with larger circuits    Solution: Different simulators have different memory requirements; reduce circuit size or switch simulators</p> </li> </ol>"},{"location":"quantum/part8_toolchain_ecosystem/#extension-challenge-multi-framework-algorithm","title":"Extension Challenge: Multi-Framework Algorithm","text":"<p>Implement a quantum algorithm using multiple frameworks and compare the results:</p> <pre><code>def multi_framework_algorithm_comparison(algorithm_name=\"bell_pair\"):\n    \"\"\"\n    Implement the same algorithm across multiple frameworks and compare\n\n    Args:\n        algorithm_name: Name of algorithm to implement\n    \"\"\"\n    # YOUR CODE HERE\n    # Implement the same algorithm in all available frameworks\n    # Run on simulators\n    # Compare results, code readability, and performance\n</code></pre>"},{"location":"quantum/part8_toolchain_ecosystem/#what-would-break-this","title":"What Would Break This?","text":"<p>Discuss with your team:</p> <ol> <li>How compatible are circuits between different frameworks?</li> <li>What happens when frameworks update or deprecate features?</li> <li>How do vendor lock-in concerns apply to quantum computing?</li> <li>What if you need features from multiple frameworks in one project?</li> </ol>"},{"location":"quantum/part8_toolchain_ecosystem/#capstone-group-task","title":"Capstone Group Task","text":"<p>Design and implement a \"Framework Evaluation Tool\" that:</p> <ol> <li>Takes a quantum algorithm specification as input</li> <li>Implements it across multiple frameworks</li> <li>Benchmarks performance, code complexity, and results accuracy</li> <li>Generates a recommendation for which framework is best suited for the specific algorithm</li> <li>Provides guidance on how to access either simulators or real hardware</li> </ol>"},{"location":"quantum/part8_toolchain_ecosystem/#how-would-this-work-on-real-hardware","title":"How Would This Work on Real Hardware?","text":"<p>With real quantum hardware:</p> <ol> <li>What extra steps are needed to run on real quantum computers?</li> <li>How do you interpret and validate results from noisy hardware?</li> <li>What cost considerations apply to using cloud quantum services?</li> <li>How do you choose between different hardware types for a given problem?</li> </ol>"},{"location":"quantum/part8_toolchain_ecosystem/#next-steps","title":"Next Steps","text":"<p>In the final session of The Quantum Playground, we'll apply everything we've learned to ship a complete quantum computing project from concept to implementation.</p>"},{"location":"quantum/part9_ship_something/","title":"Part 9: Ship Something","text":""},{"location":"quantum/part9_ship_something/#objective","title":"Objective","text":"<p>Apply all your quantum computing knowledge to build and ship a complete quantum application. By the end of this session, your team will conceptualize, design, implement, document, and present a quantum computing project that demonstrates your collective understanding and skills.</p>"},{"location":"quantum/part9_ship_something/#core-concepts","title":"Core Concepts","text":""},{"location":"quantum/part9_ship_something/#project-development-lifecycle","title":"Project Development Lifecycle","text":"<ol> <li>Conceptualization: Define the problem and quantum approach</li> <li>Design: Create a technical specification and architecture</li> <li>Implementation: Code the quantum and classical components</li> <li>Testing: Verify correctness and analyze performance</li> <li>Documentation: Create clear explanations and user guides</li> <li>Presentation: Communicate your work effectively</li> </ol>"},{"location":"quantum/part9_ship_something/#potential-project-categories","title":"Potential Project Categories","text":"Category Description Example Projects Quantum Algorithms Implement and analyze quantum algorithms Shor's algorithm simulator, Grover's algorithm for database search Quantum Machine Learning Apply quantum techniques to ML problems Quantum neural networks, quantum clustering Quantum Chemistry Simulate molecular systems Hydrogen molecule energy estimation, reaction rate calculation Quantum Games Create games with quantum mechanics Quantum chess, superposition puzzle game Quantum Education Build educational tools Interactive Bloch sphere visualizer, quantum circuit playground Quantum Tools Develop utilities for quantum developers Circuit optimizer, noise analyzer, framework converter"},{"location":"quantum/part9_ship_something/#visual-explanation","title":"Visual Explanation","text":"<p>A successful quantum project requires both quantum and classical components working together, with careful consideration of the problem domain, available quantum resources, and effective visualization and communication of results.</p>"},{"location":"quantum/part9_ship_something/#getting-started","title":"Getting Started","text":"<p>This final session is less structured than previous ones, as your team will define your own project. Here are some suggested starting points:</p>"},{"location":"quantum/part9_ship_something/#project-ideas","title":"Project Ideas","text":"<ol> <li> <p>Quantum Random Number Generator Service</p> </li> <li> <p>Create a web service that provides true quantum randomness</p> </li> <li>Implement both simulator and hardware backends</li> <li> <p>Add visualization of the quantum process</p> </li> <li> <p>Quantum Portfolio Optimizer</p> </li> <li> <p>Use quantum optimization for asset allocation</p> </li> <li>Compare with classical optimization methods</li> <li> <p>Visualize the optimization landscape</p> </li> <li> <p>Quantum Game of Life</p> </li> <li> <p>Implement Conway's Game of Life with quantum rules</p> </li> <li>Explore superposition and entanglement effects</li> <li> <p>Create an interactive visualization</p> </li> <li> <p>Quantum Music Composer</p> </li> <li> <p>Use quantum algorithms to generate musical patterns</p> </li> <li>Map quantum states to musical elements</li> <li> <p>Create an interface for musical exploration</p> </li> <li> <p>Quantum Chemistry Calculator</p> </li> <li> <p>Estimate molecular ground states</p> </li> <li>Compare different variational approaches</li> <li> <p>Visualize molecular orbitals</p> </li> <li> <p>Quantum Machine Learning Classifier</p> </li> <li>Implement a quantum classifier for a standard dataset</li> <li>Compare with classical ML techniques</li> <li>Analyze performance vs. dataset size</li> </ol>"},{"location":"quantum/part9_ship_something/#project-template","title":"Project Template","text":"<p>Here's a basic file structure to get you started:</p> <pre><code>project_name/\n\u251c\u2500\u2500 README.md                 # Project overview and instructions\n\u251c\u2500\u2500 requirements.txt          # Dependencies\n\u251c\u2500\u2500 documentation/            # Detailed documentation\n\u2502   \u251c\u2500\u2500 design.md             # Technical design\n\u2502   \u2514\u2500\u2500 presentation.md       # Presentation notes\n\u251c\u2500\u2500 src/                      # Source code\n\u2502   \u251c\u2500\u2500 quantum/              # Quantum components\n\u2502   \u2502   \u2514\u2500\u2500 circuits.py       # Quantum circuits\n\u2502   \u251c\u2500\u2500 classical/            # Classical components\n\u2502   \u2502   \u2514\u2500\u2500 processing.py     # Classical processing\n\u2502   \u2514\u2500\u2500 main.py               # Main entry point\n\u251c\u2500\u2500 tests/                    # Test cases\n\u2502   \u251c\u2500\u2500 test_quantum.py       # Quantum component tests\n\u2502   \u2514\u2500\u2500 test_classical.py     # Classical component tests\n\u2514\u2500\u2500 visualization/            # Visualization code\n    \u2514\u2500\u2500 visualize.py          # Visualization functions\n</code></pre>"},{"location":"quantum/part9_ship_something/#readmemd-template","title":"README.md Template","text":"<pre><code># [Project Name]\n\n## Overview\n\nBrief description of the project and its quantum aspects.\n\n## Problem Statement\n\nWhat problem does this project solve? Why use quantum computing?\n\n## Quantum Approach\n\nExplanation of the quantum techniques used.\n\n## Installation\n</code></pre> <p>pip install -r requirements.txt</p> <pre><code>## Usage\n</code></pre> <p>python src/main.py</p> <pre><code>## Results\nSummary of results and findings.\n\n## Team Members\n- Person A: Role/contributions\n- Person B: Role/contributions\n- Person C: Role/contributions\n\n## License\n[Choose a license]\n</code></pre>"},{"location":"quantum/part9_ship_something/#project-development-process","title":"Project Development Process","text":"<p>Here's a suggested development process for your team:</p>"},{"location":"quantum/part9_ship_something/#phase-1-conceptualization-30-45-minutes","title":"Phase 1: Conceptualization (30-45 minutes)","text":"<ol> <li>Brainstorm project ideas</li> <li>Evaluate feasibility given time constraints</li> <li>Select a project and define scope</li> <li>Identify quantum aspects and classical components</li> </ol>"},{"location":"quantum/part9_ship_something/#phase-2-design-30-45-minutes","title":"Phase 2: Design (30-45 minutes)","text":"<ol> <li>Create system architecture</li> <li>Design quantum circuits/algorithms</li> <li>Plan classical pre/post-processing</li> <li>Define interfaces between components</li> <li>Create a design document</li> </ol>"},{"location":"quantum/part9_ship_something/#phase-3-implementation-90-120-minutes","title":"Phase 3: Implementation (90-120 minutes)","text":"<ol> <li>Set up project structure</li> <li>Implement quantum components</li> <li>Implement classical components</li> <li>Integrate components</li> <li>Add logging and debugging</li> </ol>"},{"location":"quantum/part9_ship_something/#phase-4-testing-30-45-minutes","title":"Phase 4: Testing (30-45 minutes)","text":"<ol> <li>Test individual components</li> <li>Test integrated system</li> <li>Compare with classical benchmarks if applicable</li> <li>Analyze performance and results</li> </ol>"},{"location":"quantum/part9_ship_something/#phase-5-documentation-presentation-30-45-minutes","title":"Phase 5: Documentation &amp; Presentation (30-45 minutes)","text":"<ol> <li>Create comprehensive README</li> <li>Document code with comments</li> <li>Prepare visualization of results</li> <li>Create presentation materials</li> </ol>"},{"location":"quantum/part9_ship_something/#team-roles","title":"Team Roles","text":"<p>Consider assigning specific roles for this capstone project:</p> <ol> <li>Project Manager: Coordinates efforts, keeps track of time, makes decisions</li> <li>Quantum Developer: Focuses on implementing quantum circuits/algorithms</li> <li>Classical Developer: Implements classical components and integration</li> <li>Testing Specialist: Creates test cases and verifies correctness</li> <li>Documentation Lead: Creates documentation and presentation materials</li> </ol>"},{"location":"quantum/part9_ship_something/#final-presentation","title":"Final Presentation","text":"<p>Prepare a 5-10 minute presentation of your project including:</p> <ol> <li>Problem statement: What problem are you solving?</li> <li>Quantum approach: Why/how quantum computing helps</li> <li>Implementation: Key aspects of your solution</li> <li>Results: What did you find/create?</li> <li>Challenges: What was difficult? How did you overcome it?</li> <li>Future work: How could this be extended?</li> </ol>"},{"location":"quantum/part9_ship_something/#quantum-project-starter-code","title":"Quantum Project Starter Code","text":"<p>Here's a minimal example to get you started with a hybrid quantum-classical application:</p> <pre><code># TEAM MEMBERS: [List names here]\n# DATE: [Today's date]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Dict, List, Tuple, Optional\nimport argparse\nimport json\nimport logging\n\n# Choose your quantum framework\ntry:\n    from qiskit import QuantumCircuit, execute, Aer\n    from qiskit.visualization import plot_histogram\n    QUANTUM_FRAMEWORK = \"qiskit\"\nexcept ImportError:\n    try:\n        import cirq\n        QUANTUM_FRAMEWORK = \"cirq\"\n    except ImportError:\n        try:\n            import pennylane as qml\n            QUANTUM_FRAMEWORK = \"pennylane\"\n        except ImportError:\n            QUANTUM_FRAMEWORK = \"none\"\n            print(\"Warning: No quantum framework available. Installing Qiskit is recommended.\")\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(\"quantum_project\")\n\nclass QuantumComponent:\n    \"\"\"Abstract base class for quantum components\"\"\"\n\n    def __init__(self, n_qubits: int):\n        \"\"\"\n        Initialize the quantum component\n\n        Args:\n            n_qubits: Number of qubits to use\n        \"\"\"\n        self.n_qubits = n_qubits\n        logger.info(f\"Initialized quantum component with {n_qubits} qubits\")\n\n    def create_circuit(self) -&gt; object:\n        \"\"\"\n        Create a quantum circuit\n\n        Returns:\n            A quantum circuit object (framework-specific)\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement create_circuit()\")\n\n    def run_circuit(self, circuit: object, shots: int = 1024) -&gt; Dict:\n        \"\"\"\n        Run a quantum circuit\n\n        Args:\n            circuit: The quantum circuit to run\n            shots: Number of repetitions\n\n        Returns:\n            Measurement results\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement run_circuit()\")\n\nclass QiskitComponent(QuantumComponent):\n    \"\"\"Qiskit implementation of quantum component\"\"\"\n\n    def create_circuit(self) -&gt; QuantumCircuit:\n        \"\"\"\n        Create a Qiskit quantum circuit\n\n        Returns:\n            A Qiskit QuantumCircuit\n        \"\"\"\n        # YOUR CODE HERE\n        # Create and return a Qiskit circuit\n        circuit = QuantumCircuit(self.n_qubits, self.n_qubits)\n\n        # Example: create a GHZ state\n        circuit.h(0)\n        for i in range(1, self.n_qubits):\n            circuit.cx(0, i)\n\n        # Add measurements\n        circuit.measure(range(self.n_qubits), range(self.n_qubits))\n\n        logger.info(f\"Created Qiskit circuit with {self.n_qubits} qubits\")\n        return circuit\n\n    def run_circuit(self, circuit: QuantumCircuit, shots: int = 1024) -&gt; Dict:\n        \"\"\"\n        Run a Qiskit quantum circuit\n\n        Args:\n            circuit: The Qiskit quantum circuit to run\n            shots: Number of repetitions\n\n        Returns:\n            Measurement counts\n        \"\"\"\n        # YOUR CODE HERE\n        # Run the circuit on a simulator and return results\n        simulator = Aer.get_backend('qasm_simulator')\n        job = execute(circuit, simulator, shots=shots)\n        result = job.result()\n        counts = result.get_counts(circuit)\n\n        logger.info(f\"Ran circuit with {shots} shots\")\n        return counts\n\nclass CirqComponent(QuantumComponent):\n    \"\"\"Cirq implementation of quantum component\"\"\"\n\n    def create_circuit(self) -&gt; cirq.Circuit:\n        \"\"\"\n        Create a Cirq quantum circuit\n\n        Returns:\n            A Cirq Circuit\n        \"\"\"\n        # YOUR CODE HERE\n        # Create and return a Cirq circuit\n        pass\n\n    def run_circuit(self, circuit: cirq.Circuit, shots: int = 1024) -&gt; Dict:\n        \"\"\"\n        Run a Cirq quantum circuit\n\n        Args:\n            circuit: The Cirq circuit to run\n            shots: Number of repetitions\n\n        Returns:\n            Measurement counts\n        \"\"\"\n        # YOUR CODE HERE\n        # Run the circuit on a simulator and return results\n        pass\n\nclass PennyLaneComponent(QuantumComponent):\n    \"\"\"PennyLane implementation of quantum component\"\"\"\n\n    def __init__(self, n_qubits: int):\n        \"\"\"\n        Initialize the PennyLane quantum component\n\n        Args:\n            n_qubits: Number of qubits to use\n        \"\"\"\n        super().__init__(n_qubits)\n        # Create a default qubit device\n        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n\n    def create_circuit(self) -&gt; callable:\n        \"\"\"\n        Create a PennyLane quantum circuit\n\n        Returns:\n            A QNode function\n        \"\"\"\n        # YOUR CODE HERE\n        # Create and return a PennyLane QNode\n        pass\n\n    def run_circuit(self, circuit: callable, shots: int = 1024) -&gt; Dict:\n        \"\"\"\n        Run a PennyLane quantum circuit\n\n        Args:\n            circuit: The PennyLane QNode to run\n            shots: Number of repetitions (may not apply to PennyLane)\n\n        Returns:\n            Measurement results\n        \"\"\"\n        # YOUR CODE HERE\n        # Run the circuit and return results\n        pass\n\nclass ClassicalComponent:\n    \"\"\"Classical processing component\"\"\"\n\n    def preprocess(self, data: List) -&gt; List:\n        \"\"\"\n        Preprocess classical data before quantum processing\n\n        Args:\n            data: Input data\n\n        Returns:\n            Preprocessed data\n        \"\"\"\n        # YOUR CODE HERE\n        # Implement classical preprocessing\n        logger.info(f\"Preprocessed {len(data)} data points\")\n        return data\n\n    def postprocess(self, quantum_results: Dict) -&gt; Dict:\n        \"\"\"\n        Process quantum results\n\n        Args:\n            quantum_results: Results from quantum computation\n\n        Returns:\n            Processed results\n        \"\"\"\n        # YOUR CODE HERE\n        # Implement classical postprocessing\n        logger.info(f\"Postprocessed {len(quantum_results)} quantum results\")\n        return quantum_results\n\nclass Visualizer:\n    \"\"\"Visualization component\"\"\"\n\n    def visualize_circuit(self, circuit: object) -&gt; None:\n        \"\"\"\n        Visualize a quantum circuit\n\n        Args:\n            circuit: Quantum circuit to visualize\n        \"\"\"\n        if QUANTUM_FRAMEWORK == \"qiskit\":\n            print(circuit.draw(output='text'))\n        elif QUANTUM_FRAMEWORK == \"cirq\":\n            print(circuit)\n        elif QUANTUM_FRAMEWORK == \"pennylane\":\n            print(circuit.tape.queue)\n\n    def visualize_results(self, results: Dict, title: str = \"Results\") -&gt; None:\n        \"\"\"\n        Visualize quantum results\n\n        Args:\n            results: Results to visualize\n            title: Plot title\n        \"\"\"\n        # YOUR CODE HERE\n        # Create visualization of results\n        if QUANTUM_FRAMEWORK == \"qiskit\":\n            plot_histogram(results)\n            plt.title(title)\n            plt.show()\n        else:\n            # Generic bar chart for other frameworks\n            plt.figure(figsize=(10, 6))\n            plt.bar(results.keys(), results.values())\n            plt.title(title)\n            plt.xlabel('Measurement Outcome')\n            plt.ylabel('Counts/Probability')\n            plt.xticks(rotation=45)\n            plt.tight_layout()\n            plt.show()\n\nclass QuantumApplication:\n    \"\"\"Main quantum application\"\"\"\n\n    def __init__(self, n_qubits: int = 3):\n        \"\"\"\n        Initialize the quantum application\n\n        Args:\n            n_qubits: Number of qubits to use\n        \"\"\"\n        self.n_qubits = n_qubits\n\n        # Initialize components based on available framework\n        if QUANTUM_FRAMEWORK == \"qiskit\":\n            self.quantum = QiskitComponent(n_qubits)\n        elif QUANTUM_FRAMEWORK == \"cirq\":\n            self.quantum = CirqComponent(n_qubits)\n        elif QUANTUM_FRAMEWORK == \"pennylane\":\n            self.quantum = PennyLaneComponent(n_qubits)\n        else:\n            raise ValueError(\"No quantum framework available\")\n\n        self.classical = ClassicalComponent()\n        self.visualizer = Visualizer()\n\n        logger.info(f\"Initialized quantum application with {n_qubits} qubits using {QUANTUM_FRAMEWORK}\")\n\n    def run(self, input_data: List = None, shots: int = 1024) -&gt; Dict:\n        \"\"\"\n        Run the quantum application\n\n        Args:\n            input_data: Input data (optional)\n            shots: Number of circuit repetitions\n\n        Returns:\n            Processed results\n        \"\"\"\n        # Default input data if none provided\n        if input_data is None:\n            input_data = list(range(self.n_qubits))\n\n        # Preprocess\n        preprocessed_data = self.classical.preprocess(input_data)\n\n        # Create and run quantum circuit\n        circuit = self.quantum.create_circuit()\n        self.visualizer.visualize_circuit(circuit)\n\n        quantum_results = self.quantum.run_circuit(circuit, shots)\n\n        # Postprocess results\n        final_results = self.classical.postprocess(quantum_results)\n\n        # Visualize\n        self.visualizer.visualize_results(final_results, \"Quantum Application Results\")\n\n        return final_results\n\n    def save_results(self, results: Dict, filename: str) -&gt; None:\n        \"\"\"\n        Save results to a file\n\n        Args:\n            results: Results to save\n            filename: Output filename\n        \"\"\"\n        with open(filename, 'w') as f:\n            json.dump(results, f, indent=2)\n        logger.info(f\"Saved results to {filename}\")\n\ndef parse_arguments():\n    \"\"\"Parse command line arguments\"\"\"\n    parser = argparse.ArgumentParser(description='Quantum Application')\n    parser.add_argument('--qubits', type=int, default=3, help='Number of qubits')\n    parser.add_argument('--shots', type=int, default=1024, help='Number of shots')\n    parser.add_argument('--output', type=str, default='results.json', help='Output file for results')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose logging')\n    return parser.parse_args()\n\ndef main():\n    \"\"\"Main entrypoint\"\"\"\n    args = parse_arguments()\n\n    # Set log level based on verbosity\n    if args.verbose:\n        logger.setLevel(logging.DEBUG)\n\n    # Check if a quantum framework is available\n    if QUANTUM_FRAMEWORK == \"none\":\n        logger.error(\"No quantum framework available. Please install Qiskit, Cirq, or PennyLane.\")\n        return\n\n    try:\n        # Initialize and run the application\n        app = QuantumApplication(n_qubits=args.qubits)\n        results = app.run(shots=args.shots)\n\n        # Save results if an output file is specified\n        if args.output:\n            app.save_results(results, args.output)\n\n    except Exception as e:\n        logger.error(f\"Error running quantum application: {str(e)}\")\n        if args.verbose:\n            import traceback\n            traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"quantum/part9_ship_something/#resources-for-project-development","title":"Resources for Project Development","text":""},{"location":"quantum/part9_ship_something/#quantum-algorithm-resources","title":"Quantum Algorithm Resources","text":"<ul> <li>Quantum Algorithm Zoo: https://quantumalgorithmzoo.org/</li> <li>Qiskit Textbook: https://qiskit.org/textbook/</li> <li>Cirq Tutorials: https://quantumai.google/cirq/tutorials</li> <li>PennyLane Demos: https://pennylane.ai/qml/demonstrations.html</li> </ul>"},{"location":"quantum/part9_ship_something/#visualization-resources","title":"Visualization Resources","text":"<ul> <li>Qiskit Visualization: https://qiskit.org/documentation/tutorials/visualization/index.html</li> <li>Matplotlib: https://matplotlib.org/</li> <li>Bloch Sphere Visualization: https://github.com/qutip/qutip/blob/master/qutip/bloch.py</li> </ul>"},{"location":"quantum/part9_ship_something/#testing-resources","title":"Testing Resources","text":"<ul> <li>Unit Testing in Python: https://docs.python.org/3/library/unittest.html</li> <li>Qiskit Test Framework: https://qiskit.org/documentation/apidoc/test.html</li> </ul>"},{"location":"quantum/part9_ship_something/#what-would-break-this","title":"What Would Break This?","text":"<p>Discuss with your team:</p> <ol> <li>What are the limitations of your quantum approach?</li> <li>How would your solution scale with problem size?</li> <li>What noise effects would impact your project on real hardware?</li> <li>Are there classical alternatives that might outperform your quantum solution?</li> </ol>"},{"location":"quantum/part9_ship_something/#how-would-this-work-on-real-hardware","title":"How Would This Work on Real Hardware?","text":"<p>Consider:</p> <ol> <li>What modifications would be needed to run on real quantum hardware?</li> <li>How would you verify that the results are correct?</li> <li>What hardware-specific constraints would you need to address?</li> <li>How would you handle noise and errors on real devices?</li> </ol>"},{"location":"quantum/part9_ship_something/#next-steps-beyond-this-course","title":"Next Steps Beyond This Course","text":"<p>After completing this quantum computing curriculum, consider these next steps:</p> <ol> <li>Deeper Algorithm Study: Focus on specific algorithms relevant to your field</li> <li>Hardware Specialization: Learn about specific quantum hardware architectures</li> <li>Industry Applications: Explore applications in finance, chemistry, or machine learning</li> <li>Academic Research: Read recent papers and explore open research questions</li> <li>Community Participation: Join quantum computing communities and contribute to open source projects</li> <li>Certification: Pursue formal quantum computing certifications (e.g., IBM Quantum certification)</li> </ol> <p>We hope you've enjoyed your journey through The Quantum Playground and are excited to continue exploring the fascinating world of quantum computing!</p>"}]}